package manager

import (
	"context"
	"fmt"
	"time"

	"github.com/pkg/errors"
	componentCVEEdgeDataStore "github.com/stackrox/rox/central/componentcveedge/datastore"
	cveDataStore "github.com/stackrox/rox/central/cve/datastore"
	deploymentDataStore "github.com/stackrox/rox/central/deployment/datastore"
	imgDataStore "github.com/stackrox/rox/central/image/datastore"
	imgCVEEdgeDataStore "github.com/stackrox/rox/central/imagecveedge/datastore"
	"github.com/stackrox/rox/central/reprocessor"
	riskManager "github.com/stackrox/rox/central/risk/manager"
	"github.com/stackrox/rox/central/role/resources"
	"github.com/stackrox/rox/central/sensor/service/connection"
	"github.com/stackrox/rox/central/vulnerabilityrequest/cache"
	"github.com/stackrox/rox/central/vulnerabilityrequest/common"
	vulnReqDataStore "github.com/stackrox/rox/central/vulnerabilityrequest/datastore"
	v1 "github.com/stackrox/rox/generated/api/v1"
	"github.com/stackrox/rox/generated/internalapi/central"
	"github.com/stackrox/rox/generated/storage"
	"github.com/stackrox/rox/pkg/batcher"
	"github.com/stackrox/rox/pkg/concurrency"
	"github.com/stackrox/rox/pkg/errorhelpers"
	"github.com/stackrox/rox/pkg/features"
	"github.com/stackrox/rox/pkg/sac"
	"github.com/stackrox/rox/pkg/search"
	deploymentOptionsMap "github.com/stackrox/rox/pkg/search/options/deployments"
	"github.com/stackrox/rox/pkg/search/scoped"
)

var (
	batchSize = 1000

	// Give the processor access as an approver so that it can properly expire
	allAccessCtx             = sac.WithAllAccess(context.Background())
	allVulnApproverAccessSac = sac.WithGlobalAccessScopeChecker(context.Background(),
		sac.AllowFixedScopes(
			sac.AccessModeScopeKeys(storage.Access_READ_ACCESS, storage.Access_READ_WRITE_ACCESS),
			sac.ResourceScopeKeys(resources.VulnerabilityManagementApprovals)))
	requesterOrApproverSAC = sac.ForResources(sac.ForResource(resources.VulnerabilityManagementRequests), sac.ForResource(resources.VulnerabilityManagementApprovals))

	clusterIDField = deploymentOptionsMap.OptionsMap.MustGet(search.ClusterID.String())
)

type managerImpl struct {
	deployments       deploymentDataStore.DataStore
	images            imgDataStore.DataStore
	imageCVEEdges     imgCVEEdgeDataStore.DataStore
	cves              cveDataStore.DataStore
	componentCVEEdges componentCVEEdgeDataStore.DataStore
	vulnReqs          vulnReqDataStore.DataStore
	connManager       connection.Manager
	riskManager       riskManager.Manager
	reprocessor       reprocessor.Loop
	activeReqCache    cache.VulnReqCache
	pendingReqCache   cache.VulnReqCache

	reObserveTimedDeferralsTickerDuration     time.Duration
	reObserveWhenFixedDeferralsTickerDuration time.Duration

	stopSig concurrency.Signal
	stopped concurrency.Signal
}

func (m *managerImpl) Start() {
	if !features.VulnRiskManagement.Enabled() {
		return
	}
	if err := m.buildCache(); err != nil {
		log.Errorf("Could not build vulnerability request cache. Vulnerability snoozing and unsnoozing may not work correctly: %v", err)
	}
	go m.runExpiredDeferralsProcessor()
}

func (m *managerImpl) Stop() {
	m.stopSig.Signal()
	m.stopped.Wait()
}

// SnoozeVulnerabilityOnRequest snoozes the CVE for the scope specified by the request
// Snoozed vulns won't result in a policy violation nor will it be included in risk calculation.
func (m *managerImpl) SnoozeVulnerabilityOnRequest(_ context.Context, request *storage.VulnerabilityRequest) error {
	// Only snooze the vulns if the request was approved and not expired
	if request.GetExpired() || request.GetStatus() != storage.RequestStatus_APPROVED {
		return errors.Errorf("vulnerability request %s not approved or expired", request.GetId())
	}
	// Add to the activeReqCache first because the request could be for images not detected in system.
	m.pendingReqCache.Remove(request.GetId())
	m.activeReqCache.Add(request)

	// Get all the requests still active on the cves.
	vulnReqIDs, err := m.getActiveVulnReqsForCVEs(request.GetCves().GetIds()...)
	if err != nil {
		return errors.Wrap(err, "could not search for vulnerability requests")
	}
	cveStateMap := m.activeReqCache.GetEffectiveVulnState(request.GetCves().GetIds(), vulnReqIDs...)

	// Search for images matching the scope instead of image+cve combination.
	// Validation of image-cve existence is performed by the image-cve datastore.
	imageIDs, err := m.getImagesIDsForVulnRequest(request)
	if err != nil {
		return errors.Wrapf(err, "could not fetch images matching vulnerability request %s", request.GetId())
	}
	if len(imageIDs) == 0 {
		return nil
	}

	for _, cve := range request.GetCves().GetIds() {
		if err := m.imageCVEEdges.UpdateVulnerabilityState(allAccessCtx, cve, imageIDs, cveStateMap[cve]); err != nil {
			return errors.Wrapf(err, "could not snooze vulnerabilities for request %s", request.GetId())
		}
	}

	go m.reprocessAffectedEntities(request.GetId(), imageIDs...)
	return nil
}

// UnSnoozeVulnerabilityOnRequest unsnoozes the CVE for the scope specified by the request
// unless there is another request that is still active that causes this CVE to remain snoozed
func (m *managerImpl) UnSnoozeVulnerabilityOnRequest(_ context.Context, request *storage.VulnerabilityRequest) error {
	m.activeReqCache.Remove(request.GetId())

	// Get all the requests still active on the cves.
	vulnReqIDs, err := m.getActiveVulnReqsForCVEs(request.GetCves().GetIds()...)
	if err != nil {
		return errors.Wrap(err, "could not search for vulnerability requests")
	}
	cveStateMap := m.activeReqCache.GetEffectiveVulnState(request.GetCves().GetIds(), vulnReqIDs...)

	// Search for images matching the scope instead of image+cve combination.
	// Validation of image-cve existence is performed by the image-cve datastore.
	imageIDs, err := m.getImagesIDsForVulnRequest(request)
	if err != nil {
		return errors.Wrapf(err, "could not fetch images matching vulnerability request %s", request.GetId())
	}
	if len(imageIDs) == 0 {
		return nil
	}
	for _, cve := range request.GetCves().GetIds() {
		if err := m.imageCVEEdges.UpdateVulnerabilityState(allAccessCtx, cve, imageIDs, cveStateMap[cve]); err != nil {
			return errors.Wrapf(err, "could not un-snooze vulnerabilities for request %s", request.GetId())
		}
	}

	go m.reprocessAffectedEntities(request.GetId(), imageIDs...)
	return nil
}

func (m *managerImpl) DeploymentCount(ctx context.Context, requestID string, query *v1.Query) (int, error) {
	request, err := m.getVulnRequest(ctx, requestID)
	if err != nil {
		return 0, nil
	}
	query, err = getAffectedResourceQuery(request, query)
	if err != nil {
		return 0, err
	}
	return m.deployments.Count(ctx, query)
}

func (m *managerImpl) ImageCount(ctx context.Context, requestID string, query *v1.Query) (int, error) {
	request, err := m.getVulnRequest(ctx, requestID)
	if err != nil {
		return 0, nil
	}
	query, err = getAffectedResourceQuery(request, query)
	if err != nil {
		return 0, err
	}
	return m.images.Count(ctx, query)
}

func (m *managerImpl) Deployments(ctx context.Context, requestID string, query *v1.Query) ([]*storage.Deployment, error) {
	request, err := m.getVulnRequest(ctx, requestID)
	if err != nil {
		return nil, nil
	}
	query, err = getAffectedResourceQuery(request, query)
	if err != nil {
		return nil, err
	}
	return m.deployments.SearchRawDeployments(ctx, query)
}

func (m *managerImpl) Images(ctx context.Context, requestID string, query *v1.Query) ([]*storage.Image, error) {
	request, err := m.getVulnRequest(ctx, requestID)
	if err != nil {
		return nil, nil
	}
	query, err = getAffectedResourceQuery(request, query)
	if err != nil {
		return nil, err
	}
	return m.images.SearchRawImages(ctx, query)
}

func (m *managerImpl) VulnsWithState(ctx context.Context, registry, remote, tag string) (map[string]storage.VulnerabilityState, error) {
	if ok, err := requesterOrApproverSAC.ReadAllowedToAny(ctx); err != nil {
		return nil, err
	} else if !ok {
		return nil, sac.ErrResourceAccessDenied
	}
	return m.activeReqCache.GetVulnsWithState(registry, remote, tag), nil
}

func (m *managerImpl) EffectiveVulnReq(ctx context.Context, registry, remote, tag, cve string) (*storage.VulnerabilityRequest, error) {
	// First check if there is an active deferral vul req since that is THE vuln req in in-effect.
	id := m.activeReqCache.GetEffectiveVulnReqID(registry, remote, tag, cve)
	if id == "" {
		id = m.pendingReqCache.GetEffectiveVulnReqID(registry, remote, tag, cve)
	}
	if id == "" {
		return nil, nil
	}
	ret, found, err := m.vulnReqs.Get(ctx, id)
	if err != nil || !found {
		return nil, err
	}
	return ret, nil
}

func (m *managerImpl) EnrichImageWithSuppressedCVEs(image *storage.Image) {
	vulnsMap := m.activeReqCache.GetVulnsWithState(image.GetName().GetRegistry(), image.GetName().GetRemote(), image.GetName().GetTag())
	if len(vulnsMap) == 0 {
		return
	}

	for _, comp := range image.GetScan().GetComponents() {
		for _, vuln := range comp.GetVulns() {
			vuln.State = vulnsMap[vuln.GetCve()]
		}
	}
}

func (m *managerImpl) reprocessAffectedEntities(requestID string, affectedImages ...string) {
	// Once the Secured Cluster image cache is invalidated, the image pull cycle is run. It further triggers image
	// risk calculation. Hence, we do not need to recalculate risk here.
	if err := m.reprocessImage(requestID, affectedImages...); err != nil {
		log.Errorf("Could not fetch Secured Cluster image cache keys in response to vuln request %q: %v", requestID, err)
	}
	go m.reprocessDeployments(requestID, affectedImages...)
}

func (m *managerImpl) reprocessDeployments(requestID string, affectedImages ...string) {
	// The re-processing will happen anyways at the next re-processing interval.
	depsByCluster, err := m.getAffectedDeployments(affectedImages...)
	if err != nil {
		log.Errorf("Cannot reprocess deployments. "+
			"Could not get deployment affected by vuln request %q: %v", requestID, err)
		return
	}

	var allDeps []string
	for cluster, deps := range depsByCluster {
		allDeps = append(allDeps, deps...)
		conn := m.connManager.GetConnection(cluster)
		if conn == nil {
			continue
		}
		if err := conn.InjectMessage(allAccessCtx, getReprocessDeploymentMsg(deps...)); err != nil {
			log.Errorf("Could not send request to reprocess deployments affected by vuln request %q", requestID)
		}
	}
	// Reprocessor throttles the requests to reprocess deployments once every the reprocessing interval.
	m.reprocessor.ReprocessRiskForDeployments(allDeps...)
}

func (m *managerImpl) reprocessImage(requestID string, affectedImages ...string) error {
	imageKeys := make([]*central.InvalidateImageCache_ImageKey, 0, len(affectedImages))
	for _, imgID := range affectedImages {
		image, found, err := m.images.GetImage(allAccessCtx, imgID)
		if err != nil {
			return errors.Wrap(err, "could not get image for reprocessing")
		}
		if !found {
			continue
		}
		imageKeys = append(imageKeys, &central.InvalidateImageCache_ImageKey{
			ImageId:       imgID,
			ImageFullName: image.GetName().GetFullName(),
		})
		go func() {
			if err := m.riskManager.CalculateRiskAndUpsertImage(image); err != nil {
				log.Errorf("Could not reprocess risk for image %s in response to vuln req %s", image.GetId(), requestID)
			}
		}()
	}

	m.connManager.BroadcastMessage(&central.MsgToSensor{
		Msg: &central.MsgToSensor_InvalidateImageCache{
			InvalidateImageCache: &central.InvalidateImageCache{
				ImageKeys: imageKeys,
			},
		},
	})
	return nil
}

func (m *managerImpl) getActiveVulnReqsForCVEs(cves ...string) ([]string, error) {
	// Get all the requests still active on the cves.
	results, err := m.vulnReqs.Search(allAccessCtx,
		search.NewQueryBuilder().
			AddExactMatches(search.CVE, cves...).
			AddBools(search.ExpiredRequest, false).ProtoQuery())
	if err != nil {
		return nil, err
	}
	return search.ResultsToIDs(results), nil
}

func (m *managerImpl) getAffectedDeployments(affectedImages ...string) (map[string][]string, error) {
	query := search.ConjunctionQuery(
		search.NewQueryBuilder().AddExactMatches(search.ImageSHA, affectedImages...).ProtoQuery(),
		search.NewQueryBuilder().AddStringsHighlighted(search.ClusterID, search.WildcardString).ProtoQuery(),
	)
	results, err := m.deployments.SearchDeployments(allAccessCtx, query)
	if err != nil {
		return nil, errors.Wrap(err, "could not get deployment results")
	}
	if len(results) == 0 {
		return nil, nil
	}

	depsByCluster := make(map[string][]string)
	for _, r := range results {
		clusterIDs := r.FieldToMatches[clusterIDField.FieldPath].GetValues()
		if len(clusterIDs) == 0 {
			log.Errorf("No cluster ID found in fields for deployment %q", r.GetId())
			continue
		}
		depsByCluster[clusterIDs[0]] = append(depsByCluster[clusterIDs[0]], r.GetId())
	}
	return depsByCluster, nil
}

func (m *managerImpl) getVulnRequest(ctx context.Context, requestID string) (*storage.VulnerabilityRequest, error) {
	request, found, err := m.vulnReqs.Get(ctx, requestID)
	if err != nil {
		return nil, nil
	}
	if !found {
		return nil, errors.Errorf("vulnerability request %q not found", requestID)
	}
	return request, nil
}

func (m *managerImpl) getImagesIDsForVulnRequest(request *storage.VulnerabilityRequest) ([]string, error) {
	imageQuery, err := getAffectedResourceQuery(request, nil)
	if err != nil {
		return nil, err
	}
	results, err := m.images.Search(allAccessCtx, imageQuery)
	if err != nil {
		return nil, err
	}
	return search.ResultsToIDs(results), nil
}

func (m *managerImpl) expireDeferrals(deferrals []*storage.VulnerabilityRequest) error {
	processingErrs := errorhelpers.NewErrorList("re-observing expired deferrals")
	for _, req := range deferrals {
		// A request can be re-observed by just marking it inactive
		// NOTE: It is possible that another request will still force this vulnerability to be deferred (e.g. if this was image scoped
		// but a global one still exists).
		if _, err := m.vulnReqs.MarkRequestInactive(allVulnApproverAccessSac, req.GetId(), "[System Generated] Request expired"); err != nil {
			processingErrs.AddWrapf(err, "marking as inactive request %s", req.GetId())
			continue
		}
		if err := m.UnSnoozeVulnerabilityOnRequest(allVulnApproverAccessSac, req); err != nil {
			processingErrs.AddWrapf(err, "unsnoozing vulns for request %s", req.GetId())
		}
	}
	return processingErrs.ToError()
}

func (m *managerImpl) getExpiredDeferrals() ([]*storage.VulnerabilityRequest, error) {
	now := fmt.Sprintf("<%s", time.Now().Format("01/02/2006 MST"))
	q := search.ConjunctionQuery(
		search.NewQueryBuilder().AddGenericTypeLinkedFields([]search.FieldLabel{search.ExpiredRequest, search.RequestExpiryTime}, []interface{}{false, now}).ProtoQuery(),
		search.NewQueryBuilder().AddExactMatches(search.RequestStatus, storage.RequestStatus_APPROVED.String(), storage.RequestStatus_APPROVED_PENDING_UPDATE.String()).ProtoQuery(),
	)
	results, err := m.vulnReqs.SearchRawRequests(allVulnApproverAccessSac, q)
	if err != nil || len(results) == 0 {
		return nil, err
	}
	return results, nil
}

func (m *managerImpl) reObserveExpiredDeferrals() {
	if m.stopped.IsDone() {
		return
	}

	deferrals, err := m.getExpiredDeferrals()
	if err != nil {
		log.Errorf("error retrieving expired deferral requests for reprocessing: %v", err)
		return
	}
	if len(deferrals) == 0 {
		return
	}

	if err := m.expireDeferrals(deferrals); err != nil {
		log.Errorf("Failed to retire expired deferral requests and re-observe associated vulnerabilities with error(s): %+v", err)
	} else {
		log.Infof("Completed retiring %d expired deferral requests and re-observing deferred vulnerabilities", len(deferrals))
	}
}

func (m *managerImpl) getFixableDeferrals() ([]*storage.VulnerabilityRequest, error) {
	q := search.ConjunctionQuery(
		search.NewQueryBuilder().AddGenericTypeLinkedFields([]search.FieldLabel{search.ExpiredRequest, search.RequestExpiresWhenFixed}, []interface{}{false, true}).ProtoQuery(),
		search.NewQueryBuilder().AddExactMatches(search.RequestStatus, storage.RequestStatus_APPROVED.String(), storage.RequestStatus_APPROVED_PENDING_UPDATE.String()).ProtoQuery(),
	)
	results, err := m.vulnReqs.SearchRawRequests(allVulnApproverAccessSac, q)
	if err != nil || len(results) == 0 {
		return nil, err
	}
	var fixableReqs []*storage.VulnerabilityRequest
	for _, res := range results {
		for _, cve := range res.GetCves().GetIds() {
			// TODO: Determine if it's worth checking cvePkg.ContainsComponentBasedCVE(cve.GetTypes()) before doing this. It would involve going to the data store to read CVE data
			// This is only necessary if somehow there ended up being a deferral on a cluster CVE. Or if it goes from an image cve to node cve
			// TODO: Test what happens if it's a cluster cve
			cveScopedCtx := scoped.Context(allAccessCtx, scoped.Scope{
				ID:    cve,
				Level: v1.SearchCategory_VULNERABILITIES,
			})

			fixableQuery, err := getAffectedResourceQuery(res, search.NewQueryBuilder().AddBools(search.Fixable, true).ProtoQuery())
			if err != nil {
				return nil, err
			}

			count, err := m.componentCVEEdges.Count(cveScopedCtx, fixableQuery)
			if err != nil {
				return nil, errors.Wrapf(err, "could not fetch cve component edge for cve %q for request %q", cve, res.GetId())
			}

			if count != 0 { // This CVE is fixable for this image (or for all images in the query)
				fixableReqs = append(fixableReqs, res)
			}
		}
	}

	return fixableReqs, nil
}

func (m *managerImpl) reObserveFixableDeferrals() {
	if m.stopped.IsDone() {
		return
	}

	deferrals, err := m.getFixableDeferrals()
	if err != nil {
		log.Errorf("error retrieving deferral requests that are now fixable for reprocessing: %v", err)
		return
	}
	if len(deferrals) == 0 {
		return
	}

	if err := m.expireDeferrals(deferrals); err != nil {
		log.Errorf("Failed to retire now-fixable deferral requests and re-observe associated vulnerabilities with error(s): %+v", err)
	} else {
		log.Infof("Completed retiring %d newly fixable deferral requests and re-observing deferred vulnerabilities", len(deferrals))
	}
}

func (m *managerImpl) runExpiredDeferralsProcessor() {
	defer m.stopped.Signal()
	reObserveTimedDeferralsTicker := time.NewTicker(m.reObserveTimedDeferralsTickerDuration)
	defer reObserveTimedDeferralsTicker.Stop()
	reObserveWhenFixedDeferralsTicker := time.NewTicker(m.reObserveWhenFixedDeferralsTickerDuration)
	defer reObserveWhenFixedDeferralsTicker.Stop()

	// Kick off a run to start
	go m.reObserveExpiredDeferrals()
	go m.reObserveFixableDeferrals()

	for {
		select {
		case <-m.stopSig.Done():
			return
		case <-reObserveTimedDeferralsTicker.C:
			go m.reObserveExpiredDeferrals()
		case <-reObserveWhenFixedDeferralsTicker.C:
			go m.reObserveFixableDeferrals()
		}
	}
}

func (m *managerImpl) buildCache() error {
	// Build active requests cache
	q := getApprovedReqQuery()
	res, err := m.vulnReqs.Search(allAccessCtx, q)
	if err != nil {
		return errors.Wrap(err, "error retrieving keys from vuln request datastore")
	}
	ids := search.ResultsToIDs(res)
	if err := buildCache(m.vulnReqs, m.activeReqCache, ids...); err != nil {
		return err
	}

	// Build pending requests cache
	q = getPendingReqQuery()
	res, err = m.vulnReqs.Search(allAccessCtx, q)
	if err != nil {
		return errors.Wrap(err, "error retrieving keys from vuln request datastore")
	}
	ids = search.ResultsToIDs(res)
	if err := buildCache(m.vulnReqs, m.activeReqCache, ids...); err != nil {
		return err
	}
	log.Info("[STARTUP] Successfully cached all vulnerability requests")
	return nil
}

func getPendingReqQuery() *v1.Query {
	return search.ConjunctionQuery(
		search.NewQueryBuilder().AddBools(search.ExpiredRequest, false).ProtoQuery(),
		search.NewQueryBuilder().AddExactMatches(search.RequestStatus, storage.RequestStatus_PENDING.String(), storage.RequestStatus_APPROVED_PENDING_UPDATE.String()).ProtoQuery(),
	)
}

func getApprovedReqQuery() *v1.Query {
	return search.ConjunctionQuery(
		search.NewQueryBuilder().AddBools(search.ExpiredRequest, false).ProtoQuery(),
		search.NewQueryBuilder().AddExactMatches(search.RequestStatus, storage.RequestStatus_APPROVED.String(), storage.RequestStatus_APPROVED_PENDING_UPDATE.String()).ProtoQuery(),
	)
}

func buildCache(vulnReqs vulnReqDataStore.DataStore, cache cache.VulnReqCache, ids ...string) error {
	vulnReqBatcher := batcher.New(len(ids), batchSize)
	for start, end, valid := vulnReqBatcher.Next(); valid; start, end, valid = vulnReqBatcher.Next() {
		vulnReqs, err := vulnReqs.GetMany(allAccessCtx, ids[start:end])
		if err != nil {
			return err
		}
		cache.AddMany(vulnReqs...)
		log.Infof("[STARTUP] Successfully cached %d/%d vulnerability requests", end, len(ids))
	}
	return nil
}

func getAffectedResourceQuery(request *storage.VulnerabilityRequest, query *v1.Query) (*v1.Query, error) {
	scopeQuery, err := getResourcesQueryForVulnReq(request)
	if err != nil {
		return nil, err
	}
	if query == nil || query.GetQuery() == nil {
		return scopeQuery, nil
	}
	return search.ConjunctionQuery(query, scopeQuery), nil
}

func getResourcesQueryForVulnReq(request *storage.VulnerabilityRequest) (*v1.Query, error) {
	requestScope := request.GetScope()
	if requestScope.GetGlobalScope() != nil {
		return search.NewQueryBuilder().AddExactMatches(search.CVE, request.GetCves().GetIds()...).ProtoQuery(), nil
	}

	if imageScope := requestScope.GetImageScope(); imageScope != nil {
		registry, remote, tag := imageScope.GetRegistry(), imageScope.GetRemote(), imageScope.GetTag()
		if tag == common.MatchAll {
			return search.ConjunctionQuery(
				search.NewQueryBuilder().AddExactMatches(search.ImageRegistry, registry).ProtoQuery(),
				search.NewQueryBuilder().AddExactMatches(search.ImageRemote, remote).ProtoQuery(),
				search.NewQueryBuilder().AddRegexes(search.ImageTag, tag).ProtoQuery(),
			), nil
		}
		return search.ConjunctionQuery(
			search.NewQueryBuilder().AddExactMatches(search.ImageRegistry, registry).ProtoQuery(),
			search.NewQueryBuilder().AddExactMatches(search.ImageRemote, remote).ProtoQuery(),
			search.NewQueryBuilder().AddExactMatches(search.ImageTag, tag).ProtoQuery(),
		), nil
	}
	return nil, errors.New("scope must be provided for a vulnerability request")
}

func getReprocessDeploymentMsg(deps ...string) *central.MsgToSensor {
	return &central.MsgToSensor{
		Msg: &central.MsgToSensor_ReprocessDeployment{
			ReprocessDeployment: &central.ReprocessDeployment{
				DeploymentIds: deps,
			},
		},
	}
}
