apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: cache-upload
spec:
  description: Upload build cache to S3-compatible storage
  params:
  - name: PATTERNS
    description: File patterns to generate cache key hash
    type: array
  - name: TARGET
    description: Cache target URI with {{hash}} placeholder
    type: string
  - name: CACHE_PATH
    description: Local path containing cache to upload
    type: string
  - name: WORKING_DIR
    description: Working directory for pattern matching
    type: string
  - name: AWS_SHARED_CREDENTIALS_FILE
    description: AWS credentials file path
    type: string
  - name: AWS_CONFIG_FILE
    description: AWS config file path
    type: string
  - name: BLOB_QUERY_PARAMS
    description: Additional query parameters for blob storage
    type: string
    default: ""
  - name: INSECURE
    description: Allow insecure connections
    type: string
    default: "false"
  - name: FORCE_CACHE_UPLOAD
    description: Force upload even if cache was fetched
    type: string
    default: "false"
  - name: FETCHED
    description: Whether cache was fetched previously (true/false)
    type: string
    default: "false"

  results:
  - name: uploaded
    description: Whether cache was successfully uploaded (true/false)

  image: quay.io/stackrox-io/apollo-ci:stackrox-build-0.4.9
  env:
  - name: PARAM_WORKING_DIR
    value: $(params.WORKING_DIR)
  - name: PARAM_CACHE_PATH
    value: $(params.CACHE_PATH)
  - name: PARAM_TARGET
    value: $(params.TARGET)
  - name: PARAM_AWS_SHARED_CREDENTIALS_FILE
    value: $(params.AWS_SHARED_CREDENTIALS_FILE)
  - name: PARAM_AWS_CONFIG_FILE
    value: $(params.AWS_CONFIG_FILE)
  - name: PARAM_BLOB_QUERY_PARAMS
    value: $(params.BLOB_QUERY_PARAMS)
  - name: PARAM_INSECURE
    value: $(params.INSECURE)
  - name: PARAM_FORCE_CACHE_UPLOAD
    value: $(params.FORCE_CACHE_UPLOAD)
  - name: PARAM_FETCHED
    value: $(params.FETCHED)
  - name: PARAM_PATTERNS_0
    value: $(params.PATTERNS[0])
  - name: PARAM_PATTERNS_1
    value: $(params.PATTERNS[1])
  - name: PARAM_PATTERNS_2
    value: $(params.PATTERNS[2])
  - name: PARAM_PATTERNS_3
    value: $(params.PATTERNS[3])
  script: |
    #!/bin/bash
    set -e

    echo "=== Cache Upload ==="
    echo "Working directory: $PARAM_WORKING_DIR"
    echo "Cache path: $PARAM_CACHE_PATH"
    echo "Target: $PARAM_TARGET"
    echo "Patterns: $PARAM_PATTERNS_0 $PARAM_PATTERNS_1 $PARAM_PATTERNS_2 $PARAM_PATTERNS_3"

    cd "$PARAM_WORKING_DIR"

    # Generate hash from file patterns (same as fetch)
    HASH=""
    for pattern in "$PARAM_PATTERNS_0" "$PARAM_PATTERNS_1" "$PARAM_PATTERNS_2" "$PARAM_PATTERNS_3"; do
      if [ -z "$pattern" ]; then
        continue
      fi
      if [[ "$pattern" == !* ]]; then
        # Skip exclusion patterns for now
        continue
      fi

      # Find files matching pattern and add to hash
      if find . -name "$pattern" -type f 2>/dev/null | head -1 | grep -q .; then
        find . -name "$pattern" -type f -exec sha256sum {} \; | sort >> /tmp/hash_input
      fi
    done

    if [ ! -f /tmp/hash_input ]; then
      echo "No files found matching patterns, skipping cache upload"
      echo "false" > $(results.uploaded.path)
      exit 0
    fi

    HASH=$(sha256sum /tmp/hash_input | cut -d' ' -f1)
    echo "Generated hash: $HASH"

    # Replace {{hash}} in target URL
    TARGET_URL=$(echo "$PARAM_TARGET" | sed "s/{{hash}}/$HASH/g")
    echo "Cache destination URL: $TARGET_URL"

    # Check if we should skip upload (cache was fetched and force is false)
    if [ "$PARAM_FORCE_CACHE_UPLOAD" = "false" ] && [ "$PARAM_FETCHED" = "true" ]; then
      echo "Cache was fetched and force upload is false, skipping upload"
      echo "false" > $(results.uploaded.path)
      exit 0
    fi

    # Check if cache directory exists and has content
    if [ ! -d "$PARAM_CACHE_PATH" ] || [ -z "$(ls -A "$PARAM_CACHE_PATH" 2>/dev/null)" ]; then
      echo "Cache directory is empty, nothing to upload"
      echo "false" > $(results.uploaded.path)
      exit 0
    fi

    # Set up AWS environment
    export AWS_SHARED_CREDENTIALS_FILE="$PARAM_AWS_SHARED_CREDENTIALS_FILE"
    export AWS_CONFIG_FILE="$PARAM_AWS_CONFIG_FILE"

    # Create cache archive
    echo "Creating cache archive..."
    cd "$PARAM_CACHE_PATH"
    tar -czf /tmp/cache.tar.gz .
    cd -

    # Upload cache
    if [ "$PARAM_INSECURE" = "true" ]; then
      AWS_ARGS="--no-verify-ssl"
    else
      AWS_ARGS=""
    fi

    # Parse S3 URL
    if [[ "$TARGET_URL" =~ s3://([^/]+)/(.+) ]]; then
      BUCKET="${BASH_REMATCH[1]}"
      KEY="${BASH_REMATCH[2]}"

      echo "Uploading cache to s3://$BUCKET/$KEY"

      # Upload cache
      if aws s3 cp "/tmp/cache.tar.gz" "s3://$BUCKET/$KEY" $AWS_ARGS $PARAM_BLOB_QUERY_PARAMS; then
        echo "Cache successfully uploaded"
        echo "true" > $(results.uploaded.path)
      else
        echo "Failed to upload cache"
        echo "false" > $(results.uploaded.path)
      fi

      # Clean up
      rm -f /tmp/cache.tar.gz
    else
      echo "Invalid S3 URL format: $TARGET_URL"
      echo "false" > $(results.uploaded.path)
    fi

    echo "=== Cache Upload Complete ==="
