---
title: "Machine learning based ranking for risk"
subtitle: "Intelligent risk recommendations for ACS"
author: "Stephan He√üelmann"
date: "2025-11-05"
format:
  revealjs:
    chalkboard:
      buttons: false
    css: styles.css
    footer: "ACS Engineering | Frankfurt meetup 2025"
    preview-links: auto
    slide-number: true
    theme: simple
---

## Agenda

::: incremental
1. **The Challenge**: Why we need machine learning for risk ranking
2. **The Algorithm**: How a random forest works
3. **Training Pipeline**: From Central data to trained model
4. **Predictions**: How to rank deployments in production
5. **Vibe Coding**: Current prototype status
:::

# The Challenge {background-color="#2d2d2d"}

## What Is Risk?

::: columns
::: {.column width="50%"}
::: incremental
- **ACS monitors thousands of deployments** across customer clusters
- Each deployment has **dozens of security characteristics**:
  - Policy violations
  - CVE vulnerabilities
  - Privileged containers
:::
:::

::: {.column width="50%"}
::: incremental
-
  - Network exposure
  - Process anomalies
  - ... and many more

- **Question:** Which deployments should security teams focus on first?
:::
:::
:::

## The Existing Risk Feature

![Current risk assessment in ACS](diagrams/current-risk.png){width="80%"}

## Shortcomings of Current Approach

::: incremental
- **üîí Intransparent**
  - The risk score is baked into ACS with a complex formula.
  - Customers cannot understand or validate how scores are calculated.

- **‚öôÔ∏è Inflexible**
  - Does not adapt to customer needs or emerging trends.
  - One-size-fits-all approach across all environments.
:::

## Current Approach: Rule-Based Multipliers

::: columns
::: {.column width="50%"}

**Feature multiplication**:

```
Risk Score =
  Policy Violations √ó
  Process Baseline √ó
  Vulnerabilities √ó
  Risky Components √ó
  Component Count √ó
  Image Age √ó
  ...
```
:::

::: {.column width="50%"}
**Drawbacks:**

::: incremental
- **Hardcoded formulas**
  - Manually tuned thresholds and weights

- **Cannot reverse the calculation**
  - Multiplied scores lose individual feature information
:::
:::
:::

## Example: Image Age Calculation

```go
// Creates a score that is:
// A) No risk when daysSinceCreated is < penalizeDaysFloor
if imageAgeInDays < penalizeDaysFloor {
    return
}

// B) Increases linearly between penalizeDaysFloor and penalizeDaysCeil
daysSincePenalized := imageAgeInDays - penalizeDaysFloor
scaledDays := float32(daysSincePenalized) / float32(penalizeDaysCeil-penalizeDaysFloor)
riskScore = float32(1) + float32(scaledDays*(maxMultiplier-1))

// C) Is 1.5 when duration is > penalizeDaysCeil
if riskScore > maxMultiplier {
    riskScore = maxMultiplier
}
```

## The Machine Learning Solution

::: incremental
- **Learn** from real or synthetic training data.
- **Identify** which security features matter most.
- **Rank** deployments by learned feature weights.
- **Explain** what features contribute to high risk scores.
- **Improve** continuously as new data arrives.
:::

# The Algorithm {background-color="#2d2d2d"}

## Random Forest: The Committee Approach

**Analogy:** Assessing fire risks for a house.

::: incremental
- Instead of **1 inspector** with a rigid checklist...
  - ... ask **1000 inspectors**.
- Each inspector looks at different factors:
  - Electrical wiring + smoke detectors
  - Flammable materials + escape routes
  - Building age + maintenance history
- **Final assessment** = average of all 1000 opinions
:::

## Random Forest Architecture

![](diagrams/randomforest-architecture.svg){fig-align="center" width="90%"}

## Why Random Forest?

::: incremental
1. ‚úÖ **Robust against outliers**
   - If one tree makes a mistake, 999 others can correct it.

2. ‚úÖ **Handles complexity**
   - Learns non-linear relationships automatically.

3. ‚úÖ **Provides Explanations**
   - Feature importance: which security issues matter most.

4. ‚úÖ **Simple Training**
   - No epochs, no backpropagation, no GPUs required.
:::

## Why **not** Random Forest?

::: incremental
1. ‚ùå **Just a regression algorithm**
   - No actual AI. Not in the training data -> won't be learned.

2. ‚ùå **Supervised learning**
   - Predictions are only as good the training input. Feature engineering matters.

3. ‚ùå **Retraining**
   - Learning new things requires new training data + model retraining.
:::

## Example: One Decision Tree

```
Is vulnerability_score > 7.5?
‚îú‚îÄ YES ‚Üí Is privileged_container_ratio > 0.5?
‚îÇ         ‚îú‚îÄ YES ‚Üí Risk Score = 8.2
‚îÇ         ‚îî‚îÄ NO ‚Üí Is external_exposure = 1?
‚îÇ                  ‚îú‚îÄ YES ‚Üí Risk Score = 6.5
‚îÇ                  ‚îî‚îÄ NO ‚Üí Risk Score = 4.1
‚îî‚îÄ NO ‚Üí Is policy_violation_score > 3.0?
         ‚îú‚îÄ YES ‚Üí Risk Score = 5.0
         ‚îî‚îÄ NO ‚Üí Risk Score = 2.3
```

::: {.fragment}
**Final Prediction:** Average of 1000s such trees
:::

# Training Pipeline {background-color="#2d2d2d"}

## End-to-End Training Flow

![](diagrams/training-pipeline.svg){fig-align="center" width="100%"}

# Predictions {background-color="#2d2d2d"}

## Prediction Flow

![](diagrams/prediction-flow.svg){fig-align="center" width="95%"}

## Feature Importance: What Matters?

**Example: Top 5 most important features**

```
1. policy_violation_score      28.5%
2. max_vulnerability_score     19.8%
3. privileged_container_ratio  15.2%
4. external_exposure            8.9%
5. process_baseline_violations  7.6%
```

:::::: {.fragment}
::: incremental
- **Insight:** Policy violations and critical CVEs drive risk ranking more than other factors.
- **Detail:** Calculated for training data and each prediction.
:::
::::::

## Performance Metric: NDCG

**NDCG = Normalized Discounted Cumulative Gain**

::: incremental
- Measures **ranking quality** on 0-1 scale
- **1.0**: perfect ranking, **0.0**: worst ranking

::: {.fragment}
**Example:**
```
True ranking:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Model ranking: [1, 2, 3, 4, 5, 6, 7, 9, 8, 10]
                                     ‚Üë Small error at bottom
NDCG ‚âà 0.98  (98% as good as perfect ranking)
```
:::
:::

## Typical Performance

:::: {.columns}
::: {.column width="50%"}
**Training Metrics**

- NDCG: **0.75-0.95**
- Training time: **~60 sec**
:::

::: {.column width="50%"}
**Model Characteristics**

- Feature count: **~20**
- Training samples: **100-1000**
- Model size: **~20 MB**
- Trees: **1000**
:::
::::

::: {.fragment}
Validation **NDCG > 0.75** means model ranks deployments reasonably well!
:::

# Vibe Coding {background-color="#2d2d2d"}

## Current Prototype

::: columns
::: {.column width="50%"}
::: incremental
ü§ñ **Model**

- Training pipeline with Central integration
- Feature extraction for deployments + images
- RandomForest model with versioning
- Validation with NDCG tracking
:::
:::

::: {.column width="50%"}
::: incremental
üì¶ **Deployment**

- Python container with FastAPI REST service
- Model persistence to local/remote storage
:::
:::
:::

## Questions?

**Future Improvements**

::: incremental
- Fine-tune different models and improve feature extraction.
- Use LLMs to generate advanced training data.
- Implement back propagation of user driven risk decisions.
- Integrate prototype with ACS UI.
:::

::: {.fragment}
**Resources**

- Code: [`stackrox/ml-risk-service`](https://github.com/stackrox/stackrox/pull/17467)
- Docs: `learning.md` (comprehensive technical guide)
:::
