defaults: &defaults
  docker:
    - image: docker.io/stackrox/apollo-ci:0.2.3
      auth:
        username: $DOCKER_IO_PULL_USERNAME
        password: $DOCKER_IO_PULL_PASSWORD
  working_directory: /go/src/github.com/stackrox/rox

setupGoogleAppCreds: &setupGoogleAppCreds
  run:
    name: Setup GCloud Service Account
    command: |
      touch /tmp/gcp.json
      chmod 0600 /tmp/gcp.json
      echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" >/tmp/gcp.json
      cci-export GOOGLE_APPLICATION_CREDENTIALS /tmp/gcp.json
      gcloud auth activate-service-account --key-file /tmp/gcp.json
      gcloud auth list

restoreGoBuildCache: &restoreGoBuildCache
  restore_cache:
    name: Restoring Go build cache
    keys:
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-master-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-
      - go-cache-v1-

saveGoBuildCache: &saveGoBuildCache
  save_cache:
    name: Saving Go build cache
    key: go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /home/circleci/.cache/go-build

goModCacheKey: &goModCacheKey 'rox-go-pkg-mod-v2-{{ checksum "go.sum" }}'
restoreGoModCache: &restoreGoModCache
  restore_cache:
    name: Restore Go module cache
    keys:
      - *goModCacheKey

saveGoModCache: &saveGoModCache
  save_cache:
    name: Saving Go module cache
    key: *goModCacheKey
    paths:
      - /go/pkg/mod

gradleCacheKey: &gradleCacheKey 'v1-rox-gradle-{{ checksum "qa-tests-backend/build.gradle" }}'
restoreGradle: &restoreGradle
  restore_cache:
    keys:
      - *gradleCacheKey
      - v1-rox-gradle

uiCacheKey: &uiCacheKey 'v4-rox-ui-deps-{{ checksum "ui/package.json" }}-{{ checksum "ui/yarn.lock" }}'
restoreUI: &restoreUI
  restore_cache:
    keys:
      - *uiCacheKey
      - v4-rox-ui-deps-{{ checksum "ui/package.json" }}-
      - v4-rox-ui-deps-

setupHelm: &setupHelm
  run:
    name: Setup tiller by default in GKE Cluster
    command:  |
       kubectl apply -f ./deploy/k8s/tillerbind.yaml

       SUCCESS=0
       for i in {1..10}; do
         if helm init --service-account helm  --tiller-connection-timeout 5 --wait; then
           SUCCESS=1
           break
         fi
         echo "Failed to connect to helm. Retrying in 5 seconds"
         sleep 5
         kubectl -n kube-system get pod
       done

       if [[ "${SUCCESS}" -eq 0 ]]; then
         echo "Failed to connect to helm"
         exit 1
       fi

refreshAlpineBaseImage: &refreshAlpineBaseImage
  run:
    name: Refresh base image
    command: docker pull alpine:3.9


setupRoxctl: &setupRoxctl
  run:
    name: Setup Roxctl from bin
    command: |
      cp bin/linux/roxctl $GOPATH/bin/roxctl

setupLicense: &setupLicense
  run:
    name: Creating License Key
    command: |
      if [[ -n "$CIRCLE_TAG" ]]; then
        echo "Issuing a CI license for testing the release build"
        ROX_LICENSE_KEY="$(licenses/ci.sh)"
      else
        echo "Using the development license for CI"
        ROX_LICENSE_KEY="$(cat deploy/common/dev-license.lic)"
      fi
      cci-export ROX_LICENSE_KEY "$ROX_LICENSE_KEY"

setupDefaultTLSCerts: &setupDefaultTLSCerts
  run:
    name: Setup default TLS certificates
    command: |
      cert_dir="$(mktemp -d)"
      ./tests/scripts/setup-certs.sh "$cert_dir" custom-tls-cert.central.stackrox.local "Server CA"
      cci-export ROX_DEFAULT_TLS_CERT_FILE "${cert_dir}/tls.crt"
      cci-export ROX_DEFAULT_TLS_KEY_FILE "${cert_dir}/tls.key"
      cci-export ROX_TEST_CA_PEM "$(cat "${cert_dir}/ca.crt")"
      cci-export ROX_TEST_CENTRAL_CN "custom-tls-cert.central.stackrox.local"

      cci-export TRUSTSTORE_PATH "${cert_dir}/keystore.p12"

      echo "Contents of ${cert_dir}:"
      ls -al "${cert_dir}"

setupClientTLSCerts: &setupClientTLSCerts
  run:
    name: Setup client auth TLS certificates
    command: |
      cert_dir="$(mktemp -d)"
      ./tests/scripts/setup-certs.sh "$cert_dir" "Client Certificate User" "Client CA"

      cci-export KEYSTORE_PATH "${cert_dir}/keystore.p12"
      cci-export CLIENT_CA_PATH "${cert_dir}/ca.crt"


waitForAPI: &waitForAPI
  run:
    name: Wait for the API server to be up
    command: |
      export API_HOSTNAME=localhost
      export API_PORT=8000
      if [[ "${LOAD_BALANCER}" == "lb" ]]; then
        export API_HOSTNAME=$(./scripts/k8s/get-lb-ip.sh)
        export API_PORT=443
      fi
      export METADATA_URL="https://${API_HOSTNAME}:${API_PORT}/v1/metadata"
      echo "METADATA_URL is set to ${METADATA_URL}"
      set +e
      SUCCESS=0
      for i in $(seq 1 25); do
        metadata="$(curl -sk --connect-timeout 5 --max-time 10 "${METADATA_URL}")"
        if [[ $? -eq 0 && "$(jq '.licenseStatus' -r \<<<"$metadata")" != "RESTARTING" ]]; then
          SUCCESS=1
          break
        fi
        sleep 5
      done
      if [[ "${SUCCESS}" == 0 ]]; then
        kubectl -n stackrox get pod
        echo "Failed to connect to Central"
        exit 1
      fi
      cci-export API_HOSTNAME "$API_HOSTNAME"
      cci-export API_PORT "$API_PORT"
      cci-export API_ENDPOINT "${API_HOSTNAME}:${API_PORT}"

waitForSensorK8s: &waitForSensorK8s
  run:
    name: Wait for the Sensor to be running K8s
    command: |
      ./scripts/ci/sensor-wait.sh

determineWhetherToRunUIDevServer: &determineWhetherToRunUIDevServer
  run:
    name: Determine whether to run the UI dev server
    command: |
      if .circleci/pr_has_label.sh ci-ui-dev-server; then
        echo "Running UI E2Es against the dev server due to the presence of the ci-ui-dev-server label."
        cci-export RUN_UI_DEV_SERVER true
      else
        echo "Running UI E2Es against the production server. Apply the ci-ui-dev-server label to your PR to run against the dev server."
        cci-export RUN_UI_DEV_SERVER false
      fi

runUIDevServer: &runUIDevServer
  run:
    name: Run the UI dev server
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        make -C ui start
      fi
    background: true

waitForUIDevServer: &waitForUIDevServer
  run:
    name: Wait for the UI dev server to be up
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
        curl -k --max-time 60 --retry 50 --retry-connrefused -4 --retry-delay 5 --retry-max-time 300 https://127.0.0.1:3000/v1/ping
      else
        echo "Not running the dev server, skipping..."
      fi

runUIE2E: &runUIE2E
  run:
    name: UI e2e tests
    command: |
      if [ "${RUN_UI_DEV_SERVER}" = "true" ]; then
        export UI_BASE_URL="https://localhost:3000"
      else
        if [[ "${LOAD_BALANCER}" == "lb" ]]; then
          LB_IP=$(./scripts/k8s/get-lb-ip.sh)
          sudo sh -c "echo >>/etc/hosts ${LB_IP} central-lb"
          export UI_BASE_URL="https://central-lb:443"
        else
          export UI_BASE_URL="https://localhost:${LOCAL_PORT}"
        fi
      fi

      make -C ui test-e2e-ci

collectImbuedUILogs: &collectImbuedUILogs
  run:
    name: Collect imbued UI logs
    command: |
      mkdir -p /tmp/imbued-ui-logs
      curl_cmd=(curl)
      if [[ -n "$ROX_USERNAME" && -n "$ROX_PASSWORD" ]]; then
        curl_cmd+=(-u "${ROX_USERNAME}:${ROX_PASSWORD}")
      fi
      status_code="$("${curl_cmd[@]}" -sk -o logs.zip -w "%{http_code}\n" "https://${API_ENDPOINT}/api/logimbue")"
      # Central returns 204 if there are no imbued logs
      if [ "${status_code}" -eq 204 ]; then
        echo "No imbued logs found."
      elif [ "${status_code}" -eq 200 ]; then
        echo "Logs imbued from the UI were found. Please find them in the artifacts section."
        unzip logs.zip -d /tmp/imbued-ui-logs
      else
        echo "Received error status code ${status_code} from the log imbue endpoint; is the log imbue handler broken?"
        exit 1
      fi
    when:
      always

storeImbuedUILogs: &storeImbuedUILogs
  store_artifacts:
    path: /tmp/imbued-ui-logs
    destination: imbued-ui-logs


collectK8sLogs: &collectK8sLogs
  run:
    name: Collect k8s logs

    command: |
      set +e
      ./scripts/ci/collect-service-logs.sh stackrox
      ./scripts/ci/collect-service-logs.sh kube-system
      ./scripts/ci/collect-service-logs.sh proxies
      ./scripts/ci/collect-infrastructure-logs.sh
    when: always

connectToMonitoring: &connectToMonitoring
  run:
    name: Connect to monitoring via port-forward on 9443

    command: |
      set +e
      nohup kubectl port-forward -n 'stackrox' svc/monitoring "9443:8443" 1>/dev/null 2>&1 &
      sleep 5
    when: always

collectMonitoringImages: &collectMonitoringImages
  run:
    name: Collect monitoring images

    command: |
      set +e
      ./monitoring/grafana/fetch-core-images.sh
    when: always

storeMonitoringImages: &storeMonitoringImages
  store_artifacts:
    path: monitoring/grafana/captures
    destination: monitoring-images

storeMonitoringMetrics: &storeMonitoringMetrics
  store_artifacts:
    path: monitoring/grafana/rawmetrics.zip
    destination: monitoring-metrics.zip

verifyNoStackRoxRestarts: &verifyNoStackRoxRestarts
  run:
    name: Verify that StackRox services did not restart during the runs

    command: |
      if [[ $(ls /tmp/k8s-service-logs/stackrox/*-previous.log | wc -l) != 0 ]]; then
        ls /tmp/k8s-service-logs/stackrox/*-previous.log
        exit 1
      fi
    when: always

storeK8sLogs: &storeK8sLogs
  store_artifacts:
    path: /tmp/k8s-service-logs
    destination: k8s-service-logs

storeProfilingResults: &storeProfilingResults
  store_artifacts:
    path: /tmp/pprof
    destination: pprof

storeCypressResults: &storeCypressResults
  store_test_results:
    path: ui/cypress/reports

storeCypressVideos: &storeCypressVideos
  store_artifacts:
    path: ui/cypress/videos
    destination: ui-e2e-videos

storeCypressScreenshots: &storeCypressScreenshots
  store_artifacts:
    path: ui/cypress/screenshots
    destination: ui-e2e-screenshots

storeQATestResults: &storeQATestResults
  store_test_results:
    path: qa-tests-backend/build/test-results/test

storeQASpockReports: &storeQASpockReports
  store_artifacts:
    path: ./qa-tests-backend/build/spock-reports
    destination: qa-test-report

checkKopsLabel: &checkKopsLabel
  run:
    name: Determine whether to run kops tests
    command: |
      set +e
      if [ -z "${CIRCLE_TAG}" ]; then
        .circleci/pr_has_label.sh ci-kops-tests
        # SPECIAL CASE: kops-tests are currently NOT run on master.
        # If you want to run it on master, change this block to be like
        # checkOpenShiftLabel.
        if [ $? -ne 0 ]; then
          echo "Skipping tests because we're not on a PR OR the PR doesn't have the ci-kops-tests label. Apply the ci-kops-tests label to your PR if you want to run them."
          circleci step halt
        fi
      fi

checkScaleLabel: &checkScaleLabel
  run:
    name: Determine whether to run Scale tests
    command: |
      set +e
      .circleci/pr_has_label.sh ci-scale-tests
      if [ $? -eq 1 ]; then
        echo "Skipping tests because we're on a PR. Apply the ci-scale-tests label to your PR if you want to run them."
        circleci step halt
      fi

check-qa-tests-kernel-module-label: &check-qa-tests-kernel-module-label
  run:
    name: Determine whether to run qa tests with kernel module
    command: |
      set +e
      .circleci/pr_has_label.sh ci-qa-tests-kernel-module
      if [ $? -eq 1 ]; then
        echo "Skipping tests because we're on a PR. Apply the ci-qa-tests-kernel-module label to your PR if you want to run them."
        circleci step halt
      fi

backupDB: &backupDB
  run:
    name: Backup DB
    command: |
      mkdir -p db-backup
      roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db backup --output db-backup

storeDBBackupArtifact: &storeDBBackupArtifact
  store_artifacts:
    path: db-backup
    destination: db-backup

updateTestRail: &updateTestRail
  run:
    name: Update test results in TestRail
    command: |
      if [[ -z "${CIRCLE_TAG}" ]]; then
        echo "Only run TestRail updater on tag builds... skipping."
        exit 0
      fi

      ls -la qa-tests-backend/gradle-results
      export RESULTS_FILE_PATH=$(pwd)/qa-tests-backend/gradle-results
      export CI_JOB_NAME=${CIRCLE_JOB}
      export IMAGE_TAG=${CIRCLE_TAG}
      echo $RESULTS_FILE_PATH
      echo $CI_JOB_NAME
      echo $IMAGE_TAG
      make -C qa-tests-backend update-test-rail
    when: always

version: 2.1

commands:

  check-label-to-skip-tests:
    description: "Checks GitHub for the given label, and skips the tests if we're on a PR, and the label is present."
    parameters:
      label:
        type: string
    steps:
      - run:
          name: Determine whether to skip tests
          command: |
            if .circleci/pr_has_label.sh << parameters.label >> ; then
              echo "Skipping tests because of the presence of the << parameters.label >> label..."
              circleci step halt
            fi

  check-label-exists-or-skip:
    description: "Checks GitHub for the given label, and skips the test if we're on a PR and the label is NOT present."
    parameters:
      label:
        type: string
    steps:
      - when:
          condition: << parameters.label >>
          steps:
            - run:
                name: Determine whether to run tests
                command: |
                  set +e
                  .circleci/pr_has_label.sh "<< parameters.label >>"
                  if [ $? -eq 1 ]; then
                    echo "Skipping tests because we're on a PR. Apply the << parameters.label >> label to your PR if you want to run them."
                    circleci step halt
                  fi

  setup-gcp:
    parameters:
      docker-login:
        type: boolean
        default: true

    steps:
      - run:
          name: Setup deployment env
          command: |
            <<# parameters.docker-login >>
            docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"
            <</ parameters.docker-login >>
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export GOOGLE_APPLICATION_CREDENTIALS /tmp/gcp.json
            if .circleci/pr_has_label.sh ci-run-against-rhel; then
              cci-export MAIN_IMAGE_REPO stackrox/main-rhel
            fi

            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list
            gcloud config set project stackrox-ci
            gcloud config set compute/region us-central1
            gcloud config unset compute/zone
            gcloud config set core/disable_prompts True

  create-gke:
    parameters:
      wait:
        type: boolean
        default: true

    steps:
      - run:
          name: Create GKE cluster
          command: |
            source .circleci/create-cluster.sh && create-cluster
            <<# parameters.wait >>
            wait-for-cluster
            <</ parameters.wait >>

  teardown-gke:
    steps:
      - run:
          name: Tear down GKE cluster
          command: |
            ./scripts/ci/cleanup-deployment.sh || true

            CLUSTER_NAME="${CLUSTER_NAME:-prevent-ci-${CIRCLE_JOB}}"
            gcloud container clusters delete "$CLUSTER_NAME" --async

          when: always

  cleanup-clusters-on-fail:
    steps:
      - run:
          name: Ensure GCloud is configured
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list

            gcloud config set project stackrox-ci
            gcloud config set compute/region us-central1
            gcloud config unset compute/zone
            gcloud config set core/disable_prompts True
          when: on_fail

      - run:
          name: Record step as fatal failure
          command: |
            gsutil cp - "gs://stackrox-ci-status/workflows/${CIRCLE_WORKFLOW_ID}/fatal-failures/${CIRCLE_JOB}" \
              </dev/null
          when: on_fail

      - run:
          name: Delete all GKE clusters for this CI run
          # The command is inline here instead of invoking a script so we can even run it
          # when checkout failed due to a force push.
          command: |
            echo "Deleting all GKE clusters for workflow $CIRCLE_WORKFLOW_ID"

            while IFS='' read -r line || [[ -n "$line" ]]; do
            	[[ -n "$line" ]] || continue

            	if [[ "$line" =~ ^([^[:space:]]+)[[:space:]]+([^[:space:]]+)$ ]]; then
            		cluster="${BASH_REMATCH[1]}"
            		zone="${BASH_REMATCH[2]}"
            	else
            		echo >&2 "Could not parse line $line"
            	fi

            	echo "Deleting cluster ${cluster} in zone ${zone} ..."
            	gcloud container clusters delete "$cluster" \
            		--async \
            		--project stackrox-ci \
            		--zone "$zone" \
            		--quiet || true

            done < <(
            	gcloud container clusters list \
            		--project stackrox-ci \
            		--filter "resourceLabels.stackrox-ci-workflow=${CIRCLE_WORKFLOW_ID}" \
            		--format 'csv[no-heading,separator=" "](name,zone)'
            	)

          when: on_fail

  provision-gke-cluster:
    parameters:
      cluster-id:
        type: string

    steps:
      - *restoreGoModCache
      - run:
          name: Run `make vet`
          command: make vet-active-tags

      - setup-gcp:
          docker-login: false

      - run:
          name: Assign cluster name
          command: |
            CLUSTER_NAME="rox-ci-<< parameters.cluster-id >>-${CIRCLE_BUILD_NUM}"
            cci-export CLUSTER_NAME "$CLUSTER_NAME"
            echo "Assigned cluster name is $CLUSTER_NAME"

      - create-gke:
          wait: false

      - run:
          name: Save cluster config
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            mkdir -p "$CONFIG_DIR"
            echo "$CLUSTER_NAME" >>"${CONFIG_DIR}/name"
            gcloud config get-value compute/zone >>"${CONFIG_DIR}/zone"

      - run:
          name: Ensure workflow is still live
          command: |
            .circleci/check-workflow-live.sh

      - run:
          name: Tear down cluster upon failure
          command: |
            gcloud container clusters delete "$CLUSTER_NAME" --async
          when: on_fail

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - .ci-clusters/<< parameters.cluster-id >>

  attach-gke-cluster:
    parameters:
      cluster-id:
        type: string

    steps:
      - run:
          name: Restore config for << parameters.cluster-id >> cluster
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            CLUSTER_NAME="$(cat "${CONFIG_DIR}/name")"
            [[ -n "$CLUSTER_NAME" ]]
            ZONE="$(cat "${CONFIG_DIR}/zone")"
            [[ -n "$ZONE" ]]
            gcloud config set compute/zone "$ZONE"
            cmd=(gcloud container clusters get-credentials --project stackrox-ci --zone "$ZONE" "$CLUSTER_NAME")
            "${cmd[@]}"
            echo "Restored config for cluster ${CLUSTER_NAME}"
            cci-export CLUSTER_NAME "$CLUSTER_NAME"
            echo
            echo "Run the following command to attach to the cluster:"
            echo
            printf " %q" "${cmd[@]}"
            echo

      - run:
          name: Waiting for << parameters.cluster-id >> cluster to stabilize
          command: |
            source .circleci/create-cluster.sh && wait-for-cluster

  deploy-webhook-server:
    steps:
      - run:
          name: Deploy Webhook server
          command: |
            ./scripts/ci/create-webhookserver.sh

  deploy-default-psp:
    steps:
      - run:
          name: Deploy Default PSP for stackrox namespace
          command: |
            ./scripts/ci/create-default-psp.sh

  deploy-authz-plugin:
    steps:
      - run:
          name: Deploy Default Authorization Plugin
          command: |
            ./scripts/ci/create-scopedaccessserver.sh

  run-qa-tests:
    parameters:
      cluster-id:
        type: string

      determine-whether-to-run:
        type: steps

    steps:
      - checkout
      - check-backend-changes

      - steps: << parameters.determine-whether-to-run >>

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - attach-gke-cluster:
          cluster-id: << parameters.cluster-id >>
      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-default-psp
      - deploy-webhook-server
      - deploy-authz-plugin

      - *restoreGradle
      - run:
          name: QA Automation Platform Part 1
          command: |
            export CLUSTER=K8S
            export HOSTNAME="${API_HOSTNAME}"
            export PORT="${API_PORT}"
            if [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
              echo "On master, running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            elif .circleci/pr_has_label.sh ci-all-qa-tests; then
              echo "ci-all-qa-tests label was specified, so running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            else
              echo "On a PR branch, running BAT tests only..."
              make -C qa-tests-backend bat-test || touch FAIL
            fi

      - *collectK8sLogs
      - *storeQATestResults
      - copy-gradle-files:
          target-dir: ${CIRCLE_JOB}-part1

      - run:
          name: QA Automation Platform Part 2
          command: |
            export CLUSTER=K8S
            export HOSTNAME="${API_HOSTNAME}"
            export PORT="${API_PORT}"
            make -C qa-tests-backend sensor-bounce-test

      - run:
          name: Test for failure in Part 1
          command: |
            [[ ! -f FAIL ]]

      - save_cache:
          key: *gradleCacheKey
          paths:
            - ~/.gradle/caches/

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: ${CIRCLE_JOB}-part2

      - *updateTestRail

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - *verifyNoStackRoxRestarts
      - *storeK8sLogs
      - *connectToMonitoring
      - *collectMonitoringImages
      - *storeMonitoringImages
      - *storeMonitoringMetrics

      - teardown-gke

  provision-openshift-cluster:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""
      provision-extra-args:
        type: string
        default: ""

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - run:
          name: Install kubectl
          working_directory: /tmp
          command: |
            wget https://storage.googleapis.com/kubernetes-release/release/v1.11.2/bin/linux/amd64/kubectl
            sudo install kubectl /usr/bin

      - run:
          name: Generate ephemeral SSH key
          command: |
            mkdir -p openshift
            ssh-keygen -t rsa -f openshift/id_rsa -C packer -N ''
            chmod 0400 openshift/id_rsa openshift/id_rsa.pub
            cat openshift/id_rsa.pub

      - run:
          name: Create cloud resources
          command: |
            extra_args=(<< parameters.provision-extra-args >>)

            docker run --rm -t \
              -v $PWD/openshift:/data \
              -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
              "stackrox/openshift-automation:terraform-${OPENSHIFT_AUTOMATION_VERSION}" create "${CIRCLE_WORKFLOW_ID:0:7}<< parameters.suffix >>" "${extra_args[@]}"

      - store_artifacts:
          path: openshift
          destination: openshift

      - run:
          name: Create Openshift cluster
          command: |
            docker run --rm -t \
            -v $PWD/openshift:/data \
            stackrox/openshift-automation:ansible-$OPENSHIFT_AUTOMATION_VERSION

      - run:
          name: Configure Kubeconfig
          command: |
            ls -lh $PWD/openshift
            export KUBECONFIG=$PWD/openshift/config
            kubectl get nodes

      - store_artifacts:
          path: openshift
          destination: openshift

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Create alias for OpenShift data directory
                command: |
                  cp -r openshift/ openshift<< parameters.suffix >>/

      - persist_to_workspace:
          root: .
          paths:
            - openshift<< parameters.suffix >>

      - run:
          name: Destroy Openshift cluster
          command: |
            docker run --rm -t \
              -v $PWD/openshift:/data \
              -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
              stackrox/openshift-automation:terraform-$OPENSHIFT_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}<< parameters.suffix >>"
          when: on_fail

  run-openshift-tests:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - setup_remote_docker

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Copy openshift data directory to canonical location
                command: |
                  [[ ! -d openshift ]] || rm -rf openshift
                  cp -r "openshift<< parameters.suffix >>" openshift

      - run:
          name: Sanity check OpenShift Env
          command: |
            pwd
            ls -lh $PWD/openshift
            export KUBECONFIG=$PWD/openshift/config
            oc get nodes

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/openshift/config"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.2
            cci-export OPENSHIFT_HOST "$(cat openshift/master)"
            cci-export ROX_IMAGE_REGISTRY docker.io
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"
            if .circleci/pr_has_label.sh ci-run-against-rhel; then
              cci-export MAIN_IMAGE_REPO stackrox/main-rhel
            fi

            ./scripts/ci/openshift-gcr-secrets.sh

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          orchestrator-flavor: openshift
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-webhook-server
      - deploy-authz-plugin

      - *restoreGradle
      - run:
          name: QA Automation Platform Part 1
          command: |
            if .circleci/pr_has_label.sh ci-no-qa-tests; then
              echo "Skipping QA tests because of the presence of the ci-no-qa-tests label..."
            else
              export CLUSTER=OPENSHIFT
              export HOSTNAME=localhost
              export PORT=${LOCAL_PORT}
              if [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
                echo "On master, running all QA tests..."
                make -C qa-tests-backend test || touch FAIL
              elif .circleci/pr_has_label.sh ci-all-qa-tests; then
                echo "ci-all-qa-tests label was specified, so running all QA tests..."
                make -C qa-tests-backend test || touch FAIL
              else
                echo "On a PR branch, running BAT tests only..."
                make -C qa-tests-backend bat-test || touch FAIL
              fi
            fi

      - *collectK8sLogs
      - *storeQATestResults
      - copy-gradle-files:
          target-dir: openshift-part1

      - run:
          name: QA Automation Platform Part 2
          command: |
            export CLUSTER=OPENSHIFT
            export HOSTNAME=localhost
            export PORT="${LOCAL_PORT}"
            make -C qa-tests-backend sensor-bounce-test


      - run:
          name: Check for failures in part 1
          command: |
            [[ ! -f FAIL ]]

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: openshift-part2

      - *updateTestRail

      - *backupDB
      - *storeDBBackupArtifact

      - *collectK8sLogs
      - *verifyNoStackRoxRestarts
      - *storeK8sLogs

      - run:
          name: Pack openshift volume
          command: |
            docker create -v /data --name openshift alpine:3.9 /bin/true
            docker cp openshift/id_rsa            openshift:/data
            docker cp openshift/id_rsa.pub        openshift:/data
            docker cp openshift/terraform.tfstate openshift:/data
          when: always

      - run:
          name: Destroy Openshift cluster
          command: |
            docker run --rm -t \
              --volumes-from openshift \
              -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
              stackrox/openshift-automation:terraform-$OPENSHIFT_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}<< parameters.suffix >>"

          when: always

  deploy-stackrox:
    parameters:
      orchestrator-flavor:
        type: string
        default: k8s
      require-cluster-admin:
        type: boolean
        default: false
      validate-autoupgrade-label:
        type: boolean
        default: false
      post-central-deploy-steps:
        type: steps
        default: []

    steps:
      - run:
          name: Deploy central to remote cluster
          command: |
            deploy_dir="deploy/<< parameters.orchestrator-flavor >>"
            "${deploy_dir}/central.sh"

            source scripts/k8s/export-basic-auth-creds.sh "$deploy_dir"
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - steps: << parameters.post-central-deploy-steps >>

      - *waitForAPI

      - run:
          name: Snapshot Kubernetes Resources
          command: |
            <<# parameters.validate-autoupgrade-label >>
            scripts/ci/sensorbundle-label/list-resources.sh >/tmp/k8s-resources-pre-sensor
            <</ parameters.validate-autoupgrade-label >>
            exit 0

      - run:
          name: Deploy Sensor
          command: |
            <<# parameters.require-cluster-admin >>
            kubectl create clusterrolebinding temporary-admin --clusterrole=cluster-admin --user circleci-rox@stackrox-ci.iam.gserviceaccount.com
            <</ parameters.require-cluster-admin >>

            deploy/<< parameters.orchestrator-flavor >>/sensor.sh
            <<# parameters.require-cluster-admin >>
            kubectl delete clusterrolebinding temporary-admin
            <</ parameters.require-cluster-admin >>

      - run:
          name: Validate Sensor bundle (upgrader)
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5
            KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              bin/linux/upgrader \
                -kube-config kubectl \
                -local-bundle deploy/<< parameters.orchestrator-flavor >>/sensor-deploy \
                -workflow validate-bundle
            kill "$proxy_pid"

      - *waitForSensorK8s

      - run:
          name: Verify all new resources have the auto-upgrade label
          command: |
            <<# parameters.validate-autoupgrade-label >>
            {
              scripts/ci/sensorbundle-label/list-resources.sh
              cat /tmp/k8s-resources-pre-sensor
            } | sort | uniq -u >/tmp/k8s-new-resources-post-sensor

            scripts/ci/sensorbundle-label/list-resources.sh 'auto-upgrade.stackrox.io/component=sensor' \
                >/tmp/k8s-sensor-labeled-resources

            {
              cat /tmp/k8s-new-resources-post-sensor
              cat /tmp/k8s-sensor-labeled-resources
              cat /tmp/k8s-sensor-labeled-resources
              cat scripts/ci/sensorbundle-label/whitelist 2>/dev/null
              cat scripts/ci/sensorbundle-label/whitelist 2>/dev/null
            } | sort | uniq -u >/tmp/k8s-new-resources-unlabeled

            if [[ -s /tmp/k8s-new-resources-unlabeled ]]; then
              echo >&2 "The following resources were created by the sensor bundle, but don't carry the sensor bundle label:"
              cat >&2 /tmp/k8s-new-resources-unlabeled
              exit 1
            fi
            <</ parameters.validate-autoupgrade-label >>
            exit 0

  check-backend-changes:
    steps:
      - run:
          name: Check if there were any backend changes
          command: |
            if [[ "$CIRCLE_BRANCH" == "master" || -n "$CIRCLE_TAG" || "$CIRCLE_BRANCH" =~ ^release/.* ]]; then
              exit 0  # Always run on master, tags, or release branches
            fi
            merge_base="$(git merge-base HEAD origin/master)"
            { git diff --name-only "${merge_base}" || echo "???" ; } | egrep -qv '^ui/' || circleci step halt

  copy-gradle-files:
    description: "Copy the gradle results files into a common directory to later parse by the TestRail updater"
    parameters:
      target-dir:
        type: string
    steps:
    - run:
        name: Copy gradle test result files
        command: |
          mkdir -p qa-tests-backend/gradle-results
          mv qa-tests-backend/build/reports/tests/test/classes/ qa-tests-backend/gradle-results/<< parameters.target-dir >>
        when: always

jobs:

  provision-cluster-k8s-tests:
    <<: *defaults
    steps:
      - checkout
      - check-label-to-skip-tests:
          label: ci-no-k8s-tests
      - provision-gke-cluster:
          cluster-id: k8s-tests

  pre-build-ui:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - *restoreUI
      - run:
          name: Fetch dependencies
          command: |
            if [[ -d ui/node_modules && -f ui/yarn.lock.sha256 && "$(cat ui/yarn.lock.sha256)" == "$(openssl dgst -r -sha256 ui/yarn.lock)" ]]; then
              exit 0  # cache is complete if the above condition matches
            fi
            make -C ui deps

      - run:
          name: Prepare UI tree state file
          command: |
            git log -n 1 --pretty=format:%H -- ui/ >/tmp/ui-tree-state

      - restore_cache:
          name: Restoring UI build cache
          key: ui-build-cache-v1-{{ checksum "/tmp/ui-tree-state" }}

      - run:
          name: Build UI
          command: |
            if [[ -d ui/build ]]; then
              exit 0  # build cache is always complete
            fi
            make -C ui build

      - save_cache:
          name: Saving UI build cache
          key: ui-build-cache-v1-{{ checksum "/tmp/ui-tree-state" }}
          paths:
            - ui/build

      - run:
          name: Preparing UI cache
          command: |
            openssl dgst -r -sha256 ui/yarn.lock >ui/yarn.lock.sha256

      - save_cache:
          key: *uiCacheKey
          paths:
            - ui/node_modules
            - ui/yarn.lock.sha256
            - ui/deps
            - ~/.cache/Cypress # Cypress binary will be put there, see https://docs.cypress.io/guides/guides/continuous-integration.html#Example-circle-yml-v2-config-file-with-yarn

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - ui/build # Copied directly into the image downstream.
            - ui/deps
            - ui/node_modules # Used for OSS license auditing downstream.

      - cleanup-clusters-on-fail

  pre-build-cli:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - *restoreGoModCache
      - *restoreGoBuildCache
      - run:
          name: Build the CLI
          command: make cli

      - run:
          name: Validate expected Go version
          command: |
            roxctl_go_version="$(roxctl version --json | jq '.GoVersion' -r)"
            expected_go_version="$(cat EXPECTED_GO_VERSION)"
            if [[ "${roxctl_go_version}" != "${expected_go_version}" ]]; then
              echo "Got unexpected go version ${roxctl_go_version} (wanted ${expected_go_version})"
              exit 1
            fi
      - *saveGoModCache

      - *saveGoBuildCache

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - bin/linux/roxctl
            - bin/darwin/roxctl
            - bin/windows/roxctl.exe

      - cleanup-clusters-on-fail

  pre-build-go-binaries:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - *restoreGoModCache
      - *restoreGoBuildCache

      - run:
          name: Build the main Go binaries
          command: make main-build

      - *saveGoBuildCache

      - run:
          name: Generate the swagger docs
          command: make swagger-docs

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - bin/linux/central
            - bin/linux/migrator
            - bin/linux/kubernetes
            - bin/linux/upgrader
            - bin/linux/collection
            - image/docs # This will go into the image as generated docs.
            - image/keys
            - deps # Used to speed up k8s-tests
            - generated # Used to speed up k8s-tests

      - cleanup-clusters-on-fail

  build:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *refreshAlpineBaseImage

      - run:
          name: Install ossls
          working_directory: /tmp
          command: |
            wget --quiet https://github.com/gruntwork-io/fetch/releases/download/v0.3.5/fetch_linux_amd64
            sudo install fetch_linux_amd64 /usr/bin/fetch
            export GITHUB_OAUTH_TOKEN="$GITHUB_TOKEN"
            fetch --repo="https://github.com/stackrox/ossls" --tag="0.6.0" --release-asset="ossls_linux_amd64" .
            sudo install ossls_linux_amd64 /usr/bin/ossls
            ossls version

      - *restoreGoModCache

      - run:
          name: Generate OSS notice
          command: |
            ls ui/node_modules
            make ossls-notice

      - run:
          name: Build main image
          command: make docker-build-main-image

      - run:
          name: Push new Docker image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            docker push "docker.io/stackrox/main:$(make --quiet tag)" | cat

      - *saveGoModCache

      - store_artifacts:
          path: bin/linux/roxctl
          destination: roxctl/roxctl-linux

      - store_artifacts:
          path: bin/darwin/roxctl
          destination: roxctl/roxctl-darwin

      - run:
          name: Comment on PR
          command: |
            wget --quiet https://github.com/joshdk/hub-comment/releases/download/0.1.0-rc6/hub-comment_linux_amd64
            sudo install hub-comment_linux_amd64 /usr/bin/hub-comment

            export TAG=$(make --quiet tag)
            hub-comment -template-file .circleci/comment-template.tpl

      - cleanup-clusters-on-fail


  build-rhel:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Login to RedHat registry
          command: |
            success="false"
            for i in {1..10}; do
              if docker login -u "$REDHAT_REGISTRY_USERNAME" -p "$REDHAT_REGISTRY_PASSWORD" https://registry.redhat.io; then
                success="true"
                break
              fi
              sleep $(( 2 ** i ))
            done

            if [[ $success == "false" ]]; then
              echo "Failed to login to RHEL registry"
              exit 1
            fi

            success="false"
            for i in {1..10}; do
              if docker pull registry.redhat.io/rhel7:7.6; then
                success="true"
                break
              fi
              sleep $(( 2 ** i ))
            done

            if [[ $success == "false" ]]; then
              echo "Failed to pull RHEL image"
              exit 1
            fi

      - run:
          name: Install ossls
          working_directory: /tmp
          command: |
            wget --quiet https://github.com/gruntwork-io/fetch/releases/download/v0.3.5/fetch_linux_amd64
            sudo install fetch_linux_amd64 /usr/bin/fetch
            export GITHUB_OAUTH_TOKEN="$GITHUB_TOKEN"
            fetch --repo="https://github.com/stackrox/ossls" --tag="0.6.0" --release-asset="ossls_linux_amd64" .
            sudo install ossls_linux_amd64 /usr/bin/ossls
            ossls version

      - *restoreGoModCache

      - run:
          name: Generate OSS notice
          command: |
            ls ui/node_modules
            make ossls-notice

      - run:
          name: Build main image for RHEL
          command: make docker-build-main-image-rhel

      - run:
          name: Push new image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG="$(make --quiet tag)"
            docker push "docker.io/stackrox/main-rhel:${TAG}" | cat

      - cleanup-clusters-on-fail

  build-deployer:
    <<: *defaults
    steps:
      - checkout
      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Build deployer image for GCP Marketplace
          command: make docker-build-deployer-image

      - run:
          name: Push new image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG="$(make --quiet tag)"
            docker push "docker.io/stackrox/deployer:${TAG}" | cat

      - cleanup-clusters-on-fail

  build-scale-monitoring-and-mock-server:
    <<: *defaults
    resource_class: large
    steps:
      - checkout
      - setup_remote_docker

      - *refreshAlpineBaseImage

      - *restoreGoModCache
      - *restoreGoBuildCache

      - run:
          name: Build images
          command: make scale-image mock-grpc-server-image monitoring-image

      - *saveGoBuildCache

      - run:
          name: Push new Docker image
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io

            TAG=$(make --quiet tag)

            for img in scale grpc-server monitoring; do
              docker push "docker.io/stackrox/${img}:${TAG}" | cat
            done

      - cleanup-clusters-on-fail

  unit-tests:
    <<: *defaults
    resource_class: xlarge
    steps:
      - checkout

      - *restoreGoModCache
      - *restoreGradle
      - *restoreUI

      - *restoreGoBuildCache

      - run:
          name: Run Go unit tests
          command: make go-unit-tests

      - *saveGoBuildCache

      - run:
          name: Generate JUnit reports
          command: make generate-junit-reports
          when: always

      - store_test_results:
          path: junit-reports

      - store_artifacts:
          path: junit-reports
          destination: junit-reports

      - run:
          name: Run UI unit tests
          command: make ui-test

      - run:
          name: Upload coverage information
          command: |
            retries=0
            success=0
            while (( retries < 5 && success == 0 )); do
              retries=$((retries + 1))
              success=1
              make upload-coverage || success=0
            done
          when: always

  style-checks:
    <<: *defaults
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache
      - *restoreGradle
      - *restoreUI

      - run:
          name: Run style checks
          command: make style

      - store_artifacts:
          path: qa-tests-backend/build/reports/codenarc
          destination: reports/codenarc

  check-generated-files-up-to-date:
    <<: *defaults
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache
      - run:
          name: Ensure that generated files are up to date. (If this fails, run `make go-generated-srcs` and commit the result.)
          command: |
            git ls-files --others --exclude-standard >/tmp/untracked
            make go-generated-srcs
            git diff --exit-code HEAD
            { git ls-files --others --exclude-standard ; cat /tmp/untracked ; } | sort | uniq -u >/tmp/untracked-new
            if [[ -s /tmp/untracked-new ]]; then
              echo 'Found new untracked files after running `make go-generated-srcs`. Did you forget to git add generated mocks?'
              cat /tmp/untracked-new
              exit 1
            fi

      - run:
          name: Ensure that all TODO references to fixed tickets are gone
          command: |
            .circleci/check-pr-fixes.sh

      - run:
          name: Ensure that there are no TODO references that the developer has marked as blocking a merge
          command: |
            # Matches comments of the form TODO(x), where x can be "DO NOT MERGE/don't-merge"/"dont-merge"/similar
            ./scripts/check-todos.sh 'do\s?n.*merge'

  integration-unit-tests:
    <<: *defaults
    resource_class: large
    steps:
      - checkout

      - *restoreGoModCache

      - *restoreGoBuildCache

      - run:
          name: Run integration tests
          # Retry a few times if the tests fail since they can be flaky.
          command: |
            success=0
            max_tries=5
            for i in $(seq 1 "${max_tries}"); do
              if make integration-unit-tests; then
                success=1
                break
              fi
              echo "Retrying (failed ${i} times)"
            done
            if [[ "${success}" == 0 ]]; then
              echo "Failed after ${max_tries} tries"
              exit 1
            fi

      - *saveGoBuildCache

  k8s-tests:
    <<: *defaults
    environment:
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - INFLUXDB_URL: "http://monitoring-1.us-central1-c.c.stackrox-ci.internal:8086"
      - LOAD_BALANCER: lb
      - ROX_PLAINTEXT_ENDPOINTS: "8080,grpc@8081"
      - ROX_CONFIG_MGMT_UI: true
      - ROX_BADGER_DB: true
      - ROX_SENSOR_AUTOUPGRADE: true

    steps:
      - checkout
      - check-label-to-skip-tests:
          label: ci-no-k8s-tests

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *restoreGoModCache
      - run:
          name: Retrieving missing dependencies
          command: make download-deps

      - *setupRoxctl
      - setup_remote_docker
      - setup-gcp

      - attach-gke-cluster:
          cluster-id: k8s-tests

      - run:
          name: Export Trusted CA file
          command: cci-export TRUSTED_CA_FILE "$PWD/tests/bad-ca/superfish.pem"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          require-cluster-admin: true
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - run:
          name: Set up client CA auth provider
          command: |
            roxctl -e "$API_ENDPOINT" -p "$ROX_PASSWORD" \
              central userpki create test-userpki -r None -c "$CLIENT_CA_PATH"

      - run:
          name: API tests
          command: |
            ./tests/yamls/roxctl_verification.sh
            make -C tests

      - run:
          name: Deploy NGINX proxies
          command: |
            export PROXY_CERTS_DIR="$(mktemp -d)"
            scripts/ci/proxy/deploy.sh
            cci-export PROXY_CERTS_DIR "$PROXY_CERTS_DIR"

      - run:
          name: Port-forward to NGINX proxies
          command: |
            nohup kubectl -n proxies port-forward svc/nginx-proxy-plain-http 10080:80 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-multiplexed 10443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-multiplexed-tls-be 11443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http1 12443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http1-plain 13443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http2 14443:443 </dev/null &>/dev/null &
            nohup kubectl -n proxies port-forward svc/nginx-proxy-tls-http2-plain 15443:443 </dev/null &>/dev/null &
            sleep 1

      - run:
          name: Test HTTP access to plain HTTP proxy
          command: |
            # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
            version="$(curl --retry 5 --retry-connrefused -4 --retry-delay 1 --retry-max-time 10 -f 'http://localhost:10080/v1/metadata' | jq -r '.version')"
            echo "Got version ${version} from server"
            [[ "$version" == "$(make --quiet tag)" ]]

      - run:
          name: Test HTTPS access to multiplexed TLS proxy
          command: |
            # --retry-connrefused only works when forcing IPv4, see https://github.com/appropriate/docker-curl/issues/5
            version="$(
              curl --cacert "${PROXY_CERTS_DIR}/ca.crt" \
                --retry 5 --retry-connrefused -4 --retry-delay 1 --retry-max-time 10 \
                --resolve "central-proxy.stackrox.local:10443:127.0.0.1" \
                -f \
                'https://central-proxy.stackrox.local:10443/v1/metadata' | jq -r '.version')"
            echo "Got version ${version} from server"
            [[ "$version" == "$(make --quiet tag)" ]]

      - run:
          name: Test roxctl access to proxies
          command: |
            proxies=(
              "Plaintext proxy:10080:plaintext"
              "Multiplexed TLS proxy with plain backends:10443"
              "Multiplexed TLS proxy with TLS backends:11443"
              "Multiplexed TLS proxy with plain backends (direct gRPC):10443:direct"
              "Multiplexed TLS proxy with TLS backends (direct gRPC):11443:direct"
              "HTTP/1 proxy with TLS backends:12443"
              "HTTP/1 proxy with plain backends:13443"
              "HTTP/2 proxy with TLS backends:14443"
              "HTTP/2 proxy with plain backends:15443"
            )

            failures=0
            for p in "${proxies[@]}"; do
              name="$(echo "$p" | cut -d: -f1)"
              port="$(echo "$p" | cut -d: -f2)"
              opt="$(echo "$p" | cut -d: -f3)"
              mkdir -p "/tmp/proxy-test-${port}-${opt}" && cd "/tmp/proxy-test-${port}-${opt}"

              extra_args=()
              case "$opt" in
                plaintext)
                  extra_args=(--plaintext --insecure)
                  ;;
                direct)
                  extra_args=(--direct-grpc)
                  ;;
              esac

              echo "Testing roxctl access through ${name}..."
              roxctl "${extra_args[@]}" -e "localhost:${port}" -p "$ROX_PASSWORD" central debug log >/dev/null || \
                failures=$((failures + 1))
              roxctl "${extra_args[@]}" -e "localhost:${port}" -p "$ROX_PASSWORD" sensor generate k8s --name remote --continue-if-exists || \
                failures=$((failures + 1))
              echo "Done."
              rm -rf "/tmp/proxy-test-${port}"
            done

            echo "Total: ${failures} failures."
            (( failures == 0 ))

      - *restoreUI
      - *determineWhetherToRunUIDevServer
      - *runUIDevServer
      - *waitForUIDevServer
      - *runUIE2E

      - *collectImbuedUILogs
      - *storeImbuedUILogs

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - *verifyNoStackRoxRestarts
      - *storeK8sLogs

      - teardown-gke

      - *storeCypressResults
      - *storeCypressScreenshots
      - *storeCypressVideos

  provision-cluster-qa-tests:
    <<: *defaults
    environment:
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "true"
    steps:
      - checkout
      - check-backend-changes
      - check-label-to-skip-tests:
          label: ci-no-qa-tests

      - provision-gke-cluster:
          cluster-id: qa-tests


  qa-tests:
    <<: *defaults
    environment:
      - LOCAL_PORT: 443
      - RUNTIME_SUPPORT: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - INFLUXDB_URL: "http://monitoring-1.us-central1-c.c.stackrox-ci.internal:8086"
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ROX_BADGER_DB: true
      - ROX_SENSOR_AUTOUPGRADE: true

    steps:
      - run-qa-tests:
          cluster-id: qa-tests
          determine-whether-to-run:
            - check-label-to-skip-tests:
                label: ci-no-qa-tests

  provision-cluster-qa-tests-kernel-module:
    <<: *defaults

    steps:
      - checkout
      - check-backend-changes
      - *check-qa-tests-kernel-module-label
      - provision-gke-cluster:
          cluster-id: qa-tests-kernel-module

  qa-tests-kernel-module:
    <<: *defaults
    environment:
      - LOCAL_PORT: 443
      - RUNTIME_SUPPORT: kernel-module
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - INFLUXDB_URL: "http://monitoring-1.us-central1-c.c.stackrox-ci.internal:8086"
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ROX_SENSOR_AUTOUPGRADE: true

    steps:
      - run-qa-tests:
          cluster-id: qa-tests-kernel-module
          determine-whether-to-run:
            - *check-qa-tests-kernel-module-label

  provision-cluster-upgrade-test:
    <<: *defaults
    steps:
      - checkout
      - check-backend-changes
      - provision-gke-cluster:
          cluster-id: upgrade-test

  upgrade-test:
    <<: *defaults
    environment:
      - STORAGE: pvc
      - LOAD_BALANCER: lb
      - ROX_SENSOR_AUTOUPGRADE: true

    steps:
      - checkout
      - check-backend-changes
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - attach-gke-cluster:
          cluster-id: upgrade-test

      - run:
          name: Checkout 2.4.16.4
          command: git checkout 2.4.16.4

      - run:
          name: Launch Central only
          command: |
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            ./deploy/k8s/central.sh

            central_image="$(kubectl -n stackrox get deploy/central -o jsonpath='{.spec.template.spec.containers[?(@.name=="central")].image}')"
            if [ "${central_image}" != "stackrox/main:2.4.16.4" ]; then
              echo "Unexpected central image!"
              kubectl -n stackrox describe deploy/central
              exit 1
            fi

            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - run:
          name: Checkout current commit again
          command: git reset --hard "${CIRCLE_SHA1}"

      - *waitForAPI
      - run:
          name: Restore DB
          command: |
            roxctl -e "${API_HOSTNAME}:${API_PORT}" -p "${ROX_PASSWORD}" central db restore \
              --file ./qa-tests-backend/artifacts/upgrade-tests/v2.4.16.4/stackrox_db_2019_03_01_03_48_45.zip \
              --timeout 1m

      - run:
          name: Do the upgrade
          command: |
            new_image="stackrox/main:$(make --quiet tag)"
            central_patch='[
              {"op": "replace", "path": "/spec/template/spec/containers/0/image", "value": "'"$new_image"'"},
              {"op": "replace", "path": "/spec/template/spec/containers/0/ports", "value": [{"name": "api", "containerPort": 8443}]}
            ]'
            kubectl -n stackrox patch deploy/central --type json -p "$central_patch"
            kubectl -n stackrox describe deploy/central

            network_policy_patch='[
              {"op": "replace", "path": "/spec/ingress/0/ports/0/port", "value": 8443}
            ]'
            kubectl -n stackrox patch networkpolicy/allow-ext-to-central --type json -p "$network_policy_patch"

            if [[ "${LOAD_BALANCER}" == "lb" ]]; then
              lb_patch='[
                {"op": "replace", "path": "/spec/ports/0/targetPort", "value": "api"}
              ]'
              kubectl -n stackrox patch service/central-loadbalancer --type json -p "$lb_patch"
            fi

            central_image="$(kubectl -n stackrox get deploy/central -o jsonpath='{.spec.template.spec.containers[?(@.name=="central")].image}')"
            if [ "${central_image}" != "${new_image}" ]; then
              echo "Unexpected central image!"
              exit 1
            fi
      - *restoreGradle

      - *setupGoogleAppCreds
      - *setupLicense

      - *waitForAPI

      - run:
          name: Install license key
          command: |
            roxctl -e "${API_HOSTNAME}:${API_PORT}" central license add --license="$ROX_LICENSE_KEY"
            sleep 3

      - *waitForAPI

      - run:
          name: Back up and restore the DB
          command: |
            roxctl -e "${API_HOSTNAME}:${API_PORT}" -p "${ROX_PASSWORD}" central db backup \
              --output /tmp/stackrox-db.zip \
              --timeout 1m

            ls -lh /tmp/stackrox-db.zip

            roxctl -e "${API_HOSTNAME}:${API_PORT}" -p "${ROX_PASSWORD}" central db restore \
              --file /tmp/stackrox-db.zip \
              --timeout 1m
            sleep 5

      - *waitForAPI

      - run:
          name: Run upgrade tests
          command: |
            export CLUSTER=K8S
            export HOSTNAME="${API_HOSTNAME}"
            export PORT="${API_PORT}"
            make -C qa-tests-backend upgrade-test

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: upgrade-test

      - *updateTestRail

      - run:
          name: Deploy sensor via upgrader
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5

            roxctl -e "${API_HOSTNAME}:${API_PORT}" -p "${ROX_PASSWORD}" sensor generate k8s \
              --name remote-new

            CLUSTER_ID="$(openssl x509 -in sensor-remote-new/sensor-cert.pem -noout -subject \
              | sed -E 's@^.*/CN=SENSOR_SERVICE: ([^/]+)/.*$@\1@g')"
            ROX_UPGRADE_PROCESS_ID=3b2cbf78-d35a-4c2c-b67b-e37f805c14da \
                ROX_CLUSTER_ID="$CLUSTER_ID" \
                ROX_CENTRAL_ENDPOINT="${API_HOSTNAME}:${API_PORT}" \
                ROX_MTLS_CA_FILE="$(pwd)/sensor-remote-new/ca.pem" \
                ROX_MTLS_CERT_FILE="$(pwd)/sensor-remote-new/sensor-cert.pem" \
                ROX_MTLS_KEY_FILE="$(pwd)/sensor-remote-new/sensor-key.pem" \
                KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              ./bin/linux/upgrader -workflow roll-forward -local-bundle sensor-remote-new -kube-config kubectl

            kill "$proxy_pid"

      - *waitForSensorK8s

      - run:
          name: Deploy sensor via upgrader a second time
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5

            CLUSTER_ID="$(openssl x509 -in sensor-remote-new/sensor-cert.pem -noout -subject \
              | sed -E 's@^.*/CN=SENSOR_SERVICE: ([^/]+)/.*$@\1@g')"
            ROX_UPGRADE_PROCESS_ID=3b2cbf78-d35a-4c2c-b67b-e37f805c14da \
                ROX_CLUSTER_ID="$CLUSTER_ID" \
                ROX_CENTRAL_ENDPOINT="${API_HOSTNAME}:${API_PORT}" \
                ROX_MTLS_CA_FILE="$(pwd)/sensor-remote-new/ca.pem" \
                ROX_MTLS_CERT_FILE="$(pwd)/sensor-remote-new/sensor-cert.pem" \
                ROX_MTLS_KEY_FILE="$(pwd)/sensor-remote-new/sensor-key.pem" \
                KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              ./bin/linux/upgrader -workflow roll-forward -local-bundle sensor-remote-new -kube-config kubectl

            kill "$proxy_pid"

      - *waitForSensorK8s

      - run:
          name: Roll back sensor upgrade
          command: |
            kubectl proxy --port 28001 &
            proxy_pid=$!
            sleep 5

            CLUSTER_ID="$(openssl x509 -in sensor-remote-new/sensor-cert.pem -noout -subject \
              | sed -E 's@^.*/CN=SENSOR_SERVICE: ([^/]+)/.*$@\1@g')"
            ROX_UPGRADE_PROCESS_ID=3b2cbf78-d35a-4c2c-b67b-e37f805c14da \
                ROX_CLUSTER_ID="$CLUSTER_ID" \
                ROX_CENTRAL_ENDPOINT="${API_HOSTNAME}:${API_PORT}" \
                ROX_MTLS_CA_FILE="$(pwd)/sensor-remote-new/ca.pem" \
                ROX_MTLS_CERT_FILE="$(pwd)/sensor-remote-new/sensor-cert.pem" \
                ROX_MTLS_KEY_FILE="$(pwd)/sensor-remote-new/sensor-key.pem" \
                KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
              ./bin/linux/upgrader -workflow roll-back -kube-config kubectl

            kill "$proxy_pid"

      - run:
          name: Verify sensor is gone
          command: |
            kubectl -n stackrox wait deploy/sensor --for=delete --timeout=30s 2>/dev/null || true
            if kubectl -n stackrox get deploy/sensor 2>/dev/null ; then
              echo >&2 "Sensor deployment still exists after 30s"
              exit 1
            fi

      - *collectK8sLogs
      - *storeK8sLogs

      - teardown-gke

  provision-cluster-scale:
    <<: *defaults
    steps:
      - checkout
      - check-backend-changes
      - *checkScaleLabel
      - provision-gke-cluster:
          cluster-id: scale-test

  scale:
    <<: *defaults
    environment:
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - STORAGE: pvc
      - LOAD_BALANCER: lb
      - OUTPUT_FORMAT: helm

    steps:
      - checkout
      - check-backend-changes
      - *checkScaleLabel
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - attach-gke-cluster:
          cluster-id: scale-test

      - *setupHelm
      - *setupGoogleAppCreds
      - *setupLicense

      - run:
          name: Launch Central only
          command: |
            ./deploy/k8s/central.sh

            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"
            cci-export ROX_RECORDER_DB_CREDS /tmp/gcp.json

      - *waitForAPI

      - run:
          name: Launch Replay
          command: |
            ./scale/flightreplay/launch_replay.sh
            mkdir /tmp/pprof
            # 65 min run so that we are confident that the run has completely finished
            ./scale/profiler/pprof.sh /tmp/pprof "${API_ENDPOINT}" 65
      - *storeProfilingResults
      - *collectK8sLogs
      - *verifyNoStackRoxRestarts
      - *storeK8sLogs

      - *connectToMonitoring
      - *collectMonitoringImages
      - *storeMonitoringImages
      - *storeMonitoringMetrics

      - teardown-gke

  provision-openshift:
    machine: true
    environment:
      - OPENSHIFT_AUTOMATION_VERSION: snapshot-0.4.2-7-g6255c49
    steps:
      - provision-openshift-cluster:
          gate-label: "ci-openshift-tests"

  provision-openshift-crio:
    machine: true
    environment:
      - OPENSHIFT_AUTOMATION_VERSION: 0.4.3
    steps:
      - provision-openshift-cluster:
          suffix: "-crio"
          gate-label: "ci-openshift-crio"
          provision-extra-args: "true"

  openshift-tests:
    <<: *defaults
    environment:
      - OPENSHIFT_AUTOMATION_VERSION: snapshot-0.4.2-7-g6255c49
      - LOCAL_PORT: 8000
      - RUNTIME_SUPPORT: ebpf
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - INFLUXDB_URL: "https://influxdb.monitoring.ci.rox.systems"
      - ROX_WHITELIST_GENERATION_DURATION: 1m

    steps:
      - run-openshift-tests:
          gate-label: "ci-openshift-tests"

  openshift-crio-tests:
    <<: *defaults
    environment:
      - OPENSHIFT_AUTOMATION_VERSION: 0.4.3
      - LOCAL_PORT: 8000
      - RUNTIME_SUPPORT: ebpf
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - INFLUXDB_URL: "https://influxdb.monitoring.ci.rox.systems"
      - ROX_WHITELIST_GENERATION_DURATION: 1m

    steps:
      - run-openshift-tests:
          suffix: "-crio"
          gate-label: "ci-openshift-crio"

  provision-kops:
    machine: true
    environment:
      - KOPS_AUTOMATION_VERSION: 0.2.6
    steps:
      - checkout
      - *checkKopsLabel

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - run:
          name: Install kubectl
          working_directory: /tmp
          command: |
            wget https://storage.googleapis.com/kubernetes-release/release/v1.13.3/bin/linux/amd64/kubectl
            sudo install kubectl /usr/bin

      - store_artifacts:
          path: kops
          destination: kops

      - run:
          name: Create Kops cluster
          command: |
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              stackrox/kops-automation:$KOPS_AUTOMATION_VERSION create "${CIRCLE_WORKFLOW_ID:0:7}"

      - run:
          name: Add /etc/hosts entry
          command: echo "$(cat $PWD/kops/master_ip) api.$(cat $PWD/kops/cluster_name)" | sudo tee -a /etc/hosts

      - run:
          name: Configure Kubeconfig
          command: |
            ls -lh $PWD/kops
            export KUBECONFIG=$PWD/kops/kube.yaml
            sudo chown "$USER" $PWD/kops/kube.yaml
            sudo chown "$USER" $PWD/kops/id_rsa
            kubectl get nodes

      - store_artifacts:
          path: kops
          destination: kops

      - persist_to_workspace:
          root: .
          paths:
            - kops

      - run:
          name: Destroy Kops cluster
          command: |
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              stackrox/kops-automation:$KOPS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail


  kops-tests:
    <<: *defaults
    environment:
      - KOPS_AUTOMATION_VERSION: 0.2.6
      - LOCAL_PORT: 8000
      - RUNTIME_SUPPORT: kernel-module
      - MONITORING_SUPPORT: true
      - SCANNER_SUPPORT: true
      - INFLUXDB_URL: "https://influxdb.monitoring.ci.rox.systems"
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - ADMISSION_CONTROLLER: true

    steps:
      - checkout
      - *checkKopsLabel

      - setup_remote_docker

      - run:
          name: Login to Docker Hub
          command: docker login -u "$DOCKER_IO_PULL_USERNAME" -p "$DOCKER_IO_PULL_PASSWORD"

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Add /etc/hosts entry
          command: echo "$(cat $PWD/kops/master_ip) api.$(cat $PWD/kops/cluster_name)" | sudo tee -a /etc/hosts

      - run:
          name: Sanity check KUBECONFIG
          command: |
            ls -lh $PWD/kops
            export KUBECONFIG=$PWD/kops/kube.yaml
            kubectl get nodes

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/kops/kube.yaml"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.4
            cci-export KOPS_HOST "$(cat kops/master)"
            cci-export ROX_IMAGE_REGISTRY docker.io
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$DOCKER_IO_PULL_PASSWORD"
            cci-export REGISTRY_USERNAME "$DOCKER_IO_PULL_USERNAME"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-webhook-server
      - deploy-authz-plugin

      - *restoreGradle
      - run:
          name: QA Automation Platform
          command: |
            export CLUSTER=K8S
            export HOSTNAME=localhost
            export PORT=${LOCAL_PORT}
            make -C qa-tests-backend test

      - *storeQATestResults
      - *storeQASpockReports
      - copy-gradle-files:
          target-dir: kops-tests

      - *updateTestRail

      - *restoreUI
      - *determineWhetherToRunUIDevServer
      - *runUIDevServer
      - *waitForUIDevServer
      - *runUIE2E
      - *collectImbuedUILogs
      - *storeImbuedUILogs

      - *collectK8sLogs
      - *backupDB
      - *storeDBBackupArtifact
      - *verifyNoStackRoxRestarts
      - *storeK8sLogs

      - *storeCypressResults
      - *storeCypressScreenshots
      - *storeCypressVideos

      - run:
          name: Pack kops volume
          command: |
            docker create -v /data --name kops alpine:3.9 /bin/true
            docker cp kops/id_rsa            kops:/data
            docker cp kops/id_rsa.pub        kops:/data
            docker cp kops/terraform.tfstate kops:/data
          when: always

      - run:
          name: Destroy Kops cluster
          command: |
            docker run --rm -t \
              -v $PWD/kops:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              stackrox/kops-automation:$KOPS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: always

  mark-collector-release:
    <<: *defaults
    resource_class: small
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints:
          - "7f:08:58:1e:80:80:6e:66:99:9a:37:cb:e9:96:0b:40"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Check out collector source code
          command: |
            mkdir -p /tmp/collector
            cd /tmp/collector
            git clone git@github.com:stackrox/collector.git .

      - run:
          name: Add current release version to RELEASED_VERSIONS file
          command: |
            collector_version="$(cat COLLECTOR_VERSION)"
            cd /tmp/collector
            git checkout master && git pull

            # We need to make sure the file ends with a newline so as not to corrupt it when appending.
            [[ ! -f RELEASED_VERSIONS ]] || sed -i'' -e '$a\' RELEASED_VERSIONS

            echo "${collector_version}  # Rox release ${CIRCLE_TAG} by ${CIRCLE_USERNAME} at $(date)" \
              >>RELEASED_VERSIONS
            git add RELEASED_VERSIONS
            git -c "user.name=roxbot" -c "user.email=roxbot@stackrox.com" commit \
                -m "Automatic update of RELEASED_VERSIONS file for Rox release ${CIRCLE_TAG}"
            git push

  push-release:
    <<: *defaults
    resource_class: small
    steps:
      - checkout
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Push image into stackrox.io
          command: |
            docker login -u "$DOCKER_IO_PUSH_USERNAME" -p "$DOCKER_IO_PUSH_PASSWORD" docker.io
            docker login -u "$STACKROX_IO_PUSH_USERNAME" -p "$STACKROX_IO_PUSH_PASSWORD" stackrox.io
            docker login -u "$STACKROX_IO_PUSH_USERNAME" -p "$STACKROX_IO_PUSH_PASSWORD" collector.stackrox.io

            MAIN_TAG=$(make --quiet tag)
            COLLECTOR_TAG=$(make --quiet collector-tag)

            docker pull docker.io/stackrox/main:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/main:$MAIN_TAG stackrox.io/main:$MAIN_TAG
            docker push        stackrox.io/main:$MAIN_TAG | cat

            docker pull docker.io/stackrox/monitoring:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/monitoring:$MAIN_TAG stackrox.io/monitoring:$MAIN_TAG
            docker push        stackrox.io/monitoring:$MAIN_TAG | cat

            docker pull    docker.io/stackrox/collector:$COLLECTOR_TAG | cat
            docker tag     docker.io/stackrox/collector:$COLLECTOR_TAG collector.stackrox.io/collector:$COLLECTOR_TAG
            docker push collector.stackrox.io/collector:$COLLECTOR_TAG | cat

      - run:
          name: Push roxctl to stackrox-hub
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list
            MAIN_TAG=$(make --quiet tag)

            cd bin
            for release in "${MAIN_TAG}" latest; do
              for platform in Linux Darwin Windows; do
                platform_lower="$(echo "$platform" | tr A-Z a-z)"
                roxctl_bin="roxctl"
                if [[ "${platform}" == "Windows" ]]; then
                  roxctl_bin="roxctl.exe"
                fi
                gsutil cp "${platform_lower}/${roxctl_bin}" "gs://sr-roxc/${release}/bin/${platform}/${roxctl_bin}"
                gsutil cp "${platform_lower}/${roxctl_bin}" "gs://sr-roxc/${release}/bin/${platform_lower}/${roxctl_bin}"
              done
            done

      - run:
          name: Push image into gcr.io for GCP Marketplace
          command: |
            docker login -u _json_key -p "$GOOGLE_CREDENTIALS_GCR_SCANNER" https://gcr.io

            MAIN_TAG=$(make --quiet tag)
            SCANNER_TAG=$(make --quiet scanner-tag)

            # Deployer was pushed up in build-deployer, so must be pulled back down here.
            docker pull docker.io/stackrox/deployer:$MAIN_TAG | cat
            docker tag  docker.io/stackrox/deployer:$MAIN_TAG gcr.io/stackrox-launcher-project-1/stackrox/deployer:$MAIN_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/deployer:$MAIN_TAG | cat

            # Main was pushed up in the previous step, so just needs to be tagged.
            docker tag  stackrox.io/main:$MAIN_TAG gcr.io/stackrox-launcher-project-1/stackrox:$MAIN_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox:$MAIN_TAG | cat

            # Monitoring was pushed up in the previous step, so just needs to be tagged.
            docker tag  stackrox.io/monitoring:$MAIN_TAG gcr.io/stackrox-launcher-project-1/stackrox/monitoring:$MAIN_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/monitoring:$MAIN_TAG | cat

            # Scanner was pushed up in its own repo, so must be pulled back down here.
            docker pull stackrox.io/scanner:$SCANNER_TAG | cat
            docker tag  stackrox.io/scanner:$SCANNER_TAG gcr.io/stackrox-launcher-project-1/stackrox/scanner:$SCANNER_TAG
            docker push gcr.io/stackrox-launcher-project-1/stackrox/scanner:$SCANNER_TAG | cat

      - run:
          name: Push offline image bundles to stackrox.io
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list

            make offline-bundle

            for release in "$(make --quiet tag)" latest; do
              gsutil cp scripts/offline-bundle/image-bundle.tgz           gs://sr-roxc/${release}/image-bundle.tgz
              gsutil cp scripts/offline-bundle/image-collector-bundle.tgz gs://sr-roxc/${release}/image-collector-bundle.tgz
            done

  regenerate-dev-license:
    <<: *defaults
    resource_class: small
    steps:
      - checkout
      - *setupGoogleAppCreds

      - run:
          name: Regenerate development license
          command: |
            scripts/regenerate-dev-license.sh

      - run:
          name: Configure git
          command: |
            git config user.email "roxbot@stackrox.com"
            git config user.name "RoxBot"

      - add_ssh_keys:
          fingerprints:
            - "b0:90:e9:6f:77:d5:fe:b6:8c:1a:a5:3f:a4:e3:41:e5"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Create temporary branch with license update commit
          command: |
            git add deploy/common/dev-license.lic
            git checkout -b "roxbot-tmp/regenerate-dev-license-${CIRCLE_BUILD_NUM}"
            git commit \
              -m"Automatic re-generation of development license ($(date '+%Y-%m-%d'))"

      - run:
          name: Push updated dev license to remote
          command: |
            commit="$(git rev-parse "roxbot-tmp/regenerate-dev-license-${CIRCLE_BUILD_NUM}")"

            git checkout master

            for i in $(seq 1 3); do
              echo "Attempting to merge into master (attempt $i of 3) ..."

              git fetch && git reset --hard origin/master
              # Note that the above `git reset --hard` requires a clean working tree. We therefore
              # cherry-pick the license update commit instead of generating it now such that we
              # invoke all scripts from the current branch (mostly relevant for testing purposes).
              git cherry-pick "$commit"
              if git push origin master:"roxbot/regenerate-dev-license-${CIRCLE_BUILD_NUM}"; then
                echo "Success!"
                exit 0
              fi

              echo "Push did not succeed. Sleeping for 10 seconds, then trying again."
              sleep 10
            done

            echo "Failed to merge into master."
            exit 1

      - run:
          name: Create PR
          command: |
            git checkout "$CIRCLE_BRANCH"
            scripts/ci/create-dev-license-update-pr.sh "roxbot/regenerate-dev-license-${CIRCLE_BUILD_NUM}"

  roxctl-windows-test:
    <<: *defaults

    steps:
      - checkout
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: "Install Wine"
          command: |
            sudo apt-get update
            sudo apt-get install -y wine64

      - run:
          name: "Generate a K8s/OpenShift bundles using roxctl.exe"
          command: |
            wine64 ./bin/windows/roxctl.exe central generate k8s none --output-dir 'Z:\home\circleci\rox-k8s-bundle'
            wine64 ./bin/windows/roxctl.exe central generate openshift none --output-dir 'Z:\home\circleci\rox-openshift-bundle'

      - run:
          name: "Verify contents of generated bundles"
          command: |
            ls /home/circleci/rox-k8s-bundle
            test -f /home/circleci/rox-k8s-bundle/README

            ls /home/circleci/rox-openshift-bundle
            test -f /home/circleci/rox-openshift-bundle/README

workflows:
  version: 2
  build_all:
    jobs:
      - pre-build-ui:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - pre-build-cli:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - pre-build-go-binaries:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - build:
          context: docker-io-push
          requires:
            - pre-build-ui
            - pre-build-cli
            - pre-build-go-binaries
          filters:
            tags:
              only: /.*/
      - build-rhel:
          context: docker-io-push
          requires:
            - pre-build-ui
            - pre-build-cli
            - pre-build-go-binaries
          filters:
            tags:
              only: /.*/
      - build-deployer:
          context: docker-io-push
          requires:
            - pre-build-cli
          filters:
            tags:
              only: /.*/
      - build-scale-monitoring-and-mock-server:
          context: docker-io-push
          filters:
            tags:
              only: /.*/
      - unit-tests:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - style-checks:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - check-generated-files-up-to-date:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - integration-unit-tests:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - provision-cluster-k8s-tests:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - k8s-tests:
          context: docker-io-pull
          requires:
            - build
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-cluster-k8s-tests
          filters:
            tags:
              only: /.*/
      - provision-cluster-qa-tests:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - qa-tests:
          context: docker-io-pull
          requires:
            - build
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-cluster-qa-tests
          filters:
            tags:
              only: /.*/
      - provision-cluster-qa-tests-kernel-module:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - qa-tests-kernel-module:
          context: docker-io-pull
          requires:
            - build
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-cluster-qa-tests-kernel-module
          filters:
            tags:
              only: /.*/
      - provision-cluster-upgrade-test:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - upgrade-test:
          context: docker-io-pull
          requires:
            - build
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-cluster-upgrade-test
          filters:
            tags:
              only: /.*/
      - provision-cluster-scale:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - scale:
          context: docker-io-pull
          requires:
            - build
            - build-rhel
            - build-scale-monitoring-and-mock-server
            - provision-cluster-scale
          filters:
            tags:
              only: /.*/
      - provision-openshift:
          context: docker-io-pull
          filters:
            tags:
              only:
              - /.*/
      - provision-openshift-crio:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - openshift-tests:
          context: docker-io-pull
          requires:
            - build
            - provision-openshift
          filters:
            tags:
              only:
              - /.*/
      - openshift-crio-tests:
          context: docker-io-pull
          requires:
            - build
            - provision-openshift-crio
          filters:
            tags:
              only: /.*/
      - provision-kops:
          context: docker-io-pull
          filters:
            tags:
              only: /.*/
      - kops-tests:
          context: docker-io-pull
          requires:
            - build
            - provision-kops
          filters:
            tags:
              only: /.*/
      - roxctl-windows-test:
          context: docker-io-pull
          requires:
            - pre-build-cli
          filters:
            tags:
              only: /.*/
      # Push release only on tags.
      # Skip tags that end in -rc.# or .x
      - mark-collector-release:
          context: docker-io-pull
          requires:
            - build
            - unit-tests
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /^(?!.*-rc\.\d+$|.*\.x$).*$/
      - push-release:
          context: docker-io-and-stackrox-io-push
          requires:
            - build
            - build-deployer
            - unit-tests
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /^(?!.*-rc\.\d+$|.*\.x$).*$/


  update_dev_license:
    triggers:
      - schedule:
          cron: "0 0 1,16 * *"
          filters:
            branches:
              only: master

    jobs:
      - regenerate-dev-license:
          context: docker-io-pull
          filters:
            branches:
              only: master
