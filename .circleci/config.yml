runOnAllTags: &runOnAllTags
  filters:
    tags:
      only: /.*/

runOnTagsOnly: &runOnTagsOnly
  filters:
    tags:
      only: /.*/
    branches:
      ignore: /.*/

# This runs a command only on rc builds,
# which is a tagged build with a tag matching the below regex.
runOnlyOnReleaseCandidateBuilds: &runOnlyOnReleaseCandidateBuilds
  filters:
    tags:
      only: /^\d+(\.\d+)?\.\d+\.\d+-rc\.\d+$/
    branches:
      ignore: /.*/

# This runs a command only on release builds,
# which is a tagged build with a tag matching the below regex.
runOnlyOnReleaseBuilds: &runOnlyOnReleaseBuilds
  filters:
    tags:
      only: /^\d+(\.\d+)?\.\d+\.\d+$/
    branches:
      ignore: /.*/

runOnAllTagsWithQuayIOPullCtx: &runOnAllTagsWithQuayIOPullCtx
  <<: *runOnAllTags
  context: quay-rhacs-eng-readonly

runOnAllTagsWithPushCtx: &runOnAllTagsWithPushCtx
  <<: *runOnAllTags
  context:
  - quay-stackrox-io-readwrite
  - quay-rhacs-eng-readwrite
  - quay-rhacs-eng-readonly

setupGoogleAppCreds: &setupGoogleAppCreds
  run:
    name: Setup GCloud Service Account
    command: |
      touch /tmp/gcp.json
      chmod 0600 /tmp/gcp.json
      echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" >/tmp/gcp.json
      cci-export GOOGLE_APPLICATION_CREDENTIALS /tmp/gcp.json
      gcloud auth activate-service-account --key-file /tmp/gcp.json
      gcloud auth list

loginToGCR: &loginToGCR
  run:
    name: Docker login to gcr.io
    command: docker login -u _json_key --password-stdin \<<<"$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" https://gcr.io
    when: always

configureGit: &configureGit
  run:
    name: Configure git
    command: |
      git config user.email "roxbot@stackrox.com"
      git config user.name "RoxBot"

restoreGoBuildCache: &restoreGoBuildCache
  restore_cache:
    name: Restoring Go build cache
    keys:
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-master-
      - go-cache-v1-{{ .Environment.CIRCLE_JOB }}-
      - go-cache-v1-

saveGoBuildCache: &saveGoBuildCache
  save_cache:
    name: Saving Go build cache
    key: go-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /home/circleci/.cache/go-build

restoreGoLintCache: &restoreGoLintCache
  restore_cache:
    name: Restoring Go lint cache
    keys:
    - go-lint-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    - go-lint-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
    - go-lint-cache-v1-{{ .Environment.CIRCLE_JOB }}-master-
    - go-lint-cache-v1-{{ .Environment.CIRCLE_JOB }}-
    - go-lint-cache-v1-

saveGoLintCache: &saveGoLintCache
  save_cache:
    name: Saving Go lint cache
    key: go-lint-cache-v1-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
    - /home/circleci/.cache/golangci-lint
    - /go/pkg/mod

restoreGoModCache: &restoreGoModCache
  restore_cache:
    name: Restore Go module cache
    keys:
      - rox-go-pkg-mod-v3-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - rox-go-pkg-mod-v3-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - rox-go-pkg-mod-v3-{{ .Environment.CIRCLE_JOB }}-master-
      - rox-go-pkg-mod-v3-{{ .Environment.CIRCLE_JOB }}-
      - rox-go-pkg-mod-v3-

goModTidy: &goModTidy
  run:
    name: Ensure all go modules are present
    command: go mod tidy
    when: always

saveGoModCache: &saveGoModCache
  save_cache:
    name: Saving Go module cache
    key: rox-go-pkg-mod-v3-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /go/pkg/mod

restoreGoBuildBinaryCache: &restoreGoBuildBinaryCache
  restore_cache:
    name: Restoring Go binary build cache
    keys:
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-master-
      - go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-
      - go-binary-cache-v2-

saveGoBuildBinaryCache: &saveGoBuildBinaryCache
  save_cache:
    name: Saving Go binary build cache
    key: go-binary-cache-v2-{{ .Environment.CIRCLE_JOB }}-{{ .Branch }}-{{ .Revision }}
    paths:
      - /home/circleci/go/src/github.com/stackrox/rox/linux-gocache

goModBinaryCacheKey: &goModBinaryCacheKey 'rox-go-binary-pkg-mod-v2-{{ checksum "go.sum" }}'
restoreGoModBinaryCache: &restoreGoModBinaryCache
  restore_cache:
    name: Restore Go module cache
    keys:
      - *goModBinaryCacheKey

saveGoModBinaryCache: &saveGoModBinaryCache
  save_cache:
    name: Saving Go module cache
    key: *goModBinaryCacheKey
    paths:
      - /home/circleci/go/pkg/mod

gradleCacheKey: &gradleCacheKey 'v1-rox-gradle-{{ checksum "qa-tests-backend/build.gradle" }}'
restoreGradle: &restoreGradle
  restore_cache:
    keys:
      - *gradleCacheKey
      - v1-rox-gradle

getECRDockerPullPassword: &getECRDockerPullPassword
  run:
    name: Get AWS ECR Docker Pull Password
    command: |
      set +e
      aws --version
      PASS="$(aws --region="${AWS_ECR_REGISTRY_REGION}" ecr get-login-password)"
      cci-export AWS_ECR_DOCKER_PULL_PASSWORD "${PASS}"
      exit 0

refreshAlpineBaseImage: &refreshAlpineBaseImage
  run:
    name: Refresh base image
    command: docker pull alpine:3.11

setupRoxctl: &setupRoxctl
  run:
    name: Setup Roxctl from bin
    command: |
      source scripts/ci/lib.sh
      install_built_roxctl_in_gopath

setupLicense: &setupLicense
  run:
    name: Creating License Key
    command: |
      if [[ -n "$CIRCLE_TAG" ]] || .circleci/pr_has_label.sh "ci-release-build"; then
        echo "Issuing a CI license for testing the release build"
        ROX_LICENSE_KEY="$(licenses/ci.sh)"
        cci-export ROX_LICENSE_KEY "$ROX_LICENSE_KEY"
      else
        echo "Using no license for CI"
      fi

setupDefaultTLSCerts: &setupDefaultTLSCerts
  run:
    name: Setup default TLS certificates
    command: |
      source ./tests/scripts/setup-certs.sh
      setup_default_TLS_certs

      cci-export ROX_DEFAULT_TLS_CERT_FILE "$ROX_DEFAULT_TLS_CERT_FILE"
      cci-export ROX_DEFAULT_TLS_KEY_FILE "$ROX_DEFAULT_TLS_KEY_FILE"
      cci-export DEFAULT_CA_FILE "$DEFAULT_CA_FILE"
      cci-export ROX_TEST_CA_PEM "$ROX_TEST_CA_PEM"
      cci-export ROX_TEST_CENTRAL_CN "$ROX_TEST_CENTRAL_CN"
      cci-export TRUSTSTORE_PATH "$TRUSTSTORE_PATH"

setupClientTLSCerts: &setupClientTLSCerts
  run:
    name: Setup client auth TLS certificates
    command: |
      source ./tests/scripts/setup-certs.sh
      setup_client_TLS_certs

      cci-export KEYSTORE_PATH "$KEYSTORE_PATH"
      cci-export CLIENT_CA_PATH "$CLIENT_CA_PATH"
      cci-export CLIENT_CERT_PATH "$CLIENT_CERT_PATH"
      cci-export CLIENT_KEY_PATH "$CLIENT_KEY_PATH"

bounceCentral: &bounceCentral
  run:
    name: "Bounce Central"
    command: |
      kubectl -n stackrox delete po $(kubectl -n stackrox get po -l app=central -o=jsonpath='{.items[0].metadata.name}') --grace-period=0

waitForAPI: &waitForAPI
  run:
    name: Wait for the API server to be up
    command: |
      source tests/e2e/lib.sh
      wait_for_api

alwaysWaitForAPI: &alwaysWaitForAPI
  run:
    name: Wait for the API server to be up
    command: |
      source tests/e2e/lib.sh
      wait_for_api
    when: always

waitForSensorK8s: &waitForSensorK8s
  run:
    name: Wait for the Sensor to be running K8s
    command: |
      ./scripts/ci/sensor-wait.sh

downloadDiagnosticBundle: &downloadDiagnosticBundle
  run:
    name: Download diagnostic bundle
    command: |
      mkdir -p /tmp/diagnostic-bundle
      curl_cmd=(curl)
      if [[ -n "$ROX_USERNAME" && -n "$ROX_PASSWORD" ]]; then
        curl_cmd+=(-u "${ROX_USERNAME}:${ROX_PASSWORD}")
      fi
      status_code="$("${curl_cmd[@]}" -sk -o diagnostic.zip -w "%{http_code}\n" "https://${API_ENDPOINT}/api/extensions/diagnostics")"
      if [ "${status_code}" -eq 200 ]; then
        echo "Please find diagnostics bundle in the artifacts section."
        unzip diagnostic.zip -d /tmp/diagnostic-bundle
      else
        echo "Received error status code ${status_code} from the diagnostics endpoint; is the diagnostics handler broken?"
        exit 1
      fi
    when: always

storeDiagnosticBundle: &storeDiagnosticBundle
  ci-artifacts/store:
    path: /tmp/diagnostic-bundle
    destination: diagnostic-bundle

storeQALogs: &storeQALogs
  ci-artifacts/store:
    path: /tmp/qa-tests-backend-logs
    destination: qa-tests-backend-logs

storeProfilingResults: &storeProfilingResults
  ci-artifacts/store:
    path: /tmp/pprof.zip
    destination: pprof.zip

storeQATestResults: &storeQATestResults
  store_test_results:
    path: qa-tests-backend/build/test-results/test

checkQASpockResults: &checkQASpockResults
  run:
    name: Surface any failures in QA tests
    command: |
      jq 'to_entries | map(select((.value.stats.failures > 0) or (.value.stats.errors > 0))) | .[].key' \
        < ./qa-tests-backend/build/spock-reports/aggregated_report.json || true

storeQASpockReports: &storeQASpockReports
  ci-artifacts/store:
    path: ./qa-tests-backend/build/spock-reports
    destination: qa-test-report

backupCentral: &backupCentral
  run:
    name: Backup Central
    command: |
      mkdir -p central-backup
      roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central backup --output central-backup

storeCentralBackupArtifact: &storeCentralBackupArtifact
  ci-artifacts/store:
    path: central-backup
    destination: central-backup

restoreDB: &restoreDB
  run:
    name: Restore DB
    command: |
      if [ -z "${ROX_POSTGRES_DATASTORE}" ] || [ "${ROX_POSTGRES_DATASTORE}" == "false" ]; then
        roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore `ls -t central-backup/stackrox_db_* |head -n1`
      else
        roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" central db restore `ls -t central-backup/postgres_db_* |head -n1`
      fi

getPrometheusMetricParser: &getPrometheusMetricParser
  run:
    name: Get prometheus-metric-parser
    command: |
      export GOPRIVATE=github.com/stackrox
      go install github.com/stackrox/prometheus-metric-parser@latest
      prometheus-metric-parser help

fetchBundleForRemoteClusterInTheDB: &fetchBundleForRemoteClusterInTheDB
  run:
    name: Fetch bundle for the "remote" cluster in the DB
    command: |
      extra_args=()
      if .circleci/pr_has_label.sh ci-race-tests; then
        # builds with -race are very slow at building the bundle (PR #8149)
        extra_args+=(-t 60s)
      fi
      roxctl -e "${API_ENDPOINT}" -p "${ROX_PASSWORD}" "${extra_args[@]}" sensor get-bundle remote
      [[ -d sensor-remote ]]

checkToRunSeparateRaceConditionTestPipe: &checkToRunSeparateRaceConditionTestPipe
  check-to-run:
    run-on-master: true
    run-on-tags: true
    label: ci-race-tests

version: 2.1

parameters:
  # A block of external parameters used when triggering CI out of the standard
  # git push cycle.
  trigger_on_demand:
    type: boolean
    default: false
  workflow_name:
    type: string
    default: ""
  main_image_tag:
    type: string
    default: ""
  main_image_repo:
    type: string
    default: ""
  registry_username:
    type: string
    default: ""
  roxctl_image_repo:
    type: string
    default: ""
  scanner_image:
    type: string
    default: ""
  registry_password:
    type: string
    default: ""
  collector_image_repo:
    type: string
    default: ""
  scanner_db_image:
    type: string
    default: ""
  rox_license_key:
    type: string
    default: ""
  ci_machine_image:
    type: string
    # https://circleci.com/docs/2.0/configuration-reference/#available-machine-images
    default: "ubuntu-2004:202111-02"
  docker_version:
    type: string
    # https://circleci.com/docs/2.0/building-docker-images/#docker-version
    default: "20.10.14"
  release_rc_tag_bash_regex:
    type: string
    # Caution when editing: make sure groups would correspond to BASH_REMATCH use.
    default: '^([[:digit:]]+(\.[[:digit:]]+)*)(-rc\.[[:digit:]]+)?$'
  collector_version:
    type: string
    default: ""

orbs:
  win: circleci/windows@2.2.0
  ci-artifacts: stackrox/ci-artifacts-orb@0.1.1

executors:
  custom:
    parameters:
      resource_class:
        type: string
        default: medium
    resource_class: << parameters.resource_class >>
    docker:
      - image: quay.io/rhacs-eng/apollo-ci:stackrox-test-cci-0.3.42
        auth:
          username: $QUAY_RHACS_ENG_RO_USERNAME
          password: $QUAY_RHACS_ENG_RO_PASSWORD
    working_directory: /go/src/github.com/stackrox/rox

  kubectl-client-1-23:
    parameters:
      resource_class:
        type: string
        default: medium
    resource_class: << parameters.resource_class >>
    docker:
      - image: quay.io/rhacs-eng/apollo-ci:stackrox-test-cci-0.3.35 # do not auto upgrade
        auth:
          username: $QUAY_RHACS_ENG_RO_USERNAME
          password: $QUAY_RHACS_ENG_RO_PASSWORD
    working_directory: /go/src/github.com/stackrox/rox

commands:
  collect-k8s-logs:
    description: "Collect k8s logs and metrics"
    parameters:
      orchestrator-flavor:
        type: string
        default: "k8s"
    steps:
      - run:
          name: Collect k8s logs and metrics
          command: |
            set +e
            echo "Collecting k8s logs"
            ./scripts/ci/collect-service-logs.sh stackrox
            ./scripts/ci/collect-service-logs.sh stackrox-operator
            ./scripts/ci/collect-service-logs.sh kube-system
            ./scripts/ci/collect-service-logs.sh proxies
            ./scripts/ci/collect-service-logs.sh squid
            ./scripts/ci/collect-service-logs.sh squid
            ./scripts/ci/collect-infrastructure-logs.sh
            ./scripts/ci/collect-collector-metrics.sh stackrox

            if [[ "<< parameters.orchestrator-flavor >>" == "openshift" ]]; then
              echo "Collecting openshift logs"
              ./scripts/ci/collect-service-logs.sh openshift-dns
              ./scripts/ci/collect-service-logs.sh openshift-apiserver
              ./scripts/ci/collect-service-logs.sh openshift-authentication
              ./scripts/ci/collect-service-logs.sh openshift-etcd
              ./scripts/ci/collect-service-logs.sh openshift-controller-manager
            fi
          when: always

  build-liveness-check:
    description: "Ensure workflow is still live"
    steps:
      - setup-gcp
      - run:
          name: Ensure workflow is still live
          command: .circleci/check-workflow-live.sh

  check-label-to-skip-tests:
    description: "Checks GitHub for the given label, and skips the tests if we're on a PR, and the label is present."
    parameters:
      label:
        type: string
    steps:
      - run:
          name: Determine whether to skip tests
          command: |
            if .circleci/pr_has_label.sh << parameters.label >> ; then
              echo "Skipping tests because of the presence of the << parameters.label >> label..."
              circleci step halt
            fi

  check-label-exists-or-skip:
    description: "Checks GitHub for the given label, and skips the test if we're on a PR and the label is NOT present."
    parameters:
      label:
        type: string
      run-on-master:
        type: boolean
        default: true
    steps:
      - when:
          condition: << parameters.label >>
          steps:
            - run:
                name: Determine whether to run tests
                command: |
                  set +e
                  if [[ "${CIRCLE_BRANCH}" == "master" && "<< parameters.run-on-master >>" != "true" ]]; then
                     echo "Skipping tests because we're on master, but this test is not on master by default."
                     circleci step halt
                  elif [[ -z "${CIRCLE_TAG}" ]]; then
                    .circleci/pr_has_label.sh "<< parameters.label >>"
                    if [ $? -eq 1 ]; then
                      echo "Skipping tests because we're on a PR. Apply the << parameters.label >> label to your PR if you want to run them."
                      circleci step halt
                    fi
                  fi

  check-to-run:
    description: "If run-on-master is set, run on master. If run-on-tags is set, run on all tags. If the label is set, run when the label is present. If changed-path is set run on changes compared to master in the given path.Otherwise skip."
    parameters:
      label:
        type: string
        default: ""
      run-on-master:
        type: boolean
      run-on-tags:
        type: boolean
      changed-path:
        type: string
        default: ""
      changed-path-ignore:
        type: string
        default: ""
    steps:
    - run:
        name: Determine whether to run stages
        command: |
          if [[ "<< parameters.run-on-master >>" == "true" && "${CIRCLE_BRANCH}" == "master" ]]; then
            echo "Running tests because we are on master"
            exit 0
          fi

          if [[ "<< parameters.run-on-tags >>" == "true" && -n "${CIRCLE_TAG}" ]]; then
            echo "Running tests because this is a tagged build"
            exit 0
          fi

          if [[ -n "<< parameters.label >>" ]]; then
            # Split on commas
            IFS=', ' ;for label in `echo "<< parameters.label >>"`; do
              if .circleci/pr_has_label.sh "${label}"; then
                echo "Running tests because the label << parameters.label >> has been set"
                exit 0
              fi
            done
          fi

          if [[ -n "<< parameters.changed-path >>" || -n "<< parameters.changed-path-ignore>>" ]]; then
            if [[ "$CIRCLE_BRANCH" == "master" ]]; then
              echo "On master"
              diff_base="HEAD^"
            else
              echo "Not on master"
              diff_base="$(git merge-base HEAD origin/master)"
            fi
            echo "Determined diff-base as ${diff_base}"
            echo "Master SHA: $(git rev-parse origin/master)"
            echo "Diffbase diff:"
            { git diff --name-only "${diff_base}" | cat ; } || true
            ignored_regex='<< parameters.changed-path-ignore >>'
            [[ -n "$ignored_regex" ]] || ignored_regex='$^' # regex that matches nothing
            match_regex='<< parameters.changed-path >>'
            [[ -n "$match_regex" ]] || match_regex='^.*$' # egrep -q '' returns 0 even on empty input, so we have to specify some pattern
            if egrep -q "$match_regex" < <({ git diff --name-only "${diff_base}" || echo "???" ; } | egrep -v "$ignored_regex"); then
              echo "Running tests because paths matching $match_regex (and not matching ${ignored_regex}) had changed."
              exit 0
            fi
          fi

          echo "Not running tests"
          circleci step halt

  gate-job:
    description: Decide whether to run a job
    parameters:
      job:
        type: string
    steps:
      - run:
          name: Decide whether to run << parameters.job >>
          command: |
            source scripts/ci/lib.sh
            # gate_job() exits with 0 when a job should *not* be run in keeping
            # with OpenShift CI semantics.
            exitstatus=0
            (gate_job '<< parameters.job >>'; exit 1) || exitstatus="$?"
            if [[ "$exitstatus" == "0" ]]; then
              echo "Not running tests"
              circleci step halt
            fi

  get-and-store-debug-dump:
    description: Runs roxctl debug dump and stores it for the job.
    steps:
      - run:
          name: Pull the dump from Central
          command: |
            source scripts/ci/lib.sh
            get_central_debug_dump debug-dump
      - ci-artifacts/store:
          path: debug-dump
          destination: debug

  get-and-store-diagnostic-bundle:
    description: Runs roxctl diagnostic bundle and stores it for the job.
    steps:
      - run:
          name: Pull the diagnostic bundle from Central
          command: |
            source scripts/ci/lib.sh
            get_central_diagnostics diagnostic-bundle
      - ci-artifacts/store:
          path: diagnostic-bundle
          destination: diagnostic

  get-central-data:
    description: Pull data from Central
    parameters:
      destination:
        type: string
        default: central-data
    steps:
      - run:
          name: Pull data from Central
          command: |
            ./scripts/grab-data-from-central.sh << parameters.destination >>
            ls -l << parameters.destination >>
          when: always
      - ci-artifacts/store:
          path: << parameters.destination >>
          destination: << parameters.destination >>

  store-k8s-logs:
    parameters:
      destination:
        type: string
        default: k8s-service-logs
    description: Stores StackRox and cluster logs as artifacts
    steps:
    - ci-artifacts/store:
        path: /tmp/k8s-service-logs
        destination: << parameters.destination >>

  check-stackrox-logs:
    parameters:
      collect:
        # In most jobs the logs are already grabbed by collect-k8s-logs
        type: boolean
        default: false
    description: Checks StackRox logs to ensure all services operated normally
    steps:
      - when:
          condition: << parameters.collect >>
          steps:
            - run:
                name: Collect the StackRox logs
                command: |
                  ./scripts/ci/collect-service-logs.sh stackrox
      - run:
          name: Verify that StackRox services did not restart during the runs
          command: |
            if [[ ! -d "/tmp/k8s-service-logs/stackrox/pods" ]]; then
              echo >&2 "StackRox logs were not collected. (Use collect: true or collect-k8s-logs.)"
              exit 1
            fi
            previous_logs=$(ls /tmp/k8s-service-logs/stackrox/pods/*-previous.log || true)
            if [[ -n "${previous_logs}" ]]; then
                echo >&2 "Previous logs found"
                if ! scripts/ci/logcheck/check-restart-logs.sh "${CIRCLE_JOB}" ${previous_logs}; then
                    exit 1
                fi
            fi
          when: always
      - run:
          name: Verify that StackRox service logs contain no suspicious entries
          command: |
            if [[ ! -d "/tmp/k8s-service-logs/stackrox/pods" ]]; then
              echo >&2 "StackRox logs were not collected. (Use collect: true or collect-k8s-logs.)"
              exit 1
            fi
            logs=$(ls /tmp/k8s-service-logs/stackrox/pods/*.log)
            filtered=$(ls ${logs} | grep -v "previous.log" || true)
            if [[ -n "${filtered}" ]]; then
                if ! scripts/ci/logcheck/check.sh ${filtered}; then
                    echo >&2 "Found at least one suspicious log file entry."
                    exit 1
                fi
            fi
          when: always

  setup-egress-proxies:
    steps:
      - run:
          name: Set up egress proxies (if desired)
          command: |
            if ! .circleci/pr_has_label.sh ci-rox-egress-proxies; then
              echo "Not setting up egress proxies"
              exit 0
            fi

            kubectl -n squid apply -R -f tests/proxy/deployment/ || true  # might fail because of SCC
            configs=(tests/proxy/config/*.yaml)

            selected_config="${configs[$((RANDOM % ${#configs[@]}))]}"
            echo "Selected configuration file ${selected_config}"
            kubectl -n stackrox apply -f "${selected_config}"

            echo "Waiting for 3min for proxy config to be picked up"
            for i in {1..180}; do
              echo -n .
              sleep 1
            done
            echo

            if [[ "$CIRCLE_JOB" == "openshift-crio-api-e2e-tests" ]]; then
              echo "Skipping egress isolation on CRI-O due to network policy issues."
              exit 0
            fi

            echo "Isolating pods"
            kubectl -n stackrox apply -R -f tests/proxy/netpol/

  checkout-with-submodules:
    steps:
    - checkout

    - add_ssh_keys:
        fingerprints:
        - "f1:20:e3:c4:eb:26:37:6f:ca:36:e0:39:cd:6b:95:ae"

    - run:
        name: Add SSH key of github.com
        command: |
          ssh-keyscan -H github.com >> ~/.ssh/known_hosts

    - run:
        name: Checkout submodules
        command: |
          git submodule update --init

  setup-go-build-env:
    steps:
      - run:
          name: Setup Go build environment
          command: |
            if .circleci/pr_has_label.sh "ci-release-build"; then
              cci-export GOTAGS release
            fi

  restore-go-mod-cache:
    steps:
      - *restoreGoModCache
      - *goModTidy

  setup-dep-env:
    parameters:
      docker-login:
        type: boolean
        default: true

      use-websocket:
        type: boolean
        default: false

    steps:
      - run:
          name: Setup deployment env
          command: |
            source scripts/ci/lib.sh
            setup_deployment_env "<< parameters.docker-login >>" "<< parameters.use-websocket >>"

  setup-gcp:
    steps:
      - run:
          name: Setup GCP env
          command: |
            source scripts/ci/gcp.sh
            setup_gcp

  teardown-gke:
    steps:
      - run:
          name: Tear down GKE cluster
          command: |
            source scripts/ci/gke.sh
            teardown_gke_cluster

          when: always

  continue-when-in-a-pr-context:
    steps:
      - run:
          name: Continue when in a PR context
          command: |
            if [[ -n "${CIRCLE_PULL_REQUEST:-}" ]]; then
              echo "This is a PR, will continue with the remaining steps in this job"
              exit 0
            fi
            echo "This is not a PR, will skip the remaining steps in this job"
            circleci step halt

  poll-for-builds-when-not-in-a-pr-context:
    steps:
      - run:
          name: Poll for builds from OpenShift CI when not in a PR context
          command: |
            if [[ -n "${CIRCLE_PULL_REQUEST:-}" ]]; then
              echo "This is a PR, will continue with the remaining steps in this job"
              exit 0
            fi
            source scripts/ci/lib.sh
            poll_for_system_test_images 3600
            echo "The required images were built"

  pre-build-ui:
    parameters:
      branding:
        type: string
        default: stackrox
    steps:
      - checkout
      - continue-when-in-a-pr-context
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox
      - restore-npm-deps-cache
      - fetch-ui-deps

      - run:
          name: Prepare UI tree state file
          command: |
            git log -n 1 --pretty=format:%H -- ui/ >/tmp/ui-tree-state
            # Add the node version to the UI tree state, since a different node version
            # should bust the cache.
            node -v >>/tmp/ui-tree-state

      - restore_cache:
          name: Restoring UI build cache
          key: ui-build-cache-v1-<< parameters.branding >>-{{ checksum "/tmp/ui-tree-state" }}

      - run:
          name: Build UI
          command: |
            if [[ -d ui/build ]]; then
              exit 0  # build cache is always complete
            fi
            make -C ui build

      - save_cache:
          name: Saving UI build cache
          key: ui-build-cache-v1-<< parameters.branding >>-{{ checksum "/tmp/ui-tree-state" }}
          paths:
            - ui/build

      - save-npm-deps-cache

      - run:
          name: Separate the UI brand
          command: |
            mv ui ui-<< parameters.branding >>

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - ui-<< parameters.branding >>/build # Copied directly into the image downstream.
            - ui-<< parameters.branding >>/deps
            - ui-<< parameters.branding >>/node_modules # node_modules dirs here and below are used for OSS license auditing downstream
            - ui-<< parameters.branding >>/apps/*/node_modules
            - ui-<< parameters.branding >>/packages/*/node_modules

      - cleanup-clusters-on-fail

  build:
    steps:
      - checkout-with-submodules
      - poll-for-builds-when-not-in-a-pr-context
      - continue-when-in-a-pr-context

      - setup_remote_docker:
          version: << pipeline.parameters.docker_version >>
      - setup-dep-env:
          docker-login: true

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Ensure product branding is set
          command: |
            if [[ -z "${ROX_PRODUCT_BRANDING}" ]]; then
              echo >&2 "Will not proceed until ROX_PRODUCT_BRANDING env variable is defined"
              exit 2
            fi

      - run:
          name: Restore the branded UI
          command: |
            cp -au ui-${ROX_PRODUCT_BRANDING}/* ui

      - *refreshAlpineBaseImage

      - restore-go-mod-cache
      - setup-go-build-env

      - run:
          name: Generate OSS notice
          command: |
            make ossls-notice

      - run:
          name: Build main image
          command: |
            make docker-build-main-image

      - run:
          name: Check debugger presence in the main image
          command: make check-debugger

      - run:
          name: Build roxctl image
          command: make docker-build-roxctl-image

      - run:
          name: Push new Docker image
          command: |
            source ./scripts/ci/lib.sh
            push_main_image_set "${CIRCLE_BRANCH}" "${ROX_PRODUCT_BRANDING}"

      - run:
          name: Push collector & scanner images tagged with main-version to registries
          command: |
            source ./scripts/ci/lib.sh
            push_matching_collector_scanner_images "${ROX_PRODUCT_BRANDING}"

      - run:
          name: Publish new versions of NPM packages if any
          command: |
            if [[ "${CIRCLE_BRANCH}" == "master" && "${ROX_PRODUCT_BRANDING}" == "STACKROX_BRANDING" ]]; then
              echo "//npm.pkg.github.com/:_authToken=\"$GITHUB_PACKAGES_PUBLISH_TOKEN\"" > ~/.npmrc
              make ui-publish-packages
            else
              echo "Not on master and not the StackRox brand, skipping publishing NPM packages."
            fi

      - *saveGoModCache

      - ci-artifacts/store:
          path: bin/linux/roxctl
          destination: roxctl/roxctl-linux

      - ci-artifacts/store:
          path: bin/darwin/roxctl
          destination: roxctl/roxctl-darwin

      - run:
          name: Comment on PR
          command: |
            if [[ "${ROX_PRODUCT_BRANDING}" == "STACKROX_BRANDING" ]]; then
              wget --quiet https://github.com/joshdk/hub-comment/releases/download/0.1.0-rc6/hub-comment_linux_amd64
              echo "2a2640f44737873dfe30da0d5b8453419d48a494f277a70fd9108e4204fc4a53 hub-comment_linux_amd64" | sha256sum -c -
              sudo install hub-comment_linux_amd64 /usr/bin/hub-comment

              export TAG=$(make --quiet tag)
              hub-comment -template-file .circleci/comment-template.tpl
            fi

      - cleanup-clusters-on-fail

  cleanup-clusters-on-fail:
    steps:
      # This only runs if the build has passed, thus short-circuiting the steps below.
      # The reason "when: on_fail" is not sufficient is that attach_workspace seems to always
      # run, and unnecessarily takes up space.
      - run:
          name: Halt if the build has passed
          command: circleci step halt

      - run:
          name: Ensure GCloud is configured
          command: |
            gcloud auth activate-service-account --key-file <(echo "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX")
            gcloud auth list

            gcloud config set project stackrox-ci
            gcloud config set compute/region us-central1
            gcloud config unset compute/zone
            gcloud config set core/disable_prompts True
          when: on_fail

      - run:
          name: Record step as fatal failure
          command: |
            gsutil cp - "gs://stackrox-ci-status/workflows/${CIRCLE_WORKFLOW_ID}/fatal-failures/${CIRCLE_JOB}" \
              </dev/null
          when: on_fail

      - run:
          name: Delete all GKE clusters for this CI run
          # The command is inline here instead of invoking a script so we can even run it
          # when checkout failed due to a force push.
          command: |
            echo "Deleting all GKE clusters for workflow $CIRCLE_WORKFLOW_ID"

            while IFS='' read -r line || [[ -n "$line" ]]; do
            	[[ -n "$line" ]] || continue

            	if [[ "$line" =~ ^([^[:space:]]+)[[:space:]]+([^[:space:]]+)$ ]]; then
            		cluster="${BASH_REMATCH[1]}"
            		zone="${BASH_REMATCH[2]}"
            	else
            		echo >&2 "Could not parse line $line"
            	fi

            	echo "Deleting cluster ${cluster} in zone ${zone} ..."
            	gcloud container clusters delete "$cluster" \
            		--async \
            		--project stackrox-ci \
            		--zone "$zone" \
            		--quiet || true

            done < <(
            	gcloud container clusters list \
            		--project stackrox-ci \
            		--filter "resourceLabels.stackrox-ci-workflow=${CIRCLE_WORKFLOW_ID}" \
            		--format 'csv[no-heading,separator=" "](name,zone)'
            	)

          when: on_fail

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *loginToGCR
      - run:
          name: Destroy EKS cluster
          command: |
            if [ -z "${CIRCLE_TAG}" ]; then
              if ! .circleci/pr_has_label.sh ci-eks-tests; then
                echo "EKS Cluster not created, skipping destroy."
                exit 0
              fi
            fi
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

      - destroy-aks-cluster:
          when: on_fail

      - destroy-openshift-4-cluster:
          when: on_fail

  destroy-aks-cluster:
    parameters:
      when:
        type: enum
        enum: ["on_fail", "always"]

    steps:
      - run:
          name: Destroy AKS cluster
          environment:
            - AKS_AUTOMATION_VERSION: 0.1.7
            - AZURE_RESOURCE_GROUP: stackrox-ci
          command: |
            if [ -z "${CIRCLE_TAG}" ]; then
              if ! .circleci/pr_has_label.sh ci-aks-tests; then
                echo "AKS Cluster not created, skipping destroy."
                exit 0
              fi
            fi
            if [ ! -d "$PWD/aks" ]; then
              echo "AKS Cluster not created (due to missing artifacts), skipping destroy."
              exit 0
            fi
            docker run --rm -t \
              -e AZURE_SP_USERNAME -e AZURE_SP_PASSWORD -e AZURE_SP_TENANT \
              gcr.io/stackrox-infra/automation-flavors/aks:$AKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}" "${AZURE_RESOURCE_GROUP}"
          when: << parameters.when >>

  destroy-openshift-4-cluster:
    parameters:
      when:
        type: enum
        enum: ["on_fail", "always"]
      cluster-name:
        type: string
        default: "openshift-4"

    steps:
      - run:
          name: Pack openshift volume for remote docker
          command: |
            docker create -v /data --name openshift alpine:3.9 /bin/true
            docker cp << parameters.cluster-name >>/data/. openshift:/data
          when: << parameters.when >>

      - run:
          name: Destroy OpenShift 4 cluster
          environment:
            - OPENSHIFT_4_AUTOMATION_VERSION: 0.2.26
          command: |
            if [ ! -d "$PWD/<< parameters.cluster-name >>" ]; then
              echo "OpenShift 4 Cluster not created (due to missing artifacts), skipping destroy."
              exit 0
            fi
            docker run --rm -t \
              --volumes-from openshift \
              -e GOOGLE_CREDENTIALS="${GOOGLE_OPENSHIFT_4_CREDENTIALS}" \
              gcr.io/stackrox-infra/automation-flavors/openshift-4:$OPENSHIFT_4_AUTOMATION_VERSION destroy
          when: << parameters.when >>

  destroy-rosa-cluster:
    description: "Destroy ROSA cluster"
    steps:
      - attach_workspace:
          at: /home/circleci
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Destroy ROSA cluster
          when: always
          command: |
            docker create -v /data --name artifacts alpine:3.9 /bin/true
            docker cp "$PWD/artifacts/." artifacts:/data
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              gcr.io/stackrox-infra/automation-flavors/openshift-rosa:0.2.17 \
              destroy

  destroy-aro-cluster:
    description: "Destroy ARO cluster"
    steps:
      - attach_workspace:
          at: /home/circleci
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Destroy ARO cluster
          when: always
          command: |
            docker create -v /data --name artifacts alpine:3.9 /bin/true
            docker cp "$PWD/artifacts/." artifacts:/data
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e AZURE_SP_SECRET_VAL="$AZURE_SP_SECRET_VAL" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              gcr.io/stackrox-infra/automation-flavors/openshift-aro:0.2.17 \
              destroy

  destroy-osd-aws-cluster:
    description: "Destroy osd-on-aws cluster"
    steps:
      - attach_workspace:
          at: /home/circleci
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Destroy osd-on-aws cluster
          when: always
          command: |
            docker create -v /data --name artifacts alpine:3.9 /bin/true
            docker cp "$PWD/artifacts/." artifacts:/data
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              gcr.io/stackrox-infra/automation-flavors/openshift-osd:0.2.26 \
              aws destroy

  destroy-osd-gcp-cluster:
    description: "Destroy osd-on-gcp cluster"
    steps:
      - attach_workspace:
          at: /home/circleci
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Destroy osd-on-gcp cluster
          when: always
          command: |
            docker create -v /data --name artifacts alpine:3.9 /bin/true
            docker cp "$PWD/artifacts/." artifacts:/data
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e GCP_SA_CREDS_JSON_BASE64="$GCP_SA_CREDS_JSON_BASE64" \
              -e GCP_PROJECT="srox-temp-dev-test" \
              gcr.io/stackrox-infra/automation-flavors/openshift-osd:0.2.26 \
              gcp destroy


  # ROSA and ARO teardown steps are not being run at the end of 'run-qa-tests'
  # as expected, and troubleshooting is time prohibitive. So rather than passing
  # teardown steps to 'run-qa-tests' I am passing a 'no-operation' and adding
  # the steps just after the call to 'run-qa-tests'. This means the cluster
  # destroy command is run under the api-e2e test context and not the 'run-qa-test'
  # context.
  nop-steps-stanza:
    description: "nop-steps-stanza"
    steps:
      - run:
          name: nop
          command: echo &>/dev/null

  provision-gke-cluster:
    parameters:
      cluster-id:
        type: string
      num-nodes:
        type: integer
        default: 3
      machine-type:
        type: string
        default: e2-standard-4

    steps:
      - run:
          name: Provision a GKE cluster
          command: |
            source scripts/ci/gke.sh
            provision_gke_cluster "<< parameters.cluster-id >>" "<< parameters.num-nodes >>" "<< parameters.machine-type >>"

      - run:
          name: Save cluster config
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            mkdir -p "$CONFIG_DIR"
            echo "$CLUSTER_NAME" >>"${CONFIG_DIR}/name"
            gcloud config get-value compute/zone >>"${CONFIG_DIR}/zone"
            cp .circleci/gke-refresh-token-forever.sh "${CONFIG_DIR}/refresh-token-forever.sh"

      - build-liveness-check

      - run:
          name: Tear down cluster upon failure
          command: |
            gcloud container clusters delete "$CLUSTER_NAME" --async
          when: on_fail

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - .ci-clusters/<< parameters.cluster-id >>

  attach-gke-cluster:
    parameters:
      cluster-id:
        type: string

    steps:
      - run:
          name: Restore config for << parameters.cluster-id >> cluster
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            export CLUSTER_NAME="$(cat "${CONFIG_DIR}/name")"
            [[ -n "$CLUSTER_NAME" ]]
            export ZONE="$(cat "${CONFIG_DIR}/zone")"
            [[ -n "$ZONE" ]]
            gcloud config set compute/zone "$ZONE"
            cmd=(gcloud container clusters get-credentials --project stackrox-ci --zone "$ZONE" "$CLUSTER_NAME")
            "${cmd[@]}"
            echo "Restored config for cluster ${CLUSTER_NAME}"
            cci-export CLUSTER_NAME "$CLUSTER_NAME"
            cci-export ZONE "$ZONE"
            echo
            echo "Run the following command to attach to the cluster:"
            echo
            printf " %q" "${cmd[@]}"
            echo

      - run:
          name: Running refresh token script in the background
          command: |
            CONFIG_DIR="/go/src/github.com/stackrox/rox/.ci-clusters/<< parameters.cluster-id >>"
            if [[ ! -x "${CONFIG_DIR}/refresh-token-forever.sh" ]]; then
              exit 0
            fi
            "${CONFIG_DIR}/refresh-token-forever.sh"
          background: true

      - run:
          name: Waiting for << parameters.cluster-id >> cluster to stabilize
          command: |
            source scripts/ci/gke.sh
            wait_for_cluster

  remove-existing-stackrox-resources:
    steps:
      - run:
          name: Remove all the existing StackRox resources (they can exist when Circle restarts a container)
          command: |
            source tests/e2e/lib.sh
            remove_existing_stackrox_resources

  deploy-webhook-server:
    steps:
      - run:
          name: Deploy Webhook server
          command: |
            certs_dir="$(mktemp -d)"
            ./scripts/ci/create-webhookserver.sh "${certs_dir}"
            GENERIC_WEBHOOK_SERVER_CA_CONTENTS="$(cat "${certs_dir}/ca.crt")"
            cci-export GENERIC_WEBHOOK_SERVER_CA_CONTENTS "${GENERIC_WEBHOOK_SERVER_CA_CONTENTS}"

  deploy-default-psp:
    steps:
      - run:
          name: Deploy Default PSP for stackrox namespace
          command: |
            ./scripts/ci/create-default-psp.sh

  deploy-clair:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Deploy Clair Scanner
                command: |
                  ./scripts/ci/clair/deploy.sh qa-clair
                  cci-export CLAIR_ENDPOINT "http://clairsvc.qa-clair:6060"
                background: true

  teardown-clair:
    parameters:
      cluster-id:
        type: string
    steps:
      - when:
          condition:
            equal: ["api-e2e-tests", << parameters.cluster-id >>]
          steps:
            - run:
                name: Teardown Clair Scanner
                command: |
                  ./scripts/ci/clair/teardown.sh qa-clair

  run-qa-tests:
    parameters:
      cluster-id:
        type: string

      determine-whether-to-run:
        type: steps

      sensor-deploy-flavor:
        type: string
        default: kubectl

      orchestrator-flavor:
        type: string
        default: k8s

      use-websocket:
        type: boolean
        default: false

      is-gke-cluster:
        type: boolean
        default: true

      connect-to-cluster:
        type: steps
        default: []

      destroy-cluster:
        type: steps
        default:
          - teardown-gke

    steps:
      - checkout
      - check-backend-changes

      - steps: << parameters.determine-whether-to-run >>

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - restore-go-mod-cache

      - *setupRoxctl
      - setup-gcp
      - setup-dep-env:
          use-websocket: << parameters.use-websocket >>
      - when:
          condition: << parameters.is-gke-cluster >>
          steps:
            - attach-gke-cluster:
                cluster-id: << parameters.cluster-id >>
      - unless:
          condition: << parameters.is-gke-cluster >>
          steps: << parameters.connect-to-cluster >>
      - remove-existing-stackrox-resources
      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-clair:
          cluster-id: << parameters.cluster-id >>

      - deploy-stackrox:
          sensor-deploy-flavor: << parameters.sensor-deploy-flavor >>
          orchestrator-flavor: << parameters.orchestrator-flavor >>

          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      - deploy-default-psp
      - deploy-webhook-server

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform Part 1
          no_output_timeout: 15m
          command: |
            if [[ "<< parameters.orchestrator-flavor >>" == "openshift" ]]; then
              export CLUSTER=OPENSHIFT
              oc get scc qatest-anyuid || oc create -f "qa-tests-backend/src/k8s/scc-qatest-anyuid.yaml"
            else
              export CLUSTER=K8S
            fi
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              echo "On nightly tag, running all QA tests but failing fast..."
              make -C qa-tests-backend test FAIL_FAST=true || touch FAIL
            elif [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
              echo "On master, running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            elif .circleci/pr_has_label.sh ci-all-qa-tests; then
              echo "ci-all-qa-tests label was specified, so running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            else
              echo "On a PR branch, running BAT tests only..."
              make -C qa-tests-backend bat-test || touch FAIL
            fi

      - collect-k8s-logs
      - store-k8s-logs:
          destination: k8s-service-logs-part-1
      - get-and-store-debug-dump
      - get-and-store-diagnostic-bundle
      - get-central-data
      - *storeQATestResults
      - *checkQASpockResults

      - run:
          name: QA Automation Platform Part 2
          no_output_timeout: 15m
          command: |
            if [[ "<< parameters.orchestrator-flavor >>" == "openshift" ]]; then
              export CLUSTER=OPENSHIFT
            else
              export CLUSTER=K8S
            fi
            make -C qa-tests-backend sensor-bounce-test

      - run:
          name: Test for failure in Part 1
          command: |
            [[ ! -f FAIL ]]

      - save_cache:
          key: *gradleCacheKey
          paths:
            - ~/.gradle/caches/

      - *storeQATestResults
      - *storeQASpockReports

      - collect-k8s-logs:
          orchestrator-flavor: << parameters.orchestrator-flavor >>
      - store-k8s-logs:
          destination: k8s-service-logs-part-2
      - get-and-store-debug-dump
      - get-and-store-diagnostic-bundle
      - get-central-data:
          destination: central-data-part2
      - *backupCentral
      - *storeCentralBackupArtifact
      - *restoreDB
      - check-stackrox-logs
      - *storeQALogs

      - teardown-clair:
          cluster-id: << parameters.cluster-id >>

      - steps: << parameters.destroy-cluster >>

  # Use this in place of 'run-qa-tests' to troubleshoot cluster bringup and
  # teardown without waiting for real tests to run. Parameter signature should
  # always match that of 'run-qa-tests'.
  run-qa-tests-fake:
    parameters:
      cluster-id:
        type: string
      determine-whether-to-run:
        type: steps
      sensor-deploy-flavor:
        type: string
        default: kubectl
      orchestrator-flavor:
        type: string
        default: k8s
      use-websocket:
        type: boolean
        default: false
      is-gke-cluster:
        type: boolean
        default: true
      connect-to-cluster:
        type: steps
        default: []
      destroy-cluster:
        type: steps
        default:
          - teardown-gke
    steps:
      - checkout
      - check-backend-changes
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox
      - run:
          name: QA Automation Platform
          command: echo "there are no tests defined since this is a fake"
      - steps: << parameters.destroy-cluster >>

  provision-openshift-cluster:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""
      crio:
        type: boolean
        default: false
      version:
        type: string

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - run:
          name: Install kubectl
          working_directory: /tmp
          command: |
            wget https://storage.googleapis.com/kubernetes-release/release/v1.11.2/bin/linux/amd64/kubectl
            sudo install kubectl /usr/bin

      - *loginToGCR
      - run:
          name: Create cloud resources
          command: |
            mkdir -m 777 -p openshift
            set +e
            try=0
            max_attempts=2
            while [[ ${try} -lt ${max_attempts} ]]; do
                try=$((try + 1))
                echo "============= Openshift provision try ${try} ============="
                docker run --rm -t -v "$PWD/openshift:/well-known" \
                  -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
                  "gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >>" \
                  create \
                    --name="${CIRCLE_WORKFLOW_ID:0:7}<< parameters.suffix >>" \
                    --crio="<< parameters.crio >>" \
                    --creation-source=ci \
                    --gcp-project=stackrox-ci \
                    --dns-project=stackrox-ci \
                    --dns-zone=openshift-ci-rox-systems \
                    --zone=us-central1-a \
                    2>&1 | scripts/ci/monitor-output.sh 60 && sudo chmod -R +r openshift
                rc=$?
                if [[ ${rc} == 0 ]]; then
                    echo "============= Openshift provision succeeded ============="
                    break
                fi
                echo "============= Openshift provision failed on try ${try} ============="
                if [[ ${try} -lt ${max_attempts} ]]; then
                    docker run --rm -t -v "$PWD/openshift:/well-known" \
                      -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
                      "gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >>" \
                      destroy
                fi
            done
            if [[ $rc != 0 ]]; then
                exit $rc
            fi
            set -e

      - ci-artifacts/store:
          path: openshift
          destination: openshift

      - run:
          name: Configure Kubeconfig
          command: |
            ls -lh $PWD/openshift
            export KUBECONFIG="$PWD/openshift/artifacts/config"
            kubectl get nodes -o wide

      - ci-artifacts/store:
          path: openshift
          destination: openshift

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Create alias for OpenShift data directory
                command: |
                  cp -r openshift/ openshift<< parameters.suffix >>/

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - openshift<< parameters.suffix >>

      - destroy-openshift-cluster:
          when: on_fail
          version: << parameters.version >>

  destroy-openshift-cluster:
    parameters:
      version:
        type: string
      when:
        type: enum
        enum: ["on_fail", "always"]

    steps:
      - run:
          name: Pack openshift volume
          when: << parameters.when >>
          command: |
            docker create -v /well-known --name openshift alpine:3.9 /bin/true
            docker cp openshift/artifacts openshift:/well-known/artifacts

      - *loginToGCR
      - run:
          name: Destroy Openshift cluster
          when: << parameters.when >>
          command: |
            docker run --rm -t \
              --volumes-from openshift \
              -e GOOGLE_CREDENTIALS="$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX" \
              gcr.io/stackrox-infra/automation-flavors/openshift-multi:<< parameters.version >> \
              destroy

  run-openshift-tests:
    parameters:
      suffix:
        type: string
        default: ""
      gate-label:
        type: string
        default: ""
      version:
        type: string
      sensor-deploy-flavor:
        type: string
        default: kubectl

    steps:
      - checkout
      - check-backend-changes

      - check-label-exists-or-skip:
          label: << parameters.gate-label >>

      - setup_remote_docker

      - setup-dep-env:
          docker-login: true

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - when:
          condition: << parameters.suffix >>
          steps:
            - run:
                name: Copy openshift data directory to canonical location
                command: |
                  [[ ! -d openshift ]] || rm -rf openshift
                  cp -r "openshift<< parameters.suffix >>" openshift

      - run:
          name: Sanity check OpenShift Env
          command: |
            pwd
            ls -lh $PWD/openshift
            export KUBECONFIG=$PWD/openshift/artifacts/config
            SUCCESS=0
            for i in {1..30}; do
             if oc get nodes; then
               SUCCESS=1
               break
             fi
             echo "Failed sanity check OpenShift. Retrying in 5 seconds"
             sleep 5
            done
            if (( !SUCCESS )); then
              echo "Sanity check failed"
              exit 1
            fi

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export KUBECONFIG "$PWD/openshift/artifacts/config"
            cci-export CLAIRIFY_IMAGE_TAG 0.5.2
            cci-export OPENSHIFT_HOST "$(cat openshift/artifacts/master)"
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$QUAY_RHACS_ENG_RO_PASSWORD"
            cci-export REGISTRY_USERNAME "$QUAY_RHACS_ENG_RO_USERNAME"

            REPO=rhacs-eng
            cci-export MAIN_IMAGE_REPO "quay.io/$REPO/main"
            cci-export CENTRAL_DB_IMAGE_REPO "quay.io/$REPO/central-db"
            cci-export COLLECTOR_IMAGE_REPO "quay.io/$REPO/collector"
            cci-export SCANNER_IMAGE "quay.io/$REPO/scanner:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"
            cci-export SCANNER_DB_IMAGE "quay.io/$REPO/scanner-db:$(cat "$(git rev-parse --show-toplevel)/SCANNER_VERSION")"

      - remove-existing-stackrox-resources
      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          orchestrator-flavor: openshift
          sensor-deploy-flavor: << parameters.sensor-deploy-flavor >>
          validate-autoupgrade-label: true
          post-central-deploy-steps:
            - *setupClientTLSCerts
            - setup-egress-proxies

      # OpenShift 3.11 networking needs a kick to ensure sensor is reachable (ROX-7869)
      - run:
          name: Kick OpenShift 3.11
          command: |
            oc -n stackrox label --overwrite svc sensor kicked="$(date +'%s')"

      - deploy-webhook-server

      - *getECRDockerPullPassword
      - *restoreGradle
      - restore-go-mod-cache
      - run:
          name: QA Automation Platform Part 1
          no_output_timeout: 15m
          command: |
            export CLUSTER=OPENSHIFT
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              echo "On nightly tag, running all QA tests but failing fast..."
              make -C qa-tests-backend test FAIL_FAST=true || touch FAIL
            elif [[ "${CIRCLE_BRANCH}" == "master" || -n "${CIRCLE_TAG}" ]]; then
              echo "On master, running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            elif .circleci/pr_has_label.sh ci-all-qa-tests; then
              echo "ci-all-qa-tests label was specified, so running all QA tests..."
              make -C qa-tests-backend test || touch FAIL
            else
              echo "On a PR branch, running BAT tests only..."
              make -C qa-tests-backend bat-test || touch FAIL
            fi

      - collect-k8s-logs:
          orchestrator-flavor: openshift
      - store-k8s-logs:
          destination: k8s-service-logs-part-1
      - get-and-store-debug-dump
      - get-and-store-diagnostic-bundle
      - get-central-data

      - *storeQATestResults
      - *checkQASpockResults

      - run:
          name: QA Automation Platform Part 2
          command: |
            export CLUSTER=OPENSHIFT
            export API_HOSTNAME=localhost
            export API_PORT="${LOCAL_PORT}"
            make -C qa-tests-backend sensor-bounce-test

      - run:
          name: Check for failures in part 1
          command: |
            [[ ! -f FAIL ]]

      - *waitForSensorK8s

      - *storeQATestResults
      - *storeQASpockReports

      - *backupCentral
      - *storeCentralBackupArtifact

      - collect-k8s-logs:
          orchestrator-flavor: openshift
      - store-k8s-logs:
          destination: k8s-service-logs-part-2
      - get-and-store-debug-dump
      - get-and-store-diagnostic-bundle
      - get-central-data:
          destination: central-data-part2
      - check-stackrox-logs
      - *storeQALogs

      - destroy-openshift-cluster:
          when: always
          version: << parameters.version >>

  deploy-stackrox:
    parameters:
      sensor-deploy-flavor:
        type: string
        default: kubectl
      orchestrator-flavor:
        type: string
        default: k8s
      validate-autoupgrade-label:
        type: boolean
        default: false
      post-central-deploy-steps:
        type: steps
        default: []
      validate-sensor-bundle:
        type: boolean
        default: true

    steps:
      - run:
          name: Deploy central to remote cluster
          command: |
            # If we're running a nightly build or race condition check, then set CGO_CHECKS=true so that central is
            # deployed with strict checks
            if .circleci/is-nightly-tag.sh || .circleci/pr_has_label.sh ci-race-tests; then
                cci-export CGO_CHECKS "true"
            fi

            if .circleci/pr_has_label.sh ci-race-tests; then
                cci-export IS_RACE_BUILD "true"
            fi

            if [[ -z "${OUTPUT_FORMAT:-}" ]]; then
              if .circleci/pr_has_label.sh ci-helm-deploy; then
                export OUTPUT_FORMAT=helm
              fi
            fi

            if [[ -n "${POD_SECURITY_POLICIES:-}" ]]; then
              cci-export POD_SECURITY_POLICIES "$POD_SECURITY_POLICIES"
            fi

            deploy_dir="deploy/<< parameters.orchestrator-flavor >>"
            "${deploy_dir}/central.sh"

            source scripts/k8s/export-basic-auth-creds.sh "$deploy_dir"
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"

      - steps: << parameters.post-central-deploy-steps >>

      - *waitForAPI

      - run:
          name: Snapshot Kubernetes Resources
          command: |
            <<# parameters.validate-autoupgrade-label >>
            scripts/ci/sensorbundle-label/list-resources.sh >/tmp/k8s-resources-pre-sensor
            <</ parameters.validate-autoupgrade-label >>
            exit 0

      - run:
          name: Deploy Sensor
          command: |
            export ROX_AFTERGLOW_PERIOD=15
            if [[ "<< parameters.sensor-deploy-flavor >>" == "helm" ]]; then
              echo "Deploying Sensor using Helm ..."
              export SENSOR_HELM_DEPLOY=true
              export ADMISSION_CONTROLLER=true
            else
              echo "Deploying sensor using kubectl ... "
              if [[ -n "${IS_RACE_BUILD}" ]]; then
                # builds with -race are slow at generating the sensor bundle
                # https://stack-rox.atlassian.net/browse/ROX-6987
                export ROXCTL_TIMEOUT=60s
              fi
            fi

            deploy/<< parameters.orchestrator-flavor >>/sensor.sh

            if [[ "<< parameters.orchestrator-flavor >>" == "openshift" ]]; then
              # Sensor is CPU starved under OpenShift causing all manner of test failures:
              # https://stack-rox.atlassian.net/browse/ROX-5334
              # https://stack-rox.atlassian.net/browse/ROX-6891
              # et al.
              kubectl -n stackrox set resources deploy/sensor -c sensor --requests 'cpu=2' --limits 'cpu=4'
            fi

      - when:
          condition: << parameters.validate-sensor-bundle >>
          steps:
            - run:
                name: Validate Sensor bundle (upgrader)
                command: |
                  if [[ "<< parameters.sensor-deploy-flavor >>" == "kubectl" && "$SENSOR_HELM_DEPLOY" != "true" ]]; then
                    kubectl proxy --port 28001 &
                    proxy_pid=$!
                    sleep 5
                    KUBECONFIG="$(pwd)/scripts/ci/kube-api-proxy/config.yml" \
                      bin/linux/upgrader \
                        -kube-config kubectl \
                        -local-bundle deploy/<< parameters.orchestrator-flavor >>/sensor-deploy \
                        -workflow validate-bundle
                    kill "$proxy_pid"
                  else
                    echo "Skipping for helm sensor deploy...  "
                  fi

      - *waitForSensorK8s

      - run:
          name: Bounce collectors to avoid restarts on initial module pull
          command: |
            kubectl -n stackrox delete pod -l app=collector --grace-period=0

      - *waitForSensorK8s

      - run:
          name: Verify all new resources have the auto-upgrade label
          command: |
            <<# parameters.validate-autoupgrade-label >>
            {
              scripts/ci/sensorbundle-label/list-resources.sh
              cat /tmp/k8s-resources-pre-sensor
            } | sort | uniq -u >/tmp/k8s-new-resources-post-sensor

            scripts/ci/sensorbundle-label/list-resources.sh 'auto-upgrade.stackrox.io/component=sensor' \
                >/tmp/k8s-sensor-labeled-resources

            {
              cat /tmp/k8s-new-resources-post-sensor
              cat /tmp/k8s-sensor-labeled-resources
              cat /tmp/k8s-sensor-labeled-resources
              cat scripts/ci/sensorbundle-label/ignorelist 2>/dev/null
              cat scripts/ci/sensorbundle-label/ignorelist 2>/dev/null
            } | sort | uniq -u >/tmp/k8s-new-resources-unlabeled

            if [[ -s /tmp/k8s-new-resources-unlabeled ]]; then
              echo >&2 "The following resources were created by the sensor bundle, but don't carry the sensor bundle label:"
              cat >&2 /tmp/k8s-new-resources-unlabeled
              exit 1
            fi
            <</ parameters.validate-autoupgrade-label >>
            exit 0

  check-backend-changes:
    steps:
      - check-to-run:
          run-on-master: false
          run-on-tags: true
          changed-path-ignore: '^ui/'

  compare-with-stored-metrics:
    parameters:
      cluster-flavor:
        type: string
      test:
        type: string
    steps:
      - run:
          name: Compare with stored metrics
          command: |
            set -x
            GS_PATH=gs://stackrox-ci-metrics/<< parameters.test >>/<< parameters.cluster-flavor >>
            COMPARE_WITH=$(gsutil ls "${GS_PATH}"/stackrox_debug\* | sort | tail -1)
            echo "Using ${COMPARE_WITH} as metrics for comparison"
            mkdir /tmp/metrics
            gsutil cp "${COMPARE_WITH}" /tmp/metrics
            COMPARE_WITH=$(ls -1 /tmp/metrics | sort | tail -1)
            THIS_RUN=$(echo ${PWD}/debug-dump/stackrox_debug*.zip)
            echo "Comparing with ${THIS_RUN}"
            COMPARE=${PWD}/scripts/ci/compare-debug-metrics.sh
            set +e
            cd /tmp/metrics && "${COMPARE}" "${COMPARE_WITH}" "${THIS_RUN}"
            # ignore failure exit from comparison
            exit 0

  create-concatenated-ui-monorepo-lock:
    description: "For UI monorepo concatenate all package.json files and yarn.lock to a single file"
    parameters:
      output-filename:
        type: string
    steps:
      - run:
          name: Concatenate package.json files and yarn.lock
          command: |
            find ui/ -type d \( -name node_modules \) -prune -false -o -name package.json -print0 | sort -z | xargs -r0 cat > << parameters.output-filename >>
            cat ui/yarn.lock >> << parameters.output-filename >>

  save-npm-deps-cache:
    description: "Prepare and save NPM dependencies cache"
    steps:
      - create-concatenated-ui-monorepo-lock:
          output-filename: ui/monorepo.lock

      - run:
          name: Calculate lock file digest
          command: |
            openssl dgst -r -sha256 ui/monorepo.lock >ui/monorepo.lock.sha256

      - save_cache:
          name: Save NPM dependencies cache
          key: v8-rox-npm-deps-{{ checksum "ui/monorepo.lock" }}
          paths:
            - ui/monorepo.lock.sha256
            - ui/deps
            - ~/.cache/Cypress # Cypress binary will be put there, see https://docs.cypress.io/guides/guides/continuous-integration.html#Example-circle-yml-v2-config-file-with-yarn
            - ui/node_modules # CircleCI doesn't support globbing here https://circleci.com/docs/2.0/caching/#basic-example-of-dependency-caching so all `node_nodules` need to be listed
            - ui/apps/platform/node_modules
            - ui/packages/ui-components/node_modules
            - ui/packages/tailwind-config/node_modules

  restore-npm-deps-cache:
    description: "Restore NPM dependencies cache"
    steps:
      - create-concatenated-ui-monorepo-lock:
          output-filename: ui/monorepo.lock

      - restore_cache:
          name: Restore NPM dependencies cache
          keys:
            - v8-rox-npm-deps-{{ checksum "ui/monorepo.lock" }}
            - v8-rox-npm-deps-

  fetch-ui-deps:
    description: "Fetch UI dependencies"
    steps:
      - run:
          name: Fetch dependencies
          command: |
            if [[ -d ui/node_modules && -f ui/monorepo.lock.sha256 && "$(cat ui/monorepo.lock.sha256)" == "$(openssl dgst -r -sha256 ui/monorepo.lock)" ]]; then
              exit 0  # cache is complete if the above condition matches
            fi
            make -C ui deps

  store-test-results-and-artifacts:
    description: "Store test results"
    parameters:
      path:
        type: string
    steps:
      - store_test_results:
          path: << parameters.path >>
      - ci-artifacts/store:
          path: << parameters.path >>

  pre-go-unit-tests:
    description: "Common pre go test steps"
    steps:
      - checkout
      - check-to-run:
          run-on-master: true
          run-on-tags: true
          changed-path-ignore: "^ui/"
      - restore-go-mod-cache
      - setup-go-build-env
      - *restoreGoBuildCache

  post-go-unit-tests:
    description: "Common post go test steps"
    parameters:
      path:
        type: string
    steps:
      - *saveGoBuildCache

      - run:
          name: Generate JUnit reports
          command: |
            make generate-junit-reports
            cp -a junit-reports << parameters.path >>
          when: always

      - store-test-results-and-artifacts:
          path: << parameters.path >>

  create-openshift-4-cluster:
    description: "Create an OpenShift 4 cluster"
    parameters:
      cluster-name:
        type: string
      openshift-version:
        type: string
        default: ocp/stable-4.7
    steps:
      - run:
          name: Create OpenShift 4 cluster
          environment:
          - OPENSHIFT_4_AUTOMATION_VERSION: 0.2.26
          - GCP_PROJECT: stackrox-ci
          command: |
            mkdir $PWD/<< parameters.cluster-name >>
            docker run --name openshift-4-automation -t \
              -v $PWD/<< parameters.cluster-name >>:/data \
              -e GOOGLE_CREDENTIALS="${GOOGLE_OPENSHIFT_4_CREDENTIALS}" \
              -e PULL_SECRET="${REDHAT_PULL_SECRET}" \
              -e REGION="us-central1" \
              -e OPENSHIFT_VERSION=<< parameters.openshift-version >> \
              gcr.io/stackrox-infra/automation-flavors/openshift-4:$OPENSHIFT_4_AUTOMATION_VERSION \
              create << parameters.cluster-name >>-"${CIRCLE_WORKFLOW_ID:0:7}" "${GCP_PROJECT}" openshift.ci.rox.systems
            docker cp openshift-4-automation:/data $PWD/<< parameters.cluster-name >>
          no_output_timeout: 60m

  openshift-ci-mimic-env:
    description: "Simulate an OpenShift CI Environment"
    steps:
      - run:
          name: Simulate an OpenShift CI Environment
          command: |
            # Create a fake OpenShift Env
            cci-export GCLOUD_SERVICE_ACCOUNT_OPENSHIFT_CI_ROX "$GCLOUD_SERVICE_ACCOUNT_CIRCLECI_ROX"
            cci-export BUILD_ID "$CIRCLE_BUILD_NUM"
            cci-export JOB_NAME "$CIRCLE_JOB"
            cci-export REPO_NAME "$CIRCLE_PROJECT_REPONAME"
            cci-export PULL_PULL_SHA "$CIRCLE_SHA1"
            cci-export OPENSHIFT_CI true
            cci-export ARTIFACT_DIR "/tmp/artifact_dir"
            mkdir -p "$ARTIFACT_DIR"

jobs:
  ###
  # Jobs independent from build / compilation (like style checks, static code analysis, unit tests)
  ###

  slack-notify:
    executor: custom
    resource_class: small
    steps:
      - run:
          name: Send Slack notification with Workflow link
          command: |
            # Cannot use .circleci/is-nightly-tag.sh because this job doesn't have access to git
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              webhook_url="$NIGHTLY_WORKFLOW_NOTIFY_WEBHOOK"
            elif [[ "$CIRCLE_TAG" =~ << pipeline.parameters.release_rc_tag_bash_regex >> ]]; then
              webhook_url="$RELEASE_WORKFLOW_NOTIFY_WEBHOOK"
            else
              exit 0
            fi

            jq -n \
              --arg workflow_id "$CIRCLE_WORKFLOW_ID" \
              --arg tag "$CIRCLE_TAG" \
              '{"text": "CircleCI build for tag `\($tag)` started! Check the status of the build under the following URL: https://circleci.com/workflow-run/\($workflow_id)"}' \
            | curl -XPOST -d @- -H 'Content-Type: application/json' "$webhook_url"

  go-postgres-tests:
    docker:
      - image: quay.io/rhacs-eng/apollo-ci:stackrox-test-0.3.42
        auth:
          username: $QUAY_RHACS_ENG_RO_USERNAME
          password: $QUAY_RHACS_ENG_RO_PASSWORD
      - image: postgres:14.2-alpine
        environment:
          POSTGRES_PASSWORD: password
    resource_class: xlarge
    environment:
      ROX_IMAGE_FLAVOR: stackrox.io
    steps:
    - pre-go-unit-tests
    - run:
        name: Run Go postgres unit tests
        command: |
          make go-postgres-unit-tests
    - post-go-unit-tests:
        path: go-unit-release-junit-reports

  designated-image-tests:

    ###
    # This job is intended to run qa-backend-tests against images that are released
    # without a full rox CI run. e.g. the DSOP images that are repackaged with a
    # different base image.
    ##

    # To trigger a run you might do something like:
    #
    #   curl -X POST --header 'Content-Type: application/json' \
    #        --header "Circle-Token: <your CI token>" https://circleci.com/api/v2/project/github/stackrox/stackrox/pipeline -d '{
    #     "branch": "3.0.50.x",
    #     "parameters": {
    #         "trigger_on_demand": true,
    #         "workflow_name": "designated_image_test",
    #         "main_image_tag": "3.0.50.1",
    #         "main_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/main",
    #         "roxctl_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/main",
    #         "scanner_db_image": "registry1.dsop.io/ironbank/stackrox/stackrox/scanner-db:2.5.0",
    #         "scanner_image": "registry1.dsop.io/ironbank/stackrox/stackrox/scanner:2.5.0",
    #         "collector_image_repo": "registry1.dsop.io/ironbank/stackrox/stackrox/collector",
    #         "registry_username": "<your DSOP registry1 username>",
    #         "registry_password": "<token from registry1>",
    #         "rox_license_key": "<a valid ROX license>"
    #     }
    # }'

    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "false"
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - LOAD_BALANCER: none
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_DEV_AUTH0_CLIENT_SECRET: ""
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m

    steps:
      - checkout

      - run:
          name: Parameter check
          command: |
            some_missing=0

            function check_and_set() {
                local name="$1"
                local value="$2"
                if [[ -z "${value}" ]]; then
                    echo "Parameter ${name} is missing"
                    some_missing=1
                else
                    cci-export "$(echo "$name" | tr a-z A-Z)" "${value}"
                fi
            }

            check_and_set "main_image_tag" "<< pipeline.parameters.main_image_tag >>"
            check_and_set "main_image_repo" "<< pipeline.parameters.main_image_repo >>"
            check_and_set "roxctl_image_repo" "<< pipeline.parameters.roxctl_image_repo >>"
            check_and_set "scanner_db_image" "<< pipeline.parameters.scanner_db_image >>"
            check_and_set "scanner_image" "<< pipeline.parameters.scanner_image >>"
            check_and_set "collector_image_repo" "<< pipeline.parameters.collector_image_repo >>"
            check_and_set "registry_username" "<< pipeline.parameters.registry_username >>"
            check_and_set "registry_password" "<< pipeline.parameters.registry_password >>"
            check_and_set "rox_license_key" "<< pipeline.parameters.rox_license_key >>"

            if (( some_missing )); then
                echo "Some required parameters are missing. Exiting."
                exit 1
            fi

      - provision-gke-cluster:
          cluster-id: "designated-image-tests"

      - setup_remote_docker

      - *setupGoogleAppCreds

      - *setupDefaultTLSCerts

      - run:
          name: Get roxctl from released artifacts
          command: |
            gsutil cp "gs://sr-roxc/${MAIN_IMAGE_TAG}/bin/linux/roxctl" "$GOPATH/bin/roxctl"
            chmod +x "$GOPATH/bin/roxctl"
            roxctl version

      - deploy-stackrox:
          validate-sensor-bundle: false
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-default-psp
      - deploy-webhook-server

      - *getECRDockerPullPassword
      - *restoreGradle
      - run:
          name: QA Automation Platform
          command: |
            export REGISTRY_USERNAME="${QUAY_RHACS_ENG_RO_USERNAME}"
            export REGISTRY_PASSWORD="${QUAY_RHACS_ENG_RO_PASSWORD}"
            export CLUSTER=K8S
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            make -C qa-tests-backend bat-test

      - *storeQATestResults
      - *storeQASpockReports

      - collect-k8s-logs
      - *backupCentral
      - *storeCentralBackupArtifact
      - check-stackrox-logs
      - store-k8s-logs
      - *storeQALogs

      # - teardown-gke

  ###
  # Build jobs
  ###

  pre-create-parameters:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - *configureGit
      - run:
          name: Allow a custom Collector version
          command: |
            if [[ -n "<< pipeline.parameters.collector_version >>" ]]; then
              echo "Using a custom collector version: << pipeline.parameters.collector_version >>"
              echo "<< pipeline.parameters.collector_version >>" > COLLECTOR_VERSION
              # Ensure that this CI pipeline has a unique tag and that it does
              # not overwrite images built from the original git ref.
              git commit -m 'avoid overwrite of existing image' COLLECTOR_VERSION
            else
              echo "A custom collector version was not set"
            fi
      - run:
          name: Creating a consistent tag for CI workflow
          command: |
            make --quiet tag | tee CI_TAG_TMP
            mv CI_TAG_TMP CI_TAG
      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - CI_TAG
            - COLLECTOR_VERSION

  pre-build-ui-stackrox:
    executor: custom
    resource_class: large
    environment:
      EKS_AUTOMATION_VERSION: 0.2.17
    steps:
      - pre-build-ui:
          branding: STACKROX_BRANDING

  pre-build-ui-rhacs:
    executor: custom
    resource_class: large
    environment:
      EKS_AUTOMATION_VERSION: 0.2.17
      REACT_APP_ROX_PRODUCT_BRANDING: RHACS_BRANDING
    steps:
      - pre-build-ui:
          branding: RHACS_BRANDING

  pre-build-cli:
    executor: custom
    resource_class: medium
    environment:
      - EKS_AUTOMATION_VERSION: 0.2.17
    steps:
      - checkout
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox
      - restore-go-mod-cache
      - setup-go-build-env
      - *restoreGoBuildCache
      - run:
          name: Build the CLI
          command: make cli

      - run:
          name: Validate expected Go version
          command: |
            ./scripts/ci/lib.sh validate_expected_go_version

      - *saveGoModCache

      - *saveGoBuildCache

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - bin/linux/roxctl
            - bin/darwin/roxctl
            - bin/windows/roxctl.exe

      - cleanup-clusters-on-fail

  pre-build-go-binaries:
    machine:
      image: << pipeline.parameters.ci_machine_image >>
    working_directory: ~/go/src/github.com/stackrox/rox
    environment:
      GOPATH: /home/circleci/go
      EKS_AUTOMATION_VERSION: 0.2.17
    resource_class: large
    steps:
      - checkout
      - attach_workspace:
          at: ~/go/src/github.com/stackrox/rox
      - run:
          name: Create cci-export utility and setup env
          command: |
            sudo mv /bin/bash /bin/real-bash
            sudo cp .circleci/bash-wrapper /bin/bash
      - run:
          name: Install Go
          command: |
            # We need to install go here to pull go modules, even though the build itself is dockerized.
            sudo rm -rf /usr/local/go
            go_version="$(cat EXPECTED_GO_VERSION)" # This is already prefixed with `go`.
            wget -c "https://storage.googleapis.com/golang/${go_version}.linux-amd64.tar.gz"
            sudo tar -C /usr/local -xvzf "${go_version}.linux-amd64.tar.gz"
            cci-export PATH $PATH:$GOPATH/bin
      - *restoreGoModBinaryCache
      - setup-go-build-env
      - *restoreGoBuildBinaryCache
      - setup-dep-env:
          docker-login: true
      - run:
          name: Build the main Go binaries
          command: |
            if .circleci/pr_has_label.sh ci-race-tests; then
              RACE=true make main-build
            else
              make main-build
            fi

      - *saveGoBuildBinaryCache
      - *saveGoModBinaryCache

      - run:
          name: Generate the swagger docs
          command: make swagger-docs

      - persist_to_workspace:
          root: ~/go/src/github.com/stackrox/rox
          paths:
            - bin/linux/central
            - bin/linux/migrator
            - bin/linux/kubernetes
            - bin/linux/admission-control
            - bin/linux/upgrader
            - bin/linux/collection
            - image/docs # This will go into the image as generated docs.
            - image/keys
            - deps # Used to speed up k8s-tests
            - generated # Used to speed up k8s-tests

      - cleanup-clusters-on-fail

  build-stackrox:
    executor: custom
    resource_class: medium
    environment:
      EKS_AUTOMATION_VERSION: 0.2.17
      ROX_PRODUCT_BRANDING: "STACKROX_BRANDING"
    steps:
      - build

  build-rhacs:
    executor: custom
    resource_class: medium
    environment:
      EKS_AUTOMATION_VERSION: 0.2.17
      ROX_PRODUCT_BRANDING: "RHACS_BRANDING"
    steps:
      - build

  build-race-condition-debug-image:
    executor: custom
    resource_class: large
    steps:
      - checkout-with-submodules
      - *checkToRunSeparateRaceConditionTestPipe

      - run:
          name: Override the main image tag with a '-rcd' suffix
          command: |
            TAG="$(make --quiet tag)"
            echo "${TAG}-rcd" > CI_TAG
            make --quiet tag

      - setup_remote_docker:
          version: << pipeline.parameters.docker_version >>

      - restore-go-mod-cache
      - setup-go-build-env

      - restore-npm-deps-cache
      - fetch-ui-deps

      - run:
          name: Generate OSS notice
          command: |
            make ossls-notice

      - setup-dep-env:
          docker-login: true

      - run:
          name: Build main image with race condition checks
          command: |
            make RACE=true CIRCLE_TAG="$(make --quiet tag)" main-image

      - run:
          name: Push new main-image with '-rcd' suffix
          command: |
            TAG="$(make --quiet tag)"

            docker login -u "$QUAY_RHACS_ENG_RW_USERNAME" --password-stdin \<<<"$QUAY_RHACS_ENG_RW_PASSWORD" quay.io
            docker tag "stackrox/main:${TAG}" "quay.io/rhacs-eng/main:${TAG}"
            ./scripts/ci/push-as-manifest-list.sh "quay.io/rhacs-eng/main:${TAG}" | cat

      - run:
          # Apparently builds for race-condition-tests get GOTAGS=release due to explicitly set CIRCLE_TAG value.
          # This results in the release image flavor set. Release image flavor expects unified image tags for collector,
          # scanner and scanner-db images. Here we ensure that such images with unified tags exist.
          name: Retag scanner, scanner-db and collector images with '-rcd' suffix
          command: |
            docker login -u "$QUAY_RHACS_ENG_RW_USERNAME" --password-stdin \<<<"$QUAY_RHACS_ENG_RW_PASSWORD" quay.io
            MAIN_TAG="$(make --quiet tag)"
            SCANNER_VERSION="$(make --quiet scanner-tag)"
            COLLECTOR_VERSION="$(make --quiet collector-tag)"
            ./scripts/ci/pull-retag-push.sh "quay.io/rhacs-eng/scanner:${SCANNER_VERSION}"    "quay.io/rhacs-eng/scanner:${MAIN_TAG}"
            ./scripts/ci/pull-retag-push.sh "quay.io/rhacs-eng/scanner-db:${SCANNER_VERSION}" "quay.io/rhacs-eng/scanner-db:${MAIN_TAG}"
            ./scripts/ci/pull-retag-push.sh "quay.io/rhacs-eng/collector:${COLLECTOR_VERSION}"      "quay.io/rhacs-eng/collector:${MAIN_TAG}"
            ./scripts/ci/pull-retag-push.sh "quay.io/rhacs-eng/collector:${COLLECTOR_VERSION}-slim" "quay.io/rhacs-eng/collector-slim:${MAIN_TAG}"

      - persist_to_workspace:
          root: /go/src/github.com/stackrox/rox
          paths:
            - CI_TAG
            - bin/linux/roxctl

  build-operator:
    executor: custom
    resource_class: large
    steps:
      - checkout

      - setup_remote_docker:
          # Default docker server version isn't new enough therefore we're asking for more recent one to enjoy Buildkit.
          version: << pipeline.parameters.docker_version >>
      - run:
          name: Login to Docker Hub and Quay.io
          command: |
            docker login -u "$QUAY_RHACS_ENG_RW_USERNAME" --password-stdin \<<<"$QUAY_RHACS_ENG_RW_PASSWORD" quay.io

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - restore-go-mod-cache
      - setup-go-build-env

      - run:
          name: Build and push images for quay.io/rhacs-eng
          command: |
            ROX_PRODUCT_BRANDING=RHACS_BRANDING make -C operator/ everything

      - run:
          name: Check that Operator image is runnable - quay.io/rhacs-eng
          command: |
            docker run --rm quay.io/rhacs-eng/stackrox-operator:$(make --quiet -C operator tag) --help

  ###
  # Jobs that build and provision test resources (like cluster provisioning)
  # Recommended naming format: provision-{test_job_name}
  # Note: cluster provisioning job names used as resource label values, they must contain only letters and '-'
  ###

  provision-gke-postgres-api-e2e-tests:
    executor: custom
    resource_class: small
    environment:
      - GCP_IMAGE_TYPE: "COS"
      - POD_SECURITY_POLICIES: "false"
    steps:
      - checkout
      - check-backend-changes
      - check-to-run:
          label: ci-postgres-tests
          run-on-master: true
          run-on-tags: false
      - provision-gke-cluster:
          cluster-id: postgres-api-e2e-tests

  provision-gke-kernel-api-e2e-tests:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - check-backend-changes

      - check-to-run:
          label: ci-kernel-module-tests
          run-on-master: false
          run-on-tags: true

      - provision-gke-cluster:
          cluster-id: kernel-api-e2e-tests

  build-scale-monitoring-and-mock-server:
    executor: custom
    resource_class: medium
    environment:
      - EKS_AUTOMATION_VERSION: 0.2.17
    steps:
      - checkout
      - setup_remote_docker

      - *refreshAlpineBaseImage

      - restore-go-mod-cache
      - setup-go-build-env
      - *restoreGoBuildCache

      - setup-dep-env:
          docker-login: true

      - run:
          name: Build images
          command: |
            make scale-image mock-grpc-server-image

      - *saveGoBuildCache

      - run:
          name: Push new Docker image
          command: |
            docker login -u "$QUAY_RHACS_ENG_RW_USERNAME" --password-stdin \<<<"$QUAY_RHACS_ENG_RW_PASSWORD" quay.io

            TAG=$(make --quiet tag)

            QUAY_REPO="rhacs-eng"
            for img in scale grpc-server; do
              ./scripts/ci/push-as-manifest-list.sh "quay.io/${QUAY_REPO}/${img}:${TAG}" | cat
            done

      - cleanup-clusters-on-fail

  provision-gke-api-scale-tests:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - check-backend-changes
      - check-label-exists-or-skip:
          label: ci-scale-tests
          run-on-master: false
      - provision-gke-cluster:
          cluster-id: scale-test
          machine-type: e2-standard-8

  provision-gke-postgres-api-scale-tests:
    executor: custom
    resource_class: small
    steps:
    - checkout
    - check-backend-changes
    - check-to-run:
        label: ci-postgres-scale-tests
        run-on-master: false
        run-on-tags: false
    - provision-gke-cluster:
        cluster-id: postgres-scale-test
        machine-type: e2-standard-16

  provision-openshift-api-e2e-tests:
    machine:
      image: << pipeline.parameters.ci_machine_image >>
    steps:
      - check-label-exists-or-skip:
          label: ci-openshift-tests
          run-on-master: false
      - provision-openshift-cluster:
          crio: false
          gate-label: ci-openshift-tests
          version: 0.2.11-1-geb3414898d

  provision-openshift-crio-api-e2e-tests:
    machine:
      image: << pipeline.parameters.ci_machine_image >>
    steps:
      - check-label-exists-or-skip:
          label: ci-openshift-crio-tests
          run-on-master: false
      - provision-openshift-cluster:
          crio: true
          gate-label: ci-openshift-crio-tests
          suffix: -crio
          version: 0.2.11-1-geb3414898d

  provision-eks-api-e2e-tests:
    executor: kubectl-client-1-23
    resource_class: small
    environment:
      - EKS_AUTOMATION_VERSION: 0.2.17
    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-eks-tests
          run-on-master: false

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *loginToGCR
      - run:
          name: Create EKS cluster
          command: |
            mkdir $PWD/eks
            docker run --name eks-automation -t \
              -v $PWD/eks:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              -e CREATION_SOURCE=CI \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION create "${CIRCLE_WORKFLOW_ID:0:7}"
            docker cp eks-automation:/data $PWD/eks

      - run:
          name: Validate Kubeconfig
          command: |
            ls -lh $PWD/eks/data
            export KUBECONFIG=$PWD/eks/data/eks-kube.yaml
            sudo chown "$USER" $PWD/eks/data/eks-kube.yaml
            kubectl get nodes

      - ci-artifacts/store:
          path: eks
          destination: eks

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - eks

      - run:
          name: Destroy EKS cluster
          command: |
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: on_fail

  provision-aks-api-e2e-tests:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-aks-tests
          run-on-master: false

      - setup_remote_docker

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *loginToGCR
      - run:
          name: Create AKS cluster
          environment:
            AKS_AUTOMATION_VERSION: "0.1.7"
            AZURE_RESOURCE_GROUP: "stackrox-ci"
            CREATION_SOURCE: "ci"
          command: |
            mkdir $PWD/aks
            docker run --name aks-automation -t \
              -v $PWD/aks:/data \
              -e AZURE_SP_USERNAME -e AZURE_SP_PASSWORD -e AZURE_SP_TENANT -e CREATION_SOURCE -e K8S_VERSION \
              gcr.io/stackrox-infra/automation-flavors/aks:$AKS_AUTOMATION_VERSION create "${CIRCLE_WORKFLOW_ID:0:7}" "${AZURE_RESOURCE_GROUP}"
            docker cp aks-automation:/data $PWD/aks

      - run:
          name: Validate Kubeconfig
          command: |
            ls -lh $PWD/aks/data
            export KUBECONFIG=$PWD/aks/data/kubeconfig
            sudo chown "$USER" $PWD/aks/data/kubeconfig
            kubectl get nodes

      - ci-artifacts/store:
          path: aks
          destination: aks

      - build-liveness-check

      - persist_to_workspace:
          root: .
          paths:
            - aks

      - destroy-aks-cluster:
          when: on_fail

  provision-openshift-4-operator-e2e-tests:
    executor: custom
    resource_class: small
    steps:
    - checkout
    - check-to-run:
        label: ci-openshift-4-operator-tests
        run-on-master: true
        run-on-tags: true
        changed-path: '^operator/'
    - check-backend-changes

    - setup_remote_docker

    - attach_workspace:
        at: /go/src/github.com/stackrox/rox

    - *loginToGCR
    - create-openshift-4-cluster:
        cluster-name: operator-oc4

    - run:
        name: Validate Kubeconfig
        command: |
          ls -lh $PWD/operator-oc4/data
          export KUBECONFIG=$PWD/operator-oc4/data/auth/kubeconfig
          sudo chown "$USER" $PWD/operator-oc4/data/auth/kubeconfig
          oc get nodes

    - ci-artifacts/store:
        path: operator-oc4
        destination: operator-oc4

    - build-liveness-check

    - persist_to_workspace:
        root: .
        paths:
        - operator-oc4

    - destroy-openshift-4-cluster:
        when: on_fail
        cluster-name: operator-oc4

  provision-openshift-4-6-operator-e2e-tests:
    executor: custom
    resource_class: small
    steps:
    - checkout
    - check-to-run:
        label: ci-openshift-4-operator-tests
        run-on-master: true
        run-on-tags: true
        changed-path: '^operator/'
    - check-backend-changes

    - setup_remote_docker

    - attach_workspace:
        at: /go/src/github.com/stackrox/rox

    - *loginToGCR
    - create-openshift-4-cluster:
        cluster-name: operator-oc4-6
        openshift-version: ocp/4.6.54

    - run:
        name: Validate Kubeconfig
        command: |
          ls -lh $PWD/operator-oc4-6/data
          export KUBECONFIG=$PWD/operator-oc4-6/data/auth/kubeconfig
          sudo chown "$USER" $PWD/operator-oc4-6/data/auth/kubeconfig
          oc get nodes

    - ci-artifacts/store:
        path: operator-oc4-6
        destination: operator-oc4-6

    - build-liveness-check

    - persist_to_workspace:
        root: .
        paths:
        - operator-oc4-6

    - destroy-openshift-4-cluster:
        when: on_fail
        cluster-name: operator-oc4-6

  provision-race-condition-tests:
    executor: custom
    resource_class: small
    environment:
      GCP_IMAGE_TYPE: "COS"
      POD_SECURITY_POLICIES: "false"
    steps:
      - checkout
      - check-backend-changes
      - *checkToRunSeparateRaceConditionTestPipe

      - provision-gke-cluster:
          cluster-id: race-condition-tests

  ###
  # Test jobs against built artifacts
  # Recommended naming format: {cluster_type}-[{flavor}-]{test_target_type}-{any_test_name}-tests
  #   e.g. 'openshift-crio-api-e2e-tests'
  ###

  gke-postgres-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - GCP_IMAGE_TYPE: "COS"
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_POSTGRES_DATASTORE: true

    steps:
      - run-qa-tests:
          cluster-id: postgres-api-e2e-tests
          sensor-deploy-flavor: helm
          determine-whether-to-run:
            - check-to-run:
                label: ci-postgres-tests
                run-on-master: true
                run-on-tags: false
          use-websocket: true

  gke-kernel-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - OUTPUT_FORMAT: helm
      - SENSOR_HELM_DEPLOY: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m

    steps:
      - run-qa-tests:
          cluster-id: kernel-api-e2e-tests
          determine-whether-to-run:
            - check-to-run:
                label: ci-kernel-module-tests
                run-on-master: false
                run-on-tags: true

  gke-api-scale-tests:
    executor: custom
    environment:
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - STORAGE: pvc
      - STORAGE_CLASS: faster
      - STORAGE_SIZE: 100
      - LOAD_BALANCER: lb
      - OUTPUT_FORMAT: helm

    steps:
      - checkout
      - check-backend-changes
      - check-label-exists-or-skip:
          label: ci-scale-tests
          run-on-master: false
      - setup_remote_docker
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - *setupRoxctl
      - setup-gcp
      - setup-dep-env
      - attach-gke-cluster:
          cluster-id: scale-test
      - remove-existing-stackrox-resources

      - *setupGoogleAppCreds
      - *setupLicense

      - run:
          name: Launch Sensor and Central with scale workload
          command: |
            ./deploy/k8s/deploy.sh
            source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
            cci-export ROX_USERNAME "$ROX_USERNAME"
            cci-export ROX_PASSWORD "$ROX_PASSWORD"
            ./scale/launch_workload.sh scale-test

      - *waitForAPI

      - run:
          name: Collect scale results
          command: |
            mkdir /tmp/pprof
            # 45 min run so that we are confident that the run has completely finished
            ./scale/profiler/pprof.sh /tmp/pprof "${API_ENDPOINT}" 45
            zip -r /tmp/pprof.zip /tmp/pprof

      - *storeProfilingResults
      - collect-k8s-logs
      - get-and-store-debug-dump
      - get-and-store-diagnostic-bundle
      - get-central-data

      - *getPrometheusMetricParser
      - compare-with-stored-metrics:
          cluster-flavor: gke
          test: scale-test
      - run:
          name: Store metrics
          command: |
            if [[ "$CIRCLE_TAG" =~ .*-nightly-.* ]]; then
              THIS_RUN=$(echo debug-dump/stackrox_debug*.zip)
              GS_PATH=gs://stackrox-ci-metrics/scale-test/gke/
              gsutil cp "${THIS_RUN}" "${GS_PATH}"
              unzip -d debug-dump/stackrox_debug "${THIS_RUN}"
              prometheus-metric-parser single --file=debug-dump/stackrox_debug/metrics-2 \
                --format=gcp-monitoring --labels='Test=ci-scale-test,ClusterFlavor=gke' \
                --project-id=stackrox-ci --timestamp=$(date -u +"%s")
            fi

      - check-stackrox-logs
      - store-k8s-logs

      - teardown-gke

  gke-postgres-api-scale-tests:
    executor: custom
    environment:
    - MONITORING_SUPPORT: false
    - SCANNER_SUPPORT: true
    - STORAGE: pvc
    - STORAGE_CLASS: faster
    - STORAGE_SIZE: 100
    - LOAD_BALANCER: lb
    - ROX_POSTGRES_DATASTORE: true

    steps:
    - checkout
    - check-backend-changes
    - check-to-run:
        label: ci-postgres-scale-tests
        run-on-master: false
        run-on-tags: false
    - setup_remote_docker
    - attach_workspace:
        at: /go/src/github.com/stackrox/rox

    - *setupRoxctl
    - setup-gcp
    - setup-dep-env
    - attach-gke-cluster:
        cluster-id: postgres-scale-test
    - remove-existing-stackrox-resources

    - *setupGoogleAppCreds
    - *setupLicense

    - run:
        name: Launch Sensor and Central with scale workload
        command: |
          ./deploy/k8s/deploy.sh
          ./scale/launch_workload.sh scale-test

          source ./scripts/k8s/export-basic-auth-creds.sh ./deploy/k8s/
          cci-export ROX_USERNAME "$ROX_USERNAME"
          cci-export ROX_PASSWORD "$ROX_PASSWORD"

    - *waitForAPI

    - run:
        name: Collect scale results
        command: |
          mkdir /tmp/pprof
          # 45 min run so that we are confident that the run has completely finished
          ./scale/profiler/pprof.sh /tmp/pprof "${API_ENDPOINT}" 45
          zip -r /tmp/pprof.zip /tmp/pprof

    - *storeProfilingResults
    - collect-k8s-logs
    - get-and-store-debug-dump
    - get-and-store-diagnostic-bundle
    - get-central-data

    - *getPrometheusMetricParser
    - compare-with-stored-metrics:
        cluster-flavor: gke
        test: scale-test

    - check-stackrox-logs
    - store-k8s-logs

    - teardown-gke

  openshift-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - SENSOR_HELM_DEPLOY: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m

    steps:
      - check-label-exists-or-skip:
          label: ci-openshift-tests
          run-on-master: false
      - run-openshift-tests:
          gate-label: ci-openshift-tests
          version: 0.2.11-1-geb3414898d

  openshift-crio-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m

    steps:
      - check-label-exists-or-skip:
          label: ci-openshift-crio-tests
          run-on-master: false
      - run-openshift-tests:
          suffix: "-crio"
          gate-label: ci-openshift-crio-tests
          version: 0.2.11-1-geb3414898d

  eks-api-e2e-tests:
    executor: kubectl-client-1-23
    environment:
      - EKS_AUTOMATION_VERSION: 0.2.17
      - LOCAL_PORT: 8000
      - COLLECTION_METHOD: kernel-module
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m

    steps:
      - checkout
      - check-label-exists-or-skip:
          label: ci-eks-tests
          run-on-master: false

      - setup_remote_docker

      - setup-dep-env:
          docker-login: true

      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: Sanity check KUBECONFIG
          command: |
            ls -lh $PWD/eks/data
            cci-export KUBECONFIG $PWD/eks/data/eks-kube.yaml
            kubectl get nodes
      - remove-existing-stackrox-resources

      - *setupRoxctl

      - run:
          name: Configure Deployment Environment
          command: |
            cci-export CLAIRIFY_IMAGE_TAG 0.5.4
            cci-export MAIN_IMAGE_TAG "$(make --quiet tag)"
            cci-export REGISTRY_PASSWORD "$QUAY_RHACS_ENG_RO_PASSWORD"
            cci-export REGISTRY_USERNAME "$QUAY_RHACS_ENG_RO_USERNAME"

      - *setupGoogleAppCreds
      - *setupLicense

      - *setupDefaultTLSCerts

      - deploy-stackrox:
          post-central-deploy-steps:
            - *setupClientTLSCerts

      - deploy-webhook-server

      - *getECRDockerPullPassword
      - *restoreGradle
      - restore-go-mod-cache
      - run:
          name: QA Automation Platform
          command: |
            export CLUSTER=K8S
            export API_HOSTNAME=localhost
            export API_PORT=${LOCAL_PORT}
            make -C qa-tests-backend test

      - *storeQATestResults
      - *storeQASpockReports

      - collect-k8s-logs
      - *backupCentral
      - *storeCentralBackupArtifact
      - *restoreDB
      - check-stackrox-logs
      - store-k8s-logs
      - *storeQALogs

      - *loginToGCR
      - run:
          name: Destroy EKS cluster
          command: |
            docker run --rm -t \
              -v $PWD/eks/data:/data \
              -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY \
              gcr.io/stackrox-infra/automation-flavors/eks:$EKS_AUTOMATION_VERSION destroy "${CIRCLE_WORKFLOW_ID:0:7}"
          when: always

  aks-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m

    steps:
      - run-qa-tests:
          cluster-id: aks-api-e2e-tests
          sensor-deploy-flavor: helm
          determine-whether-to-run:
            - check-to-run:
                label: ci-aks-tests
                run-on-master: false
                run-on-tags: true
          use-websocket: true
          is-gke-cluster: false
          connect-to-cluster:
            - run:
                name: Restore KUBECONFIG
                command: |
                  ls -lh $PWD/aks/data
                  cci-export KUBECONFIG "$PWD/aks/data/kubeconfig"
                  kubectl get nodes
          destroy-cluster:
            - *loginToGCR
            - destroy-aks-cluster:
                when: always

  openshift-4-operator-e2e-tests:
    executor: custom
    resource_class: small
    environment:
    - IMAGE_TAG_BASE: "quay.io/rhacs-eng/stackrox-operator"

    steps:
    - checkout
    - check-to-run:
        label: ci-openshift-4-operator-tests
        run-on-master: true
        run-on-tags: true
        changed-path: '^operator/'
    - check-backend-changes

    - setup_remote_docker

    - attach_workspace:
        at: /go/src/github.com/stackrox/rox

    - run:
        name: Restore KUBECONFIG
        command: |
          ls -lh $PWD/operator-oc4/data
          cci-export KUBECONFIG "$PWD/operator-oc4/data/auth/kubeconfig"
          oc get nodes

    - *loginToGCR
    - *setupRoxctl
    - setup-gcp
    - setup-dep-env
    - remove-existing-stackrox-resources
    - *setupGoogleAppCreds

    - run:
        name: Start previous version of operator manager on cluster
        command: make -C operator deploy-previous-via-olm
        no_output_timeout: 30m

    - run:
        name: Run Operator upgrade tests
        command: make -C operator test-upgrade
        no_output_timeout: 30m

    - store-test-results-and-artifacts:
        path: operator/build/kuttl-test-artifacts

    - run:
        name: Run Operator e2e tests
        command: make -C operator test-e2e-deployed
        no_output_timeout: 30m

    - store-test-results-and-artifacts:
        path: operator/build/kuttl-test-artifacts

    - run:
        name: Run Operator Bundle Scorecard tests
        command: ./operator/scripts/retry.sh 4 2 make -C operator bundle-test-image

    - collect-k8s-logs:
        orchestrator-flavor: openshift
    - store-k8s-logs:
        destination: operator-e2e-test-k8s-service-logs

    - destroy-openshift-4-cluster:
        when: always
        cluster-name: operator-oc4

  openshift-4-6-operator-e2e-tests:
    executor: custom
    resource_class: small
    environment:
    - IMAGE_TAG_BASE: "quay.io/rhacs-eng/stackrox-operator"

    steps:
    - checkout
    - check-to-run:
        label: ci-openshift-4-operator-tests  # drive all openshift-4*-operator-e2e-tests from a single label
        run-on-master: true
        run-on-tags: true
        changed-path: '^operator/'
    - check-backend-changes

    - setup_remote_docker

    - attach_workspace:
        at: /go/src/github.com/stackrox/rox

    - run:
        name: Restore KUBECONFIG
        command: |
          ls -lh $PWD/operator-oc4-6/data
          cci-export KUBECONFIG "$PWD/operator-oc4-6/data/auth/kubeconfig"
          oc get nodes

    - *loginToGCR
    - *setupRoxctl
    - setup-gcp
    - setup-dep-env
    - remove-existing-stackrox-resources
    - *setupGoogleAppCreds

    - run:
        name: Start previous version of operator manager on cluster
        command: make -C operator deploy-previous-via-olm
        no_output_timeout: 30m

    - run:
        name: Run Operator upgrade tests
        command: make -C operator test-upgrade
        no_output_timeout: 30m

    - store-test-results-and-artifacts:
        path: operator/build/kuttl-test-artifacts

    - run:
        name: Run Operator e2e tests
        command: make -C operator test-e2e-deployed
        no_output_timeout: 30m

    - store-test-results-and-artifacts:
        path: operator/build/kuttl-test-artifacts

    - run:
        name: Run Operator Bundle Scorecard tests
        command: ./operator/scripts/retry.sh 4 2 make -C operator bundle-test-image

    - collect-k8s-logs:
        orchestrator-flavor: openshift
    - store-k8s-logs:
        destination: operator-e2e-test-k8s-service-logs

    - destroy-openshift-4-cluster:
        when: always
        cluster-name: operator-oc4-6

  roxctl-windows-test:
    executor:
      name: win/default  # References orbs.win defined above.
      size: "medium"

    steps:
      - checkout
      - attach_workspace:
          at: /go/src/github.com/stackrox/rox

      - run:
          name: "Generate K8s/OpenShift bundles using roxctl.exe"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            ./bin/windows/roxctl.exe central generate k8s none --output-dir rox-k8s-bundle
            ./bin/windows/roxctl.exe central generate openshift none --output-dir rox-openshift-bundle

      - run:
          name: "Verify contents of generated bundles"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            dir --recursive rox-k8s-bundle
            test -f rox-k8s-bundle/README
            dir --recursive rox-openshift-bundle
            test -f rox-openshift-bundle/README

      - run:
          name: "Verify version"
          shell: bash.exe
          command: |
            # In Windows, the workspace ends up getting attached relative to the current working directory
            # so we need to do this cd every time.
            cd go/src/github.com/stackrox/rox
            version_from_yaml="$(grep -rnw -e 'image.*main' rox-k8s-bundle/central | sed -E 's/.*main.*:([^" ]+).*/\1/g')"
            version_from_roxctl="$(./bin/windows/roxctl.exe version)"
            printf "Versions:\n  from YAML: %s\nfrom roxctl: %s\n" "${version_from_yaml}" "${version_from_roxctl}"
            test "${version_from_yaml}" = "${version_from_roxctl}"

  race-condition-tests:
    executor: custom
    environment:
      LOCAL_PORT: 443
      COLLECTION_METHOD: ebpf
      GCP_IMAGE_TYPE: "COS"
      MONITORING_SUPPORT: "false"
      SCANNER_SUPPORT: "true"
      ROX_BASELINE_GENERATION_DURATION: 1m
      LOAD_BALANCER: lb
      ADMISSION_CONTROLLER: "true"
      ADMISSION_CONTROLLER_UPDATES: "true"
      ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      IS_RACE_BUILD: "true"


    steps:
      - run-qa-tests:
          cluster-id: race-condition-tests
          sensor-deploy-flavor: helm
          determine-whether-to-run:
            - *checkToRunSeparateRaceConditionTestPipe
          use-websocket: true

  ###
  # Post-build jobs to handle built artifacts (like tag, scan and publish images)
  ###

  scan-images-in-quay:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - setup_remote_docker

      - run:
          name: Check for fixable vulns
          command: |
            ./release/scripts/vuln_check.sh || ./release/scripts/vuln_check.sh

  ###
  # Infrastructure and workflow utility jobs
  ###

  retrigger-master-workflow:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - run:
          name: Maybe retrigger master workflow
          command: |
            [[ -n "${CIRCLE_TAG}" ]] # Assert that this job is only run on tags
            # If the tag was pushed on master, then retrigger the workflow!
            # We do this because the output of `make tag` changes in this case, and we want to rerun the master workflow
            # in order to produce new images with the updated version of `make tag`.
            # Note that Circle changes the local master ref to point to the tag, so we need to check against origin/master.
            if git branch -a --contains "${CIRCLE_TAG}" | grep -qx '[[:space:]]*remotes/origin/master$'; then
              curl -sS --fail -X POST -u "${CIRCLE_TOKEN_ROXBOT}:" "https://circleci.com/api/v1.1/project/github/stackrox/rox/build" -d '{"branch": "master"}'
            fi

  trigger-nightly-build:
    executor: custom
    resource_class: small
    steps:
      - checkout

      - *configureGit

      - add_ssh_keys:
          fingerprints:
            - "b0:90:e9:6f:77:d5:fe:b6:8c:1a:a5:3f:a4:e3:41:e5"

      - run:
          name: Add SSH key of github.com
          command: |
            ssh-keyscan -H github.com >> ~/.ssh/known_hosts

      - run:
          name: Create a commit and tag for nightly build
          command: |
            # Add an empty commit to diverge from master
            git commit --allow-empty -m "Nightly build $(date)"
            NIGHTLY_TAG="$(git describe --tags --abbrev=0 --exclude '*-nightly-*')-nightly-$(date '+%Y%m%d')"
            git tag "$NIGHTLY_TAG"
            git push origin "$NIGHTLY_TAG"

      - run:
          name: Remove tags more than 3 days old
          command: |
            beforeDate=$(date --date=@$(($(date +'%s') - (3 * 24 * 60 * 60))) +'%Y%m%d')
            echo "Anything prior to ${beforeDate} will be deleted"
            tags=$(git tag --list '*nightly*')
            for tag in $tags; do
                echo "Considering nightly tag: ${tag}"
                datePart="${tag##*-}"
                echo "  date part: ${datePart}"
                if [[ "${datePart}" =~ ^-?[0-9]+$ ]] && [ "${datePart}" -lt ${beforeDate} ]; then
                    echo "  this tag is a candidate for deletion"
                    git push --delete origin "${tag}"
                else
                    echo "  this tag is not a candidate for deletion"
                fi
            done

  provision-openshift-rosa-api-e2e-tests:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - check-to-run:
          label: ci-openshift-rosa-tests
          run-on-master: false
          run-on-tags: true
      - *loginToGCR
      - setup_remote_docker
      - run:
          name: Launch ROSA cluster
          timeout: 7200
          command: |
            docker create -v /data --name artifacts alpine:3.14 /bin/true
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              gcr.io/stackrox-infra/automation-flavors/openshift-rosa:0.2.17 \
              create
            mkdir -p /home/circleci/artifacts
            docker cp artifacts:/data/. /home/circleci/artifacts
      - persist_to_workspace:
          root: /home/circleci
          paths:
            - artifacts

  provision-openshift-aro-api-e2e-tests:
    executor: custom
    resource_class: small
    steps:
      - checkout
      - check-to-run:
          label: ci-openshift-aro-tests
          run-on-master: false
          run-on-tags: true
      - *loginToGCR
      - setup_remote_docker
      - run:
          name: Launch ARO cluster
          timeout: 7200
          command: |
            docker create -v /data --name artifacts alpine:3.14 /bin/true
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e AZURE_SP_SECRET_VAL="$AZURE_SP_SECRET_VAL" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              gcr.io/stackrox-infra/automation-flavors/openshift-aro:0.2.17 \
              create
            mkdir -p /home/circleci/artifacts
            docker cp artifacts:/data/. /home/circleci/artifacts
      - persist_to_workspace:
          root: /home/circleci
          paths:
            - artifacts

  provision-osd-aws-api-e2e-tests:
    executor: custom
    resource_class: large
    steps:
      - checkout
      - check-to-run:
          label: ci-osd-aws-tests
          run-on-master: false
          run-on-tags: true
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Launch osd-on-aws cluster
          timeout: 7200
          command: |
            docker create -v /data --name artifacts alpine:3.14 /bin/true
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              gcr.io/stackrox-infra/automation-flavors/openshift-osd:0.2.26 \
              aws create
            mkdir -p /home/circleci/artifacts
            docker cp artifacts:/data/. /home/circleci/artifacts
      - persist_to_workspace:
          root: /home/circleci
          paths:
            - artifacts

  provision-osd-gcp-api-e2e-tests:
    executor: custom
    resource_class: large
    steps:
      - checkout
      - check-to-run:
          label: ci-osd-gcp-tests
          run-on-master: false
          run-on-tags: true
      - setup_remote_docker
      - *loginToGCR
      - run:
          name: Launch osd-on-gcp cluster
          timeout: 7200
          command: |
            docker create -v /data --name artifacts alpine:3.14 /bin/true
            docker run --rm -t \
              --volumes-from artifacts \
              -e SCRATCH="/data" \
              -e CIRCLECI="$CIRCLECI" \
              -e OPENSHIFT_CLUSTER_MANAGER_API_TOKEN="$OPENSHIFT_CLUSTER_MANAGER_API_TOKEN" \
              -e REDHAT_PULL_SECRET_BASE64="$REDHAT_PULL_SECRET_BASE64" \
              -e GCP_SA_CREDS_JSON_BASE64="$GCP_SA_CREDS_JSON_BASE64" \
              -e GCP_PROJECT="srox-temp-dev-test" \
              gcr.io/stackrox-infra/automation-flavors/openshift-osd:0.2.26 \
              gcp create
            mkdir -p /home/circleci/artifacts
            docker cp artifacts:/data/. /home/circleci/artifacts
      - persist_to_workspace:
          root: /home/circleci
          paths:
            - artifacts

  openshift-rosa-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m

    steps:
      - run-qa-tests:
          cluster-id: openshift-rosa-api-e2e-tests
          sensor-deploy-flavor: helm
          orchestrator-flavor: openshift
          determine-whether-to-run:
            - check-to-run:
                label: ci-openshift-rosa-tests
                run-on-master: false
                run-on-tags: true
          use-websocket: true
          is-gke-cluster: false
          connect-to-cluster:
            - run:
                name: Restore KUBECONFIG and login to ROSA
                command: |
                  cci-export STORAGE pvc
                  cci-export KUBECONFIG "$PWD/artifacts/kubeconfig"
                  source "$PWD/artifacts/dotenv"
                  oc_login_command=$(grep "oc login https.*username.*password" "$PWD/artifacts/log/rosa-create-admin.log")
                  eval "$oc_login_command --insecure-skip-tls-verify=true"
                  oc get nodes
          destroy-cluster:
            - nop-steps-stanza
      - destroy-rosa-cluster

  openshift-aro-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m

    steps:
      - run-qa-tests:
          cluster-id: openshift-aro-api-e2e-tests
          sensor-deploy-flavor: helm
          orchestrator-flavor: openshift
          determine-whether-to-run:
            - check-to-run:
                label: ci-openshift-aro-tests
                run-on-master: false
                run-on-tags: true
          use-websocket: true
          is-gke-cluster: false
          connect-to-cluster:
            - run:
                name: Restore KUBECONFIG and login to ARO
                command: |
                  cci-export STORAGE pvc
                  cci-export KUBECONFIG "$PWD/artifacts/kubeconfig"
                  source "$PWD/artifacts/dotenv"
                  oc login "$API_ENDPOINT" --username "$CONSOLE_USER" --password "$CONSOLE_PASSWORD" --insecure-skip-tls-verify=true
                  oc get nodes
          destroy-cluster:
            - nop-steps-stanza
      - destroy-aro-cluster

  osd-aws-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - ROX_VULN_RISK_MANAGEMENT: true

    steps:
      - run-qa-tests:
          cluster-id: osd-aws-api-e2e-tests
          sensor-deploy-flavor: helm
          orchestrator-flavor: openshift
          determine-whether-to-run:
            - check-to-run:
                label: ci-osd-aws-tests
                run-on-master: false
                run-on-tags: true
          use-websocket: true
          is-gke-cluster: false
          connect-to-cluster:
            - run:
                name: Restore KUBECONFIG and login to osd-aws
                command: |
                  cci-export STORAGE pvc
                  cci-export KUBECONFIG "$PWD/artifacts/kubeconfig"
                  source "$PWD/artifacts/dotenv"
                  oc login "$CLUSTER_API_ENDPOINT" \
                    --username "$CLUSTER_USERNAME" \
                    --password "$CLUSTER_PASSWORD" \
                    --insecure-skip-tls-verify=true
                  oc get nodes
          destroy-cluster:
            - nop-steps-stanza
      - destroy-osd-aws-cluster

  osd-gcp-api-e2e-tests:
    executor: custom
    environment:
      - LOCAL_PORT: 443
      - COLLECTION_METHOD: ebpf
      - MONITORING_SUPPORT: false
      - SCANNER_SUPPORT: true
      - ROX_WHITELIST_GENERATION_DURATION: 1m
      - LOAD_BALANCER: lb
      - ADMISSION_CONTROLLER: true
      - ADMISSION_CONTROLLER_UPDATES: true
      - ROX_NETWORK_GRAPH_PORTS: true
      - ROX_NETWORK_GRAPH_EXTERNAL_SRCS: true
      - ROX_SYSLOG_INTEGRATION: true
      - ROX_NETWORK_BASELINE_OBSERVATION_PERIOD: 2m
      - ROX_BASELINE_GENERATION_DURATION: 1m
      - ROX_VULN_RISK_MANAGEMENT: true

    steps:
      - run-qa-tests:
          cluster-id: osd-gcp-api-e2e-tests
          sensor-deploy-flavor: helm
          orchestrator-flavor: openshift
          determine-whether-to-run:
            - check-to-run:
                label: ci-osd-gcp-tests
                run-on-master: false
                run-on-tags: true
          use-websocket: true
          is-gke-cluster: false
          connect-to-cluster:
            - run:
                name: Restore KUBECONFIG and login to osd-gcp
                command: |
                  cci-export STORAGE pvc
                  cci-export KUBECONFIG "$PWD/artifacts/kubeconfig"
                  source "$PWD/artifacts/dotenv"
                  oc login "$CLUSTER_API_ENDPOINT" \
                    --username "$CLUSTER_USERNAME" \
                    --password "$CLUSTER_PASSWORD" \
                    --insecure-skip-tls-verify=true
                  oc get nodes
          destroy-cluster:
            - nop-steps-stanza
      - destroy-osd-gcp-cluster

workflows:
  version: 2

  build_all:
    when:
      not: << pipeline.parameters.trigger_on_demand >>
    jobs: &buildAllJobs
      - slack-notify:
          <<: *runOnTagsOnly
          context: custom-executor-pull
      - go-postgres-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - pre-create-parameters:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - pre-build-ui-stackrox:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - pre-create-parameters
      - pre-build-ui-rhacs:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - pre-create-parameters
      - pre-build-cli:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - pre-create-parameters
      - pre-build-go-binaries:
          <<: *runOnAllTagsWithPushCtx
          requires:
            - pre-create-parameters
      - build-stackrox:
          <<: *runOnAllTagsWithPushCtx
          requires:
            - pre-build-ui-stackrox
            - pre-build-cli
            - pre-build-go-binaries
      - build-rhacs:
          <<: *runOnAllTagsWithPushCtx
          requires:
            - pre-build-ui-rhacs
            - pre-build-cli
            - pre-build-go-binaries
      - build-operator:
          <<: *runOnAllTags
          context:
            - quay-stackrox-io-readwrite
            - quay-rhacs-eng-readwrite
            - quay-rhacs-eng-readonly
      - build-race-condition-debug-image:
          <<: *runOnAllTagsWithPushCtx
      - build-scale-monitoring-and-mock-server:
          <<: *runOnAllTagsWithPushCtx
      - provision-gke-postgres-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - gke-postgres-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-gke-postgres-api-e2e-tests
      - provision-gke-kernel-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - gke-kernel-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-gke-kernel-api-e2e-tests
      - provision-gke-api-scale-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - gke-api-scale-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-gke-api-scale-tests
      - provision-gke-postgres-api-scale-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - gke-postgres-api-scale-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
          - build-stackrox
          - build-rhacs
          - build-scale-monitoring-and-mock-server
          - provision-gke-postgres-api-scale-tests
      - provision-openshift-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - openshift-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-openshift-api-e2e-tests
      - provision-openshift-crio-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - openshift-crio-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-openshift-crio-api-e2e-tests
      - provision-eks-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - eks-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-scale-monitoring-and-mock-server
            - provision-eks-api-e2e-tests
      # ROX-8306 - Disabled due to Azure account issue
      # - provision-aks-api-e2e-tests:
      #     <<: *runOnAllTagsWithQuayIOPullCtx
      # - aks-api-e2e-tests:
      #     <<: *runOnAllTagsWithQuayIOPullCtx
      #     requires:
      #       - build-stackrox
      #       - build-rhacs
      #       - build-scale-monitoring-and-mock-server
      #       - provision-aks-api-e2e-tests
      - provision-openshift-4-operator-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - provision-openshift-4-6-operator-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - openshift-4-operator-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-operator
            - provision-openshift-4-operator-e2e-tests
      - openshift-4-6-operator-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-stackrox
            - build-rhacs
            - build-operator
            - provision-openshift-4-6-operator-e2e-tests
      - provision-race-condition-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
      - race-condition-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-race-condition-debug-image
            - build-scale-monitoring-and-mock-server
            - provision-race-condition-tests
      - roxctl-windows-test:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - pre-build-cli

      - provision-openshift-rosa-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          context:
            - osd-cluster-manager
            - quay-rhacs-eng-readonly

      - openshift-rosa-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-scale-monitoring-and-mock-server
            - provision-openshift-rosa-api-e2e-tests
            - build-stackrox
            - build-rhacs
          context:
            - osd-cluster-manager
            - quay-rhacs-eng-readonly

      - provision-openshift-aro-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          context:
            - com-redhat-cloud
            - quay-rhacs-eng-readonly
            - azure-ci
            - aro-cluster-manager
            - osd-cluster-manager

      - openshift-aro-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-scale-monitoring-and-mock-server
            - provision-openshift-aro-api-e2e-tests
            - build-stackrox
            - build-rhacs
          context:
            - com-redhat-cloud
            - quay-rhacs-eng-readonly
            - azure-ci
            - aro-cluster-manager
            - osd-cluster-manager

      - provision-osd-aws-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          context:
            - quay-rhacs-eng-readonly
            - osd-cluster-manager

      - osd-aws-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-scale-monitoring-and-mock-server
            - provision-osd-aws-api-e2e-tests
            - build-stackrox
            - build-rhacs
          context:
            - quay-rhacs-eng-readonly
            - osd-cluster-manager

      - provision-osd-gcp-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          context:
            - quay-rhacs-eng-readonly
            - osd-cluster-manager

      - osd-gcp-api-e2e-tests:
          <<: *runOnAllTagsWithQuayIOPullCtx
          requires:
            - build-scale-monitoring-and-mock-server
            - provision-osd-gcp-api-e2e-tests
            - build-stackrox
            - build-rhacs
          context:
            - quay-rhacs-eng-readonly
            - osd-cluster-manager

      - scan-images-in-quay:
          <<: *runOnTagsOnly
          context:
          - quay-rhacs-eng-readonly
          requires:
            - build-stackrox
            - build-rhacs

      - retrigger-master-workflow:
          <<: *runOnTagsOnly
          context: custom-executor-pull

  nightly:
    triggers:
      - schedule:
          # still overnight from US PoV, but early enough for the early risers in Europe to not have to wait for the results
          cron: "0 5 * * *"
          filters:
            branches:
              only: master

    jobs:
      - trigger-nightly-build:
          filters:
            branches:
              only: master
          context: quay-rhacs-eng-readonly

  ###
  # These workflows can be triggered on demand.
  ##

  # To trigger a RHACS branded build for example:
  #
  #   curl -X POST --header 'Content-Type: application/json' \
  #        --header "Circle-Token: <your CI token>" https://circleci.com/api/v2/project/github/stackrox/stackrox/pipeline -d '{
  #     "branch": "<your branch>",
  #     "parameters": {
  #         "trigger_on_demand": true,
  #         "workflow_name": "build-rhacs"
  #     }
  # }'

  build-rhacs:
    when:
      equal: [ "build-rhacs", << pipeline.parameters.workflow_name >> ]
    jobs:
      - pre-create-parameters:
          context: custom-executor-pull
      - pre-build-ui-rhacs:
          context: custom-executor-pull
          requires:
            - pre-create-parameters
      - pre-build-cli:
          context: custom-executor-pull
          requires:
            - pre-create-parameters
      - pre-build-go-binaries:
          requires:
            - pre-create-parameters
      - build-rhacs:
          requires:
            - pre-build-ui-rhacs
            - pre-build-cli
            - pre-build-go-binaries

  designated_image_test:
    when:
      equal: [ "designated_image_test", << pipeline.parameters.workflow_name >> ]
    jobs:
      - designated-image-tests:
          context: quay-rhacs-eng-readonly

  # To trigger build_all with custom parameters.
  #
  #   curl -X POST --header 'Content-Type: application/json' \
  #        --header "Circle-Token: <your CI token>" https://circleci.com/api/v2/project/github/stackrox/stackrox/pipeline -d '{
  #     "branch": "<your branch>",
  #     "parameters": {
  #         "trigger_on_demand": true,
  #         "workflow_name": "build_all",
  #         "collector_version": "1.2.3"
  #     }
  # }'

  build_all_on_demand:
    when:
      equal: [ "build_all", << pipeline.parameters.workflow_name >> ]
    jobs:
      *buildAllJobs
