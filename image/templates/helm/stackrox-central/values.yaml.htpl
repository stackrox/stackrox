## StackRox Central chart default settings file.
##
## This file includes the default settings for the StackRox Central chart.
## It serves as a form of documentation for all the possible settings that a
## user can override are. HOWEVER, if you want to override some settings, DO NOT
## create a copy of this file to be used as a baseline, or modify it in place.
## Instead, create a file that contains only those settings you want to override,
## and pass it to helm or roxctl via the `-f` parameter.
##
## For example, if you want to disable the deployment of scanner, create a file
## `values-override.yaml` (or any name you choose) with the following contents:
##
##  scanner:
##    disable: true
##
## and then invoke helm by passing `-f values-override.yaml` to
## `helm install`/`helm upgrade`.
##
## Alternatively, if you want to override just a few values, you can set them directly
## via the `--set` command, e.g.,
##  $ helm install --set scanner.disable=true ...
##
## Note that an arbitrary number of `-f` and `--set` parameters can be combined. It is
## generally a good practice to store secret data such as the admin password separate from
## non-sensitive configuration data.
##
#
## Configuration for image pull secrets.
## These should usually be set via the command line when running `helm install`, e.g.,
## helm install --set imagePullSecrets.username=myuser --set imagePullSecrets.password=mypass,
## or be stored in a separate YAML-encoded secrets file.
#imagePullSecrets:
#  # Username and password to be used for pulling images.
#  # These should usually be set via the command line when running `helm install`, e.g.,
#  # helm install --set imagePullSecrets.username=myuser --set imagePullSecrets.password=mypass,
#  # or be stored in a separate YAML-encoded secrets file.
#  username: null
#  password: null
#
#  # If no image pull secrets are provided, an installation would usually fail. In order to
#  # prevent it from failing, this option must explicitly be set to true.
#  allowNone: false
#
#  # If there exist available image pull secrets in the cluster that are managed separately,
#  # set this value to the list of the respective secret names. While it is recommended to
#  # record the secret names in a persisted YAML file, providing a single string containing
#  # a comma-delimited list of secret names is also supported, for easier interaction with
#  # --set.
#  useExisting: []
#
#  # Whether to import any secrets from the default service account existing in the StackRox
#  # namespace. The default service account often contains "standard" image pull secrets that
#  # should be used by default for image pulls, hence this defaults to true. Only has an effect
#  # if server-side lookups are enabled.
#  useFromDefaultServiceAccount: true
#
## Common settings for all image properties
#image:
#  # The image registry to use. Unless overridden in the more specific configs, this
#  # determines the base registry for each image referenced in this config file.
#  registry: [< required "" .MainRegistry >]
#
## Settings regarding the installation environment
#env:
#  # Treat the environment as an OpenShift cluster. Leave this unset to use auto-detection
#  # based on available API resources on the server.
#  # Possible values: null, false, true, 3, 4
#  openshift: null
#
#  # Treat the environment as Istio-enabled. Leave this unset to use auto-detection based on
#  # available API resources on the server.
#  # Possible values: null, false, true
#  istio: null
#
#  # The cloud provider platform where the target Kubernetes cluster is running. Leave this
#  # unset to use auto-detection based on the Kubernetes version.
#  # Possible values: null, "default", "gke"
#  platform: null
#
#  # Whether to run StackRox in offline mode. When run in offline mode, no connections to external
#  # endpoints will be made.
#  offlineMode: false
#
#  # The proxy configuration for Central and Scanner, specified either as an embedded YAML
#  # directionary, or as an (expandable) string.
#  proxyConfig: null
#
#
## Settings for the StackRox Service CA certificates.
## If `cert` and `key` are both set (it is an error to set only one of the two), the corresponding
## values are used as the PEM-encoded certificate and private key for the internal Service CA.
## If they are left unspecified, they are generated under the following conditions:
## - `generate` is explicitly set to true, or
## - `generate` is unset (null), and the Helm chart is being freshly installed (as opposed to being
##    upgraded).
#ca:
#  cert: null
#  key: null
#  generate: null
#

## Additional CA certificates to trust, besides system roots
## If specified, this should be a map mapping file names to PEM-encoded contents.
#additionalCAs: null
#
[<- if and .TelemetryEnabled (and (eq .TelemetryKey "") (eq .TelemetryEndpoint ""))>]
#central:
#
#  # Settings for telemetry data collection. Telemetry is enabled by default.
#  telemetry:
#    enabled: true
#    storage:
#      endpoint: null
#      key: null
[<- else >]
central:
# # Settings for telemetry data collection.
  telemetry:
    enabled: [< .TelemetryEnabled >]
    storage:
      endpoint: "[< .TelemetryEndpoint >]"
      key: "[< .TelemetryKey >]"
[<- end >]
#
#
#  config: "@config/central/config.yaml|config/central/config.yaml.default"
#
#  endpointsConfig: "@config/central/endpoints.yaml|config/central/endpoints.yaml.default"
#
#
#  nodeSelector: null
#
#  jwtSigner:
#    key: null
#    generate: null
#
#  # Settings for the internal service-to-service TLS certificate used by central.
#  # See the documentation for `ca` at the top level for an explanation.
#  serviceTLS:
#    cert: null
#    key: null
#    generate: null
#
#  defaultTLS:
#    cert: null
#    key: null
#
#  image:
#    registry: null
#    name: main
#    tag: 4.0.0
#    fullRef: null
#
#  adminPassword:
#    value: null
#    generate: null
#    htpasswd: null
#
#  resources:
#    requests:
#      memory: "4Gi"
#      cpu: "1500m"
#    limits:
#      memory: "8Gi"
#      cpu: "4000m"
#
#  persistence:
#    hostPath: null
#    persistentVolumeClaim:
#      claimName: null
#      createClaim: null
#      storageClass: null
#      size: null
#    none: null
#
#  exposure:
#
#    # LoadBalancer configuration.
#    # Disabled by default.
#    # Default port is 443.
#    loadBalancer:
#      enabled: null
#      port: null
#      ip: null
#
#    # NodePort configuration.
#    # Disabled by default.
#    nodePort:
#      enabled: null
#      port: null
#
#    # Route configuration.
#    # Disabled by default.
#    route:
#      enabled: null
#      # Specify a custom hostname if desired, otherwise accept the default from OpenShift.
#      host: null
#
#  db:
#    # External signifies that a Postgres wire-compatible database has already been deployed and a Central DB pod
#    # does not need to be deployed
#    external: false
#
#    source:
#      # ConnectionString should not be specified if the Central DB deployment is being managed by the helm chart
#      # The connection string must be in the format described here https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNSTRING
#      # client_encoding=UTF8 is required in any connection string and the only supported encoding
#      # statementTimeoutMs is ignored for external database connections
#      # If using a connection that supports "statement_timeout" it is recommended to include "statement_timeout=1200000"
#      # Do NOT use a connection string with a password field. Instead specify the value below in the password section.
#      connectionString: null
#      minConns: 10
#      maxConns: 90
#      statementTimeoutMs: 1200000
#
#    # The admin password setting for communication with Central's DB.
#    # When a value is set explicitly, this is always used, even on upgrade.
#    # Otherwise, a password will be automatically generated if `generate` is set to true,
#    # or left unset (null) and the Helm chart is being installed (as opposed to upgraded).
#    # Should only be used when utilizing Postgres as central's DB
#    password:
#      value: null
#      generate: null
#
#    postgresConfig: "@config/centraldb/postgresql.conf|config/centraldb/postgresql.conf.default"
#    hbaConfig: "@config/centraldb/pg_hba.conf|config/centraldb/pg_hba.conf.default"
#
#    # Specifying configOverride mounts the specified config map in the same namespace which must contain
#    # both pg_hba.conf and postgresql.conf. This should only be used when the default settings are not
#    # sufficient and manual override is required.
#    configOverride: null
#
#    nodeSelector: null
#
#    # Settings for the internal service-to-service TLS certificate used by central.
#    # See the documentation for `ca` at the top level for an explanation.
#    serviceTLS:
#      cert: null
#      key: null
#      generate: null
#
#    image:
#      registry: null
#      name: central-db
#      tag: 4.0.0
#      fullRef: null
#
#    resources:
#      requests:
#        memory: "8Gi"
#        cpu: "4"
#      limits:
#        memory: "16Gi"
#        cpu: "8"
#
#    persistence:
#      hostPath: null
#      persistentVolumeClaim:
#        claimName: null
#        createClaim: null
#        storageClass: null
#        size: null
#      none: null
#
## Configuration options relating to StackRox Scanner.
#scanner:
#  # If this is set to true, StackRox will be deployed without scanner. No other setting in this
#  # section will have any effect.
#  disable: false
#
#  # Default number of scanner replicas created upon startup. The actual number might be higher
#  # or lower if autoscaling is enabled (see below).
#  replicas: 3
#
#  logLevel: INFO
#
#  # Settings related to autoscaling the scanner deployment.
#  autoscaling:
#    # If true, autoscaling will be disabled. None of the other settings in this section will
#    # have any effect.
#    disable: false
#    minReplicas: 1
#    maxReplicas: 5
#
#  # Resource settings for the scanner deployment.
#  resources:
#    requests:
#      memory: "1500Mi"
#      cpu: "1000m"
#    limits:
#      memory: "4Gi"
#      cpu: "2000m"
#
#  image:
#    registry: null
#    name: scanner
#    tag: 2.3.2
#    fullRef: null
#
#  dbImage:
#    registry: null
#    name: scanner-db
#    tag: 2.3.2
#    fullRef: null
#
#  # Resource settings for the scanner-db deployment.
#  dbResources:
#    limits:
#      cpu: 2
#      memory: 4Gi
#    requests:
#      cpu: 200m
#      memory: 200Mi
#
#  # The admin password setting for communication with scanner's DB.
#  # When a value is set explicitly, this is always used, even on upgrade.
#  # Otherwise, a password will be automatically generated if `generate` is set to true,
#  # or left unset (null) and the Helm chart is being installed (as opposed to upgraded).
#  dbPassword:
#    value: null
#    generate: null
#
#  # Settings for the internal service-to-service TLS certificate used by scanner.
#  # See the documentation for `ca` at the top level for an explanation.
#  serviceTLS:
#    cert: null
#    key: null
#    generate: null
#
#  # Settings for the internal service-to-service TLS certificate used by scanner-db.
#  # See the documentation for `ca` at the top level for an explanation.
#  dbServiceTLS:
#    cert: null
#    key: null
#    generate: null
#
## Configuration options relating to Scanner V4.
#scannerV4:
#
#  # If this is set to false, Scanner V4 will be enabled and deployed.
#  # If it is set to true, no other setting in this section will have any effect.
#  disable: true
#
#  image:
#    registry: null
#    name: [< required "" .ScannerV4ImageRemote >]
#    tag: [< required "" .ScannerV4ImageTag >]
#    fullRef: null
#
#  indexer:
#    # The log level for the indexer deployment. This typically does not need to be changed.
#    logLevel: INFO
#
#    # If you want to enforce indexer to only run on certain nodes, you can specify
#    # a node selector here to make sure Scanner V4 can only be scheduled on Nodes with the
#    # given label.
#    nodeSelector: null
#
#    # If the nodes selected by the node selector are tainted, you can specify the corresponding taint tolerations here.
#    tolerations: null
#
#    # If scheduling needs specific affinities, you can specify the corresponding affinities here.
#    affinity:
#      podAntiAffinity:
#        preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 100
#          podAffinityTerm:
#            labelSelector:
#              matchLabels:
#                app: scanner-v4-indexer
#            topologyKey: kubernetes.io/hostname
#      nodeAffinity:
#        preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 50
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/infra
#              operator: Exists
#        - weight: 25
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/compute
#              operator: Exists
#        # From v1.20 node-role.kubernetes.io/control-plane replaces node-role.kubernetes.io/master (removed in
#        # v1.25). We apply both because our goal is not to run pods on control plane nodes for any version of k8s.
#        - weight: 100
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/master
#              operator: DoesNotExist
#        - weight: 100
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/control-plane
#              operator: DoesNotExist
#
#    # Default number of indexer replicas created upon startup. The actual number might be higher
#    # or lower if autoscaling is enabled (see below).
#    replicas: 3
#
#    # Settings related to autoscaling the indexer deployment.
#    autoscaling:
#      # If true, autoscaling will be disabled. None of the other settings in this section will
#      # have any effect.
#      disable: false
#      minReplicas: 2
#      maxReplicas: 5
#
#    # Resource settings for the indexer deployment.
#    resources:
#      requests:
#        memory: "1500Mi"
#        cpu: "1000m"
#      limits:
#        memory: "3Gi"
#        cpu: "2000m"
#
#    # Settings for the internal service-to-service TLS certificate used by indexer.
#    # See the documentation for `ca` at the top level for an explanation.
#    serviceTLS:
#      cert: null
#      key: null
#      generate: null
#
#  matcher:
#    # The log level for the matcher deployment. This typically does not need to be changed.
#    logLevel: INFO
#
#    # If you want to enforce matcher to only run on certain nodes, you can specify
#    # a node selector here to make sure Scanner V4 can only be scheduled on Nodes with the
#    # given label.
#    nodeSelector: null
#
#    # If the nodes selected by the node selector are tainted, you can specify the corresponding taint tolerations here.
#    tolerations: null
#
#    # If scheduling needs specific affinities, you can specify the corresponding affinities here.
#    affinity:
#      podAntiAffinity:
#        preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 100
#          podAffinityTerm:
#            labelSelector:
#              matchLabels:
#                app: scanner-v4-matcher
#            topologyKey: kubernetes.io/hostname
#      nodeAffinity:
#        preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 50
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/infra
#              operator: Exists
#        - weight: 25
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/compute
#              operator: Exists
#        # From v1.20 node-role.kubernetes.io/control-plane replaces node-role.kubernetes.io/master (removed in
#        # v1.25). We apply both because our goal is not to run pods on control plane nodes for any version of k8s.
#        - weight: 100
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/master
#              operator: DoesNotExist
#        - weight: 100
#          preference:
#            matchExpressions:
#            - key: node-role.kubernetes.io/control-plane
#              operator: DoesNotExist
#
#    # Default number of matcher replicas created upon startup. The actual number might be higher
#    # or lower if autoscaling is enabled (see below).
#    replicas: 2
#    # Settings related to autoscaling the matcher deployment.
#    autoscaling:
#      # If true, autoscaling will be disabled. None of the other settings in this section will
#      # have any effect.
#      disable: false
#      minReplicas: 2
#      maxReplicas: 3
#
#    # Resource settings for the matcher deployment.
#    resources:
#      requests:
#        memory: "4Gi"
#        cpu: "1000m"
#      limits:
#        memory: "5Gi"
#        cpu: "2000m"
#
#    # Settings for the internal service-to-service TLS certificate used by matcher.
#    # See the documentation for `ca` at the top level for an explanation.
#    serviceTLS:
#      cert: null
#      key: null
#      generate: null
#
#  db:
#    image:
#      registry: null
#      name: [< required "" .ScannerV4DBImageRemote >]
#      tag: [< required "" .ScannerV4ImageTag >]
#      fullRef: null
#
#    # Resource settings for the DB deployment.
#    resources:
#      requests:
#        cpu: "200m"
#        memory: "2Gi"
#      limits:
#        cpu: "2000m"
#        memory: "4Gi"
#
#    # Configure DB password. If `generate` is true, a new password will be
#    # generated. If it is false, a valid password is expected in `value`.
#    password:
#      generate: null
#      value: null
#
#    # Configure persistence backend for the DB.
#    # By default a PVC is used. Unless a Storage Class is specified
#    # a default Storage Class is expected.
#    persistence:
#      hostPath: null
#      persistentVolumeClaim:
#        claimName: "scanner-v4-db"
#        size: "50Gi"
#        createClaim: true
#        storageClass: null
#       none: null
#
#    nodeSelector: null
#
#    # Settings for the internal service-to-service TLS certificate used by the DB.
#    # See the documentation for `ca` at the top level for an explanation.
#    serviceTLS:
#      cert: null
#      key: null
#      generate: null
#
#    postgresConfig: "@config-templates/scanner-v4-db/postgresql.conf|config-templates/scanner-v4-db/postgresql.conf.default"
#    hbaConfig: "@config-templates/scanner-v4-db/pg_hba.conf|config-templates/scanner-v4-db/pg_hba.conf.default"
#
#    # Specifying configOverride mounts the specified config map in the same namespace which must contain
#    # both pg_hba.conf and postgresql.conf. This should only be used when the default settings are not
#    # sufficient and manual override is required.
#    configOverride: null
#
#
## EXPERT SETTINGS. You usually do not need to touch those.
#
## If set to true, allow deploying in a namespace other than "stackrox". This is unsupported, so
## use at your own risk.
#allowNonstandardNamespace: false
#
## If set to true, allow a release name other than "stackrox-central-services". There are no issues
## with that, but for streamlining purposes, we want to encourage all users to stick with the
## default name, and make it a little harder to deviate from that.
#allowNonstandardReleaseName: false
#
#meta:
#  # This controls whether the built-in `lookup` function will be used. If you see an error
#  # about there being no function `lookup`, set this to `false` (might be required on Helm
#  # versions before 3.1).
#  useLookup: true
#
#  # This is a dictionary from file names to contents that can be used to inject files that
#  # would usually be included via .Files.Get into the chart rendering.
#  fileOverrides: {}
#
#  # This configuration section allows overriding settings that would be inferred from the
#  # running API server.
#  apiServer:
#    # The Kubernetes version running on the API server. This is used for auto-detection
#    # of the platform.
#    version: null
#    # The list of available API resources on the server, in the form of "apps/v1" or
#    # "apps/v1/Deployment". This is used to detect environment capabilities.
#    overrideAPIResources: null
#    # A list of extra API resources that should be assumed to exist on the API server. This
#    # can be used in conjunction with both data obtained from the API server, or data set
#    # via `overrideAPIResources`.
#    extraAPIResources: []
#
#monitoring:
#  # Enables integration with OpenShift platform monitoring.
#  openshift:
#    enabled: true
